{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will assume that our task is sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Functionality Test (MFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Minimum Functionality Test is like a unit test in Software Engineering.\n",
    "If you are testing a certain capability (e.g. 'can the model handle negation?'), an MFT is composed of simple examplese that verify a specific behavior.  \n",
    "Let's create a very simple MFT for negations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good, easy, academic, educational, interesting, average, ordinary, old, art, enjoyable, entertaining, ideal, exciting, actual, original, independent, bad, innocent, excellent, amazing, interactive, intelligent, amateur, great, awful, important, introductory, experimental, awards, engaging'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's find some positive and negative adjectives\n",
    "', '.join(editor.suggest('This is not {a:mask} {thing}.', thing=['book', 'movie', 'show', 'game'])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['good', 'enjoyable', 'exciting', 'excellent', 'amazing', 'great', 'engaging']\n",
    "neg = ['bad', 'terrible', 'awful', 'horrible']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some data with both positive and negative negations, assuming `1` means positive and `0` means negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = editor.template('This is not {a:pos} {mask}.', pos=pos, labels=0, save=True, nsamples=100)\n",
    "ret += editor.template('This is not {a:neg} {mask}.', neg=neg, labels=1, save=True, nsamples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily turn this data into an MFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(ret.data, labels=ret.labels, name='Simple negation',\n",
    "           capability='Negation', description='Very simple negations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `ret` is a dict where keys have the right names for test arguments, we can also use a simpler call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(**ret, name='Simple negation',\n",
    "           capability='Negation', description='Very simple negations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an off-the-shelf sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict_proba(inputs):\n",
    "    p1 = np.array([(sentiment(x)[0] + 1)/2. for x in inputs]).reshape(-1, 1)\n",
    "    p0 = 1- p1\n",
    "    return np.hstack((p0, p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15, 0.85],\n",
       "       [0.85, 0.15]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions are random\n",
    "predict_proba(['good', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of running tests.  \n",
    "In the first (and simplest) way, you pass a function as argument to `test.run`, which gets called to make predictions.  \n",
    "We assume that the function returns a tuple with `(predictions, confidences)`, so we have a wrapper to turn softmax (like our function above) into this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([[0.15, 0.85]]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_pp(['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have this function, running the test is as simple as calling `test.run`.  \n",
    "You can run the test on a subset of testcases (for speed's sake) by specifying `n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run a test, you can print a summary of the results with `test.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      200\n",
      "Test cases run:  100\n",
      "Fails (rate):    47 (47.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 This is not an awful situation.\n",
      "----\n",
      "0.8 This is not an enjoyable video.\n",
      "----\n",
      "0.0 This is not an awful disaster.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this off-the-shelf system has trouble with negation.\n",
    "Note the failures: examples that should be negative are predicted as positive and vice versa (the number shown is the probability of positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using jupyter notebooks, you can use `test.visual_summary()` for a nice visualization version of these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105169ea6cf44d7e8c4073fb882d03ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 30, 'nfailed': 17, 'nfiltered': 0}, summarizer={'name': 'Simple negation', 'dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to run a test is from a prediction file.  \n",
    "First, we export the test into a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_raw_file('/tmp/raw_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not an amazing approach.\r\n",
      "This is not a great film.\r\n",
      "This is not an amazing article.\r\n",
      "This is not an engaging report.\r\n",
      "This is not an enjoyable bet.\r\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/raw_file.txt | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you get predictions from the examples in the raw file (in order) however you want, and save them in a prediction file.  \n",
    "Let's simulate this process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = open('/tmp/raw_file.txt').read().splitlines()\n",
    "preds = predict_proba(docs)\n",
    "f = open('/tmp/softmax_preds.txt', 'w')\n",
    "for p in preds:\n",
    "    f.write('%f %f\\n' % tuple(p))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.200000 0.800000\r\n",
      "0.700000 0.300000\r\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/softmax_preds.txt | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the test from this file.  \n",
    "We have to specify the file format (see the API for possible choices), or a function that takes a line in the file and outputs predictions and confidences.  \n",
    "Since we had already run this test, we have to set `overwrite=True` to overwrite the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run_from_file('/tmp/softmax_preds.txt', file_format='softmax', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      200\n",
      "Fails (rate):    94 (47.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 This is not an amazing approach.\n",
      "----\n",
      "0.7 This is not an engaging look.\n",
      "----\n",
      "0.0 This is not an awful movie.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Invariance test (INV) is when we apply label-preserving perturbations to inputs and expect the model prediction to remain the same.  \n",
    "Let's start by creating a fictitious dataset to serve as an example, and process it with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['This was a very nice movie directed by John Smith.',\n",
    "           'Mary Keen was brilliant.', \n",
    "          'I hated everything about this.',\n",
    "          'This movie was very bad.',\n",
    "          'I really liked this movie.',\n",
    "          'just bad.',\n",
    "          'amazing.',\n",
    "          ]\n",
    "pdataset = list(nlp.pipe(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply a simple perturbation: changing people's names and expecting predictions to remain the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a very nice movie directed by John Smith.\n",
      "This was a very nice movie directed by Michael King.\n",
      "This was a very nice movie directed by Christopher Gray.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(pdataset, Perturb.change_names)\n",
    "print('\\n'.join(t.data[0][:3]))\n",
    "print('...')\n",
    "test = INV(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 22 examples\n",
      "Test cases:      2\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different test: adding typos and expecting predictions to remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a very nice movie directed by John Smith.\n",
      "Thi swas a very nice movie directed by John Smith.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(dataset, Perturb.add_typos)\n",
    "print('\\n'.join(t.data[0][:3]))\n",
    "print('...')\n",
    "test = INV(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 14 examples\n",
      "Test cases:      7\n",
      "Fails (rate):    1 (14.3%)\n",
      "\n",
      "Example fails:\n",
      "0.8 amazing\n",
      "0.5 maazing\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional Expectation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Directional Expectation test (DIR) is just like an INV, in the sense that we apply a perturbation to existing inputs. However, instead of expecting invariance, we expect the model to behave in a some specified way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's start with a very simple perturbation: we'll add very negative phrases to the end of our small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_negative(x):\n",
    "    phrases = ['Anyway, I thought it was bad.', 'Having said this, I hated it', 'The director should be fired.']\n",
    "    return ['%s %s' % (x, p) for p in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This was a very nice movie directed by John Smith.',\n",
       " ['This was a very nice movie directed by John Smith. Anyway, I thought it was bad.',\n",
       "  'This was a very nice movie directed by John Smith. Having said this, I hated it',\n",
       "  'This was a very nice movie directed by John Smith. The director should be fired.'])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0], add_negative(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would we expect after this perturbation? I think the least we should expect is that the prediction probability of positive should **not go up** (that is, it should be monotonically decreasing).  \n",
    "Monotonicity is an expectation function that is built in, so we don't need to implement it.\n",
    "`tolerance=0.1` means we won't consider it a failure if the prediction probability goes up by less than 0.1, only if it goes up by more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.expect import Expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_decreasing = Expect.monotonic(label=1, increasing=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(dataset, add_negative)\n",
    "test = DIR(**t, expect=monotonic_decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 28 examples\n",
      "Test cases:      7\n",
      "After filtering: 6 (85.7%)\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing custom expectation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are writing a custom expectation functions, it must return a float or bool for each example such that:\n",
    "- `> 0` (or True) means passed,\n",
    "- `<= 0` or False means fail, and (optionally) the magnitude of the failure, indicated by distance from 0, e.g. -10 is worse than -1\n",
    "- `None` means the test does not apply, and this should not be counted\n",
    "\n",
    "Each test case can have multiple examples. In our MFTs, each test case only had a single example, but in our INVs and DIRs, they had multiple examples (e.g. we changed people's names to various other names).\n",
    "\n",
    "You can write custom expectation functions at multiple levels of granularity.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation on a single example\n",
    "\n",
    "If you want to write an expectation function that acts on each individual example, you write a function with the following signature:\n",
    "\n",
    "`def fn(x, pred, conf, label=None, meta=None):`\n",
    "\n",
    "For example, let's write a (useless) expectation function that checks that every prediction confidence is higher than 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that expects prediction confidence to always be more than 0.9\n",
    "def high_confidence(x, pred, conf, label=None, meta=None):\n",
    "    return conf.max() > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then wrap this function with `Expect.single`, and apply it to our previous test to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "expect_fn = Expect.single(high_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      7\n",
      "Fails (rate):    7 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 This movie was very bad.\n",
      "0.1 This movie was very bad. Anyway, I thought it was bad.\n",
      "\n",
      "----\n",
      "0.9 This was a very nice movie directed by John Smith.\n",
      "0.5 This was a very nice movie directed by John Smith. Anyway, I thought it was bad.\n",
      "0.5 This was a very nice movie directed by John Smith. Having said this, I hated it\n",
      "\n",
      "----\n",
      "0.9 Mary Keen was brilliant.\n",
      "0.6 Mary Keen was brilliant. Anyway, I thought it was bad.\n",
      "0.5 Mary Keen was brilliant. Having said this, I hated it\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.set_expect(expect_fn)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every test case fails now: there is always some prediction in it that has confidence smaller than 0.95.  \n",
    "By default, the way we aggregate all results in a test case is such that the testcase fails if **any** examples in it fail (for MFTs), or **any but the first** fail for INVs and DIRs (because the first is usually the original data point before perturbation). You can change these defaults with the `agg_fn` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation on  pairs \n",
    "\n",
    "Most of the time for DIRs, you want to write an expectation function that acts on pairs of `(original, new)` examples - that is, the original point and the perturbed points. If this is the case, the signature is as follows:\n",
    "\n",
    "`def fn(orig_pred, pred, orig_conf, conf, labels=None, meta=None)`\n",
    "\n",
    "For example, let's write an expectation function that checks that the prediction **changed** after applying the perturbation, and wrap it with `Expect.pairwise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changed_pred(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return pred != orig_pred\n",
    "expect_fn = Expect.pairwise(changed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually create a new test where we add negation to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This was a very nice movie directed by John Smith.',\n",
       "  'This was not a very nice movie directed by John Smith.'],\n",
       " ['Mary Keen was brilliant.', 'Mary Keen was not brilliant.']]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Perturb.perturb(pdataset, Perturb.add_negation)\n",
    "t.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 10 examples\n",
      "Test cases:      5\n",
      "Fails (rate):    1 (20.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 I really liked this movie.\n",
      "0.6 I really didn't like this movie.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test = DIR(**t, expect=expect_fn)\n",
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write much more complex expectation functions, but these are enough for this tutorial.  \n",
    "You can check out `expect.py` or the notebooks for Sentiment Analysis, QQP and SQuAD for many additional examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
