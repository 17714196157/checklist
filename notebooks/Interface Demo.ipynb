{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import MFT\n",
    "from checklist.inv_dir import INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832f4c7d575647398f60da43fa4eb426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TemplateEditor(bert_suggests=[('really', 'lot'), ('just', 'lot'), ('really', 'book'), ('just', 'friend'), ('ha…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "\n",
    "#template_strs, tagged_keys, tag_dict, bert_suggests = \n",
    "editor.visual_suggest(\n",
    "    '{i} {bert} {pos_verb} {a:bert}.', \n",
    "    i=['I', 'We'], \n",
    "    the=['this', 'that', 'the']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('really', 'lot'), ('just', 'lot'), ('really', 'book')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.temp_selects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3c3a8f3dde4f93ba6a1da61f4373c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TemplateEditor(bert_suggests=['picture', 'world', 'story', 'place', 'one', 'state', 'day', 'time', 'language',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]\n",
    "comp_pairs = list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))\n",
    "\n",
    "\n",
    "#template_strs, tagged_keys, tag_dict, bert_suggests = \n",
    "editor.visual_suggest(\n",
    "    {\n",
    "        \"C\": '{first_name} is {a:comp[0]} {bert}.',\n",
    "        \"Q\": 'Who is {comp[1]}?',\n",
    "        \"A\": '{first_name}',\n",
    "    },\n",
    "    comp=comp_pairs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d725838a264c4ab885e6f5bd904a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TemplateEditor(bert_suggests=['innocent', 'Muslim', 'gay', 'dead', 'homosexual', 'American', 'bisexual', 'Jewi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]\n",
    "comp_pairs = list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))\n",
    "\n",
    "\n",
    "#template_strs, tagged_keys, tag_dict, bert_suggests = \n",
    "editor.visual_suggest((\n",
    "    'Is {first_name1} {last_name1} {a:bert}?',\n",
    "    'Is {first_name2} {last_name2} {bert}?',\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['innocent', 'Muslim', 'gay']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.temp_selects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mltests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-23024398e364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/marcotcr/work/ml-tests/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmltests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mwrapped_pp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictorWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mltests'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MARCO'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n",
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wtshuang/datasets/models/imdb-iclr_orig\n",
      "Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/wtshuang/datasets/raw/glue_data//imdb/iclr_orig', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_list='None', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=2, no_cuda=False, num_train_epochs=10.0, output_dir='//home/wtshuang/datasets/models//imdb-iclr_orig/', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=1000000000000, seed=42, server_ip='', server_port='', task_name='sst-2', tokenizer_name='', train_batch_size=16, warmup_steps=0, weight_decay=0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"@LolaMElemshaty Hahahahhahahaha, kan hyb2a John cena the 2nd xD and no need to be sorry!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TONGSHUANG'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 180)\n",
    "\n",
    "mltest_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml-tests\"))\n",
    "sys.path.append(mltest_path)\n",
    "mlfeedback_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml_feedback\"))\n",
    "sys.path.append(mlfeedback_path)\n",
    "\n",
    "from ml_feedback.analysis.sentiment_utils import *\n",
    "# get the model\n",
    "model_names = {\n",
    "    \"orig\": \"imdb-iclr_orig\"\n",
    "}\n",
    "from ml_feedback.analysis.sentiment_utils import *\n",
    "models = load_models_by_name(model_names)\n",
    "\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "sentiment = models[\"orig\"]\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n",
    "\n",
    "\n",
    "from ml_feedback.analysis.sentiment_utils import *\n",
    "\n",
    "df = read_one_dataset(\"semeval_iclr_nonbinary\")\n",
    "if len(df) > 2000:\n",
    "    df = df.sample(2000)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentences = df.sentence.tolist()\n",
    "parsed_data = list(nlp.pipe(sentences))\n",
    "labels = df.label.tolist()\n",
    "parsed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun, overwrite=True)\n",
    "#print(', '.join(editor.suggest('It was {a:bert} {air_noun}.')[:40]))\n",
    "\n",
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'tough', 'weird', 'aggressive', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)\n",
    "\n",
    "\n",
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8970 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1122/1122 [01:48<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      8970\n",
      "Fails (rate):    262 (2.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) That is an aggressive flight.\n",
      "----\n",
      "2 (0.9) The pilot is average.\n",
      "----\n",
      "2 (1.0) That was an aggressive pilot.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fa5d4965424774be2dbe3638fa90da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 8708, 'nfailed': 262, 'nfiltered': 0}, summarizer={'name': 'TESTNAME_PLACEHOL…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels = [2] * len(data)\n",
    "data += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = MFT(data, labels=labels)\n",
    "# test = MFT(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']\n",
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [00:47<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      2000\n",
      "After filtering: 1995 (99.8%)\n",
      "Fails (rate):    108 (5.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) That was a tough service.\n",
      "2 (0.7) That was an absolutely tough service.\n",
      "\n",
      "----\n",
      "0 (1.0) This was a tough pilot.\n",
      "2 (1.0) This was an amazingly tough pilot.\n",
      "\n",
      "----\n",
      "0 (1.0) We dread that plane.\n",
      "2 (1.0) We sure dread that plane.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "data += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "test = DIR(data, monotonic_label)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cba7d6e45ca414fbf58196c1739a93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 1887, 'nfailed': 108, 'nfiltered': 0}, summarizer={'name': 'TESTNAME_PLACEHOL…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [00:54<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      2000\n",
      "After filtering: 53 (2.6%)\n",
      "Fails (rate):    15 (28.3%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) The flight was aggressive.\n",
      "2 (1.0) The flight was mostly aggressive.\n",
      "\n",
      "----\n",
      "2 (0.8) The flight was aggressive.\n",
      "2 (1.0) The flight was somewhat aggressive.\n",
      "\n",
      "----\n",
      "2 (0.7) This plane was average.\n",
      "2 (1.0) This plane was reasonably average.\n",
      "\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)\n",
    "\n",
    "\n",
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'a little', 'a bit', 'slightly']\n",
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)\n",
    "\n",
    "data = editor.template(['{it} {air_noun} {be} {pos_adj}.', '{it} {air_noun} {be} {red} {pos_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "data += editor.template(['{it} {air_noun} {be} {neg_adj}.', '{it} {air_noun} {be} {red} {neg_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "test = DIR(data, monotonic_label_down)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n",
    "test.visual_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import MFT\n",
    "from checklist.inv_dir import INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MARCO'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "model = model_wrapper.ModelWrapper()\n",
    "new_pp = PredictorWrapper.wrap_softmax(model.predict_proba)\n",
    "\n",
    "\n",
    "qs = []\n",
    "labels = []\n",
    "all_questions = set()\n",
    "for x in open('/home/marcotcr/datasets/glue/glue_data/QQP/dev.tsv').readlines()[1:]:\n",
    "    try:\n",
    "        q1, q2, label = x.strip().split('\\t')[3:]\n",
    "    except:\n",
    "        print(x)\n",
    "        continue\n",
    "    all_questions.add(q1)\n",
    "    all_questions.add(q2)\n",
    "    qs.append((q1, q2))\n",
    "    labels.append(label)\n",
    "labels = np.array(labels).astype(int)\n",
    "\n",
    "import pickle\n",
    "spacy_map =  pickle.load(open('/home/marcotcr/tmp/processed_qqp.pkl', 'rb'))\n",
    "parsed_qs = [(spacy_map[q[0]], spacy_map[q[1]]) for q in qs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wtshuang/datasets/models/QQP-all\n",
      "Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/wtshuang/datasets/raw/glue_data//QQP/all', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_list='None', learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=2, no_cuda=False, num_train_epochs=3.0, output_dir='//home/wtshuang/datasets/models//QQP-all/', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=1000000000000, seed=42, server_ip='', server_port='', task_name='qqp', tokenizer_name='', train_batch_size=16, warmup_steps=0, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TONGSHUANG'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 180)\n",
    "\n",
    "mltest_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml-tests\"))\n",
    "sys.path.append(mltest_path)\n",
    "mlfeedback_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml_feedback\"))\n",
    "sys.path.append(mlfeedback_path)\n",
    "\n",
    "from ml_feedback.analysis.qqp_utils import *\n",
    "\n",
    "\n",
    "# get the model\n",
    "model_names = {\n",
    "    \"qqp\": \"QQP-all\",\n",
    "}\n",
    "from ml_feedback.analysis.qqp_utils import *\n",
    "models = load_models_by_name(model_names)\n",
    "\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "model = models[\"qqp\"]\n",
    "new_pp = PredictorWrapper.wrap_softmax(model.predict_proba)\n",
    "\n",
    "\n",
    "\n",
    "# getting the data\n",
    "\n",
    "df = read_one_dataset(\"quora\", max_size=2000)\n",
    "parsed_qs = list(zip(\n",
    "    nlp.pipe(df.question1.tolist()), \n",
    "    nlp.pipe(df.question2.tolist())\n",
    "))\n",
    "labels = df.is_duplicate.tolist()\n",
    "parsed_qs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountant, engineer, journalist, attorney, editor, artist, architect, actor, interpreter, analyst, intern, assistant, escort, organizer, economist, actress, administrator, investigator, agent, investor, secretary, author, entrepreneur, executive, advisor, auditor, educator, instructor, employee, adviser\n"
     ]
    }
   ],
   "source": [
    "professions = editor.suggest('{first_name} works as {a:bert}.')[:30]\n",
    "print(', '.join(professions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#professions = editor.suggest('{first_name} works as {a:bert}.')[:30]\n",
    "#professions += editor.suggest('{first_name} {last_name} works as {a:bert}.')[:30]\n",
    "#professions = list(set(professions))\n",
    "professions = []\n",
    "# professions\n",
    "other_nouns = ['player', 'person', 'friend', 'kid', 'candidate']\n",
    "nouns = list(set(professions + other_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(', '.join(editor.suggest('Is {first_name} {last_name} {a:bert} {noun}?', noun=nouns)[:50]))\n",
    "adjs = ['effective', 'actual', 'American', 'active', 'honest', 'excellent', 'elite', 'acomplished', 'official', 'outstanding', 'experienced', 'independent', 'international', 'aspiring', 'average', 'good', 'amazing', 'exceptional', 'successful', 'accredited', 'English', 'real', 'bad', 'terrible', 'fake', 'unusual', 'influential', 'incompetent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 999 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [00:13<00:00,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      999\n",
      "Fails (rate):    415 (41.5%)\n",
      "\n",
      "Example fails:\n",
      "0.7 ('Is Ryan Long a kid?', 'Is Ryan Long an effective kid?')\n",
      "----\n",
      "1.0 ('Is Danielle Price a friend?', 'Is Danielle Price an average friend?')\n",
      "----\n",
      "1.0 ('Is Amy King a player?', 'Is Amy King an average player?')\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(('Is {first_name} {last_name} {a:noun}?', 'Is {first_name} {last_name} {a:adj} {noun}?'),\n",
    "                noun=nouns,\n",
    "                adj=adjs,\n",
    "                remove_duplicates=True, \n",
    "                nsamples=1000)\n",
    "test = MFT(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43e242b10b049beac302af99739ff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'nTested': 999, 'nFailed': 415, 'nFiltered': 0}, summarizer={'name': 'TESTNAME_PLACEHOLD…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change name, loc, number in only one where orig prediction is duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1332 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 167/167 [00:16<00:00, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      82\n",
      "After filtering: 43 (52.4%)\n",
      "Fails (rate):    41 (95.3%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What will happen if Donald Trump became the president of America ?', 'What will happen if Donald Trump wins the election ?')\n",
      "1.0 ('What will happen if Donald Trump became the president of America ?', 'What will happen if Henry Trump wins the election ?')\n",
      "1.0 ('What will happen if Donald Trump became the president of America ?', 'What will happen if Jose Trump wins the election ?')\n",
      "\n",
      "----\n",
      "1.0 ('According to the Bible , is the second coming of Jesus in the near future ?', 'Is the second coming near ?')\n",
      "1.0 ('According to the Bible , is the second coming of Austin in the near future ?', 'Is the second coming near ?')\n",
      "0.9 ('According to the Bible , is the second coming of Joseph in the near future ?', 'Is the second coming near ?')\n",
      "\n",
      "----\n",
      "1.0 (\"Who runs Trump 's Twitter account ? Does Donald Trump write his own tweets ?\", 'Does Donald Trump really write his own tweets ?')\n",
      "1.0 (\"Who runs Trump 's Twitter account ? Does Donald Trump write his own tweets ?\", 'Does Cody Trump really write his own tweets ?')\n",
      "1.0 (\"Who runs Trump 's Twitter account ? Does Austin Trump write his own tweets ?\", 'Does Donald Trump really write his own tweets ?')\n",
      "\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def wrap_apply_to_each(fn, both=False, *args, **kwargs):\n",
    "    def new_fn(qs, *args, **kwargs):\n",
    "        q1, q2 = qs\n",
    "        ret = []\n",
    "        fnq1 = fn(q1, *args, **kwargs)\n",
    "        fnq2 = fn(q2, *args, **kwargs)\n",
    "        if type(fnq1) != list:\n",
    "            fnq1 = [fnq1]\n",
    "        if type(fnq2) != list:\n",
    "            fnq2 = [fnq2]\n",
    "        ret.extend([(x, str(q2)) for x in fnq1])\n",
    "        ret.extend([(str(q1), x) for x in fnq2])\n",
    "        if both:\n",
    "            ret.extend([(x, x2) for x, x2 in itertools.product(fnq1, fnq2)])\n",
    "        return [x for x in ret if x[0] and x[1]]\n",
    "    return new_fn\n",
    "data = Perturb.perturb(parsed_qs, wrap_apply_to_each(Perturb.change_names), nsamples=1500, first_only=True)\n",
    "expect_fn = Expect.eq(0)\n",
    "expect_fn = Expect.slice_orig(expect_fn, lambda orig, *args: orig == 1)\n",
    "test = DIR(data, expect_fn)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b04e0aeccb44fab41da946b9cc1566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 2, 'nfailed': 41, 'nfiltered': 0}, summarizer={'name': 'TESTNAME_PLACEHOLDER'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
