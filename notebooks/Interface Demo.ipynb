{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = checklist.editor.Editor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom checklist.pred_wrapper import PredictorWrapper\\n\\nimport sys\\nsys.path.appendnd('/home/marcotcr/work/ml-tests/')\\nfrom mltests import model_wrapper\\nsentiment = model_wrapper.ModelWrapper()\\nwrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\\nimport csv\\nr = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\\nlabels = []\\nconfs = []\\nairlines = []\\ntdata = []\\nreasons = []\\nfor row in r:\\n    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\\n    labels.append(sentiment)\\n    confs.append(conf)\\n    airlines.append(airline)\\n    tdata.append(text)\\n    reasons.append(row['negativereason'])\\n\\nmapping = {'negative': 0, 'positive': 2, 'neutral': 1}\\nlabels = np.array([mapping[x] for x in labels]).astype(int)\\n\\n\\nnlp = spacy.load('en_core_web_sm')\\n\\nsentences = tdata\\nparsed_data = list(nlp.pipe(sentences))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MARCO'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.appendnd('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n",
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wtshuang/datasets/models/imdb-iclr_orig\n",
      "Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/wtshuang/datasets/raw/glue_data//imdb/iclr_orig', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_list='None', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=2, no_cuda=False, num_train_epochs=10.0, output_dir='//home/wtshuang/datasets/models//imdb-iclr_orig/', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=1000000000000, seed=42, server_ip='', server_port='', task_name='sst-2', tokenizer_name='', train_batch_size=16, warmup_steps=0, weight_decay=0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Al Michaels said he never saw Frank Gifford lose his cool in 12 yrs working alongside him. Poised amid TV madness https://t.co/uvBolxzwW6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TONGSHUANG'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 180)\n",
    "import spacy\n",
    "mltest_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml-tests\"))\n",
    "sys.path.append(mltest_path)\n",
    "mlfeedback_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml_feedback\"))\n",
    "sys.path.append(mlfeedback_path)\n",
    "\n",
    "from ml_feedback.analysis.sentiment_utils import *\n",
    "# get the model\n",
    "model_names = {\n",
    "    \"orig\": \"imdb-iclr_orig\"\n",
    "}\n",
    "from ml_feedback.analysis.sentiment_utils import *\n",
    "models = load_models_by_name(model_names)\n",
    "\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "sentiment = models[\"orig\"]\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n",
    "\n",
    "\n",
    "from ml_feedback.analysis.sentiment_utils import *\n",
    "\n",
    "df = read_one_dataset(\"semeval_iclr_nonbinary\")\n",
    "if len(df) > 2000:\n",
    "    df = df.sample(2000)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentences = df.sentence.tolist()\n",
    "parsed_data = list(nlp.pipe(sentences))\n",
    "labels = df.label.tolist()\n",
    "parsed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.test_suite import TestSuite\n",
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)\n",
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'weird', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)\n",
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "There is already a test named Sentiment-laden words in context suite. Run with overwrite=True to overwrite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fb067d2b7248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# test = MFT(t.data, labels=t.labels, templates=t.templates)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMFT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment-laden words in context'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Vocabulary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Use positive and negative verbs and adjectives with airline nouns such as seats, pilot, flight, etc. E.g. \"This was a bad flight\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/sourcetree/checklist_ui/checklist/test_suite.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, test, name, capability, description, format_example_fn, print_fn, overwrite)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There is already a test named %s suite. Run with overwrite=True to overwrite'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: There is already a test named Sentiment-laden words in context suite. Run with overwrite=True to overwrite"
     ]
    }
   ],
   "source": [
    "t = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "#t += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=2, save=True)\n",
    "#t += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], labels=0, save=True)\n",
    "#t += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=0, save=True)\n",
    "#t += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=0, save=True)\n",
    "# equivalent to:\n",
    "# test = MFT(t.data, labels=t.labels, templates=t.templates)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'Sentiment-laden words in context', 'Vocabulary', 'Use positive and negative verbs and adjectives with airline nouns such as seats, pilot, flight, etc. E.g. \"This was a bad flight\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 200 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 25/25 [00:02<00:00,  9.71it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83bcce245d5442d82b59771c4bccc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 200, 'nfailed': 0, 'nfiltered': 0}, summarizer={'name': None, 'description': …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.run(new_pp, n=200)\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b3515c390b4d26a25256fca3d735d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 200, 'nfailed': 0, 'nfiltered': 0}, summarizer={'name': 'Sentiment-laden word…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.visual_summary_by_name('Sentiment-laden words in context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4446fe76aeee4e8792cc636465b6141b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.visual_summary_table(capabilities=[\"robust\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']\n",
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['That is an extraordinary aircraft.',\n",
       "  'That is an exceptionally extraordinary aircraft.'],\n",
       " ['That is an incredible crew.', 'That is a rather incredible crew.'],\n",
       " ['It was a good seat.', 'It was an especially good seat.'],\n",
       " ['It is an adorable cabin crew.', 'It is an especially adorable cabin crew.'],\n",
       " ['This is an incredible flight.',\n",
       "  'This is an exceedingly incredible flight.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "#t += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "#t += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "#t += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "t.data[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [00:48<00:00, 10.41it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a34be10b134d77beb831270f80a1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 1913, 'nfailed': 80, 'nfiltered': 0}, summarizer={'name': None, 'description'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "t += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "test = DIR(t.data, monotonic_label, templates=t.templates)\n",
    "description = '''Test is composed of pairs of sentences (x1, x2), where we add an intensifier\n",
    "such as \"really\",or \"very\" to x2 and expect the confidence to NOT go down (with tolerance=0.1). e.g.:\n",
    "We disregard cases where the prediction of x1 is neutral.\n",
    "'''\n",
    "test.run(new_pp)\n",
    "test.visual_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390eca5c463c40178e6098e87a3e0c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 1913, 'nfailed': 80, 'nfiltered': 0}, summarizer={'name': 'intensifiers', 'de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "description = '''Test is composed of pairs of sentences (x1, x2), where we add an intensifier\n",
    "such as \"really\",or \"very\" to x2 and expect the confidence to NOT go down (with tolerance=0.1). e.g.:\n",
    "We disregard cases where the prediction of x1 is neutral.\n",
    "'''\n",
    "suite.add(test, 'intensifiers', 'Vocabulary', description)\n",
    "suite.visual_summary_by_name(\"intensifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "Sentiment-laden words in context\n",
      "Test cases:      2964\n",
      "Fails (rate):    1 (0.0%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.9 0.0 That seat was extraordinary.\n",
      "----\n",
      "\n",
      "\n",
      "intensifiers\n",
      "Test cases:      2000\n",
      "After filtering: 1997 (99.8%)\n",
      "Fails (rate):    88 (4.4%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.0 0.9 It is an average staff.\n",
      "1.0 0.0 0.0 It is an incredibly average staff.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 We dread this flight.\n",
      "0.3 0.7 0.0 We particularly dread this flight.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 It is a sad crew.\n",
      "0.0 0.0 1.0 It is a quite sad crew.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [00:54<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      2000\n",
      "After filtering: 53 (2.6%)\n",
      "Fails (rate):    15 (28.3%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) The flight was aggressive.\n",
      "2 (1.0) The flight was mostly aggressive.\n",
      "\n",
      "----\n",
      "2 (0.8) The flight was aggressive.\n",
      "2 (1.0) The flight was somewhat aggressive.\n",
      "\n",
      "----\n",
      "2 (0.7) This plane was average.\n",
      "2 (1.0) This plane was reasonably average.\n",
      "\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)\n",
    "\n",
    "\n",
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'a little', 'a bit', 'slightly']\n",
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)\n",
    "\n",
    "data = editor.template(['{it} {air_noun} {be} {pos_adj}.', '{it} {air_noun} {be} {red} {pos_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "data += editor.template(['{it} {air_noun} {be} {neg_adj}.', '{it} {air_noun} {be} {red} {neg_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "test = DIR(data, monotonic_label_down)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n",
    "test.visual_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972c97921ce640cd9f487b16fd1274dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TemplateEditor(bert_suggests=[('always', 'interview'), ('always', 'opinion'), ('also', 'explanation'), ('alway…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "\n",
    "#template_strs, tagged_keys, tag_dict, bert_suggests = \n",
    "editor.visual_suggest(\n",
    "    '{i} {mask} {pos_verb} {a:mask}.', \n",
    "    i=['I', 'We'], \n",
    "    the=['this', 'that', 'the']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('really', 'lot'), ('just', 'lot'), ('really', 'book')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.temp_selects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246100f64436421e9010634b9e646f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TemplateEditor(bert_suggests=['version', 'picture', 'story', 'day', 'game', 'one', 'language', 'place', 'case'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]\n",
    "comp_pairs = list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))\n",
    "\n",
    "\n",
    "#template_strs, tagged_keys, tag_dict, bert_suggests = \n",
    "editor.visual_suggest(\n",
    "    {\n",
    "        \"C\": '{first_name} is {a:comp[0]} {mask}.',\n",
    "        \"Q\": 'Who is {comp[1]}?',\n",
    "        \"A\": '{first_name}',\n",
    "    },\n",
    "    comp=comp_pairs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52c3e063a064627be2b097fb1d12839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TemplateEditor(bert_suggests=['Muslim', 'innocent', 'gay', 'homosexual', 'dead', 'Jewish', 'suspect', 'pregnan…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]\n",
    "comp_pairs = list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))\n",
    "\n",
    "\n",
    "#template_strs, tagged_keys, tag_dict, bert_suggests = \n",
    "editor.visual_suggest((\n",
    "    'Is {first_name1} {last_name1} {a:mask}?',\n",
    "    'Is {first_name2} {last_name2} {mask}?',\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['innocent', 'Muslim', 'gay']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.temp_selects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import MFT\n",
    "from checklist.inv_dir import INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MARCO'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "model = model_wrapper.ModelWrapper()\n",
    "new_pp = PredictorWrapper.wrap_softmax(model.predict_proba)\n",
    "\n",
    "\n",
    "qs = []\n",
    "labels = []\n",
    "all_questions = set()\n",
    "for x in open('/home/marcotcr/datasets/glue/glue_data/QQP/dev.tsv').readlines()[1:]:\n",
    "    try:\n",
    "        q1, q2, label = x.strip().split('\\t')[3:]\n",
    "    except:\n",
    "        print(x)\n",
    "        continue\n",
    "    all_questions.add(q1)\n",
    "    all_questions.add(q2)\n",
    "    qs.append((q1, q2))\n",
    "    labels.append(label)\n",
    "labels = np.array(labels).astype(int)\n",
    "\n",
    "import pickle\n",
    "spacy_map =  pickle.load(open('/home/marcotcr/tmp/processed_qqp.pkl', 'rb'))\n",
    "parsed_qs = [(spacy_map[q[0]], spacy_map[q[1]]) for q in qs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wtshuang/datasets/models/QQP-all\n",
      "Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/wtshuang/datasets/raw/glue_data//QQP/all', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_list='None', learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=2, no_cuda=False, num_train_epochs=3.0, output_dir='//home/wtshuang/datasets/models//QQP-all/', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=1000000000000, seed=42, server_ip='', server_port='', task_name='qqp', tokenizer_name='', train_batch_size=16, warmup_steps=0, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TONGSHUANG'S LOADING\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 180)\n",
    "\n",
    "mltest_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml-tests\"))\n",
    "sys.path.append(mltest_path)\n",
    "mlfeedback_path = os.path.abspath(os.path.expanduser(\"~/sourcetree/ml_feedback\"))\n",
    "sys.path.append(mlfeedback_path)\n",
    "\n",
    "from ml_feedback.analysis.qqp_utils import *\n",
    "\n",
    "\n",
    "# get the model\n",
    "model_names = {\n",
    "    \"qqp\": \"QQP-all\",\n",
    "}\n",
    "from ml_feedback.analysis.qqp_utils import *\n",
    "models = load_models_by_name(model_names)\n",
    "\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "model = models[\"qqp\"]\n",
    "new_pp = PredictorWrapper.wrap_softmax(model.predict_proba)\n",
    "\n",
    "\n",
    "\n",
    "# getting the data\n",
    "\n",
    "df = read_one_dataset(\"quora\", max_size=2000)\n",
    "parsed_qs = list(zip(\n",
    "    nlp.pipe(df.question1.tolist()), \n",
    "    nlp.pipe(df.question2.tolist())\n",
    "))\n",
    "labels = df.is_duplicate.tolist()\n",
    "parsed_qs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountant, engineer, journalist, attorney, editor, artist, architect, actor, interpreter, analyst, intern, assistant, escort, organizer, economist, actress, administrator, investigator, agent, investor, secretary, author, entrepreneur, executive, advisor, auditor, educator, instructor, employee, adviser\n"
     ]
    }
   ],
   "source": [
    "professions = editor.suggest('{first_name} works as {a:bert}.')[:30]\n",
    "print(', '.join(professions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#professions = editor.suggest('{first_name} works as {a:bert}.')[:30]\n",
    "#professions += editor.suggest('{first_name} {last_name} works as {a:bert}.')[:30]\n",
    "#professions = list(set(professions))\n",
    "professions = []\n",
    "# professions\n",
    "other_nouns = ['player', 'person', 'friend', 'kid', 'candidate']\n",
    "nouns = list(set(professions + other_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(', '.join(editor.suggest('Is {first_name} {last_name} {a:bert} {noun}?', noun=nouns)[:50]))\n",
    "adjs = ['effective', 'actual', 'American', 'active', 'honest', 'excellent', 'elite', 'acomplished', 'official', 'outstanding', 'experienced', 'independent', 'international', 'aspiring', 'average', 'good', 'amazing', 'exceptional', 'successful', 'accredited', 'English', 'real', 'bad', 'terrible', 'fake', 'unusual', 'influential', 'incompetent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 999 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [00:13<00:00,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      999\n",
      "Fails (rate):    415 (41.5%)\n",
      "\n",
      "Example fails:\n",
      "0.7 ('Is Ryan Long a kid?', 'Is Ryan Long an effective kid?')\n",
      "----\n",
      "1.0 ('Is Danielle Price a friend?', 'Is Danielle Price an average friend?')\n",
      "----\n",
      "1.0 ('Is Amy King a player?', 'Is Amy King an average player?')\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(('Is {first_name} {last_name} {a:noun}?', 'Is {first_name} {last_name} {a:adj} {noun}?'),\n",
    "                noun=nouns,\n",
    "                adj=adjs,\n",
    "                remove_duplicates=True, \n",
    "                nsamples=1000)\n",
    "test = MFT(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43e242b10b049beac302af99739ff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'nTested': 999, 'nFailed': 415, 'nFiltered': 0}, summarizer={'name': 'TESTNAME_PLACEHOLD…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change name, loc, number in only one where orig prediction is duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1332 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 167/167 [00:16<00:00, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      82\n",
      "After filtering: 43 (52.4%)\n",
      "Fails (rate):    41 (95.3%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What will happen if Donald Trump became the president of America ?', 'What will happen if Donald Trump wins the election ?')\n",
      "1.0 ('What will happen if Donald Trump became the president of America ?', 'What will happen if Henry Trump wins the election ?')\n",
      "1.0 ('What will happen if Donald Trump became the president of America ?', 'What will happen if Jose Trump wins the election ?')\n",
      "\n",
      "----\n",
      "1.0 ('According to the Bible , is the second coming of Jesus in the near future ?', 'Is the second coming near ?')\n",
      "1.0 ('According to the Bible , is the second coming of Austin in the near future ?', 'Is the second coming near ?')\n",
      "0.9 ('According to the Bible , is the second coming of Joseph in the near future ?', 'Is the second coming near ?')\n",
      "\n",
      "----\n",
      "1.0 (\"Who runs Trump 's Twitter account ? Does Donald Trump write his own tweets ?\", 'Does Donald Trump really write his own tweets ?')\n",
      "1.0 (\"Who runs Trump 's Twitter account ? Does Donald Trump write his own tweets ?\", 'Does Cody Trump really write his own tweets ?')\n",
      "1.0 (\"Who runs Trump 's Twitter account ? Does Austin Trump write his own tweets ?\", 'Does Donald Trump really write his own tweets ?')\n",
      "\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def wrap_apply_to_each(fn, both=False, *args, **kwargs):\n",
    "    def new_fn(qs, *args, **kwargs):\n",
    "        q1, q2 = qs\n",
    "        ret = []\n",
    "        fnq1 = fn(q1, *args, **kwargs)\n",
    "        fnq2 = fn(q2, *args, **kwargs)\n",
    "        if type(fnq1) != list:\n",
    "            fnq1 = [fnq1]\n",
    "        if type(fnq2) != list:\n",
    "            fnq2 = [fnq2]\n",
    "        ret.extend([(x, str(q2)) for x in fnq1])\n",
    "        ret.extend([(str(q1), x) for x in fnq2])\n",
    "        if both:\n",
    "            ret.extend([(x, x2) for x, x2 in itertools.product(fnq1, fnq2)])\n",
    "        return [x for x in ret if x[0] and x[1]]\n",
    "    return new_fn\n",
    "data = Perturb.perturb(parsed_qs, wrap_apply_to_each(Perturb.change_names), nsamples=1500, first_only=True)\n",
    "expect_fn = Expect.eq(0)\n",
    "expect_fn = Expect.slice_orig(expect_fn, lambda orig, *args: orig == 1)\n",
    "test = DIR(data, expect_fn)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b04e0aeccb44fab41da946b9cc1566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 2, 'nfailed': 41, 'nfiltered': 0}, summarizer={'name': 'TESTNAME_PLACEHOLDER'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
