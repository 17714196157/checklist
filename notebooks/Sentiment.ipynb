{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<checklist.text_generation.TextGenerator at 0x7fd20a030668>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "editor.tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template((('{first_name} is a good guy', 'Who is a good guy?'), '{first_name}', '3') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great, good, excellent, amazing, bad, terrible, incredible, awful, expensive, extraordinary, new, interesting, wonderful, nice, fantastic, real, important, awesome, unusual, American, different, poor, exceptional, beautiful, ordinary, little, impressive, special, enormous, actual, easy, big, horrible, huge, fine, lousy, perfect, international, decent, terrific\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('It was {a:bert} {air_noun}.')[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'tough', 'weird', 'aggressive', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liked, enjoyed, like, appreciate, appreciated, enjoy, loved, love, miss, missed, hate, needed, likes, wanted, got, recommend, prefer, admired, value, need, respected, want, dislike, enjoying, respect, enjoys, admire, was, feel, liking, dig, hated, mean, preferred, underestimated, disliked, used, get, use, valued, understand, did, found, adore, helped, dug, trust, remember, noticed, about, tried, hit, had, regret, took, felt, praised, cherish, have, left, loves, understood, do, LOVE, compliment, trusted, bought, thank, treasure, applaud, support, rate, know, commend, credit, underestimate, see, supported, think, thought, all, saw, picked, believe, beat, welcome, considered, impressed, improved, met, chose, thanks, deserved, envy, blame, for, help, packed, recommended, in, follow, choose, loving, misses, is, into, lost, consider, changed, worked, meant, leave, joined, wish, regretted, made, earned, rocked, cherished, take, worth, finished, experienced, are, fancy, handled, thanked, followed, saved, spoiled, told, surprised, salute, meet, pleased, try, recruited, crave, owe, reviewed, doubt, values, forgive, hired, hope, remembered, stressed, praise, respects, just, LIKE, knew, am, welcomed, tested, sold, Love, Like, survived, ate, planned, drove, ordered, sampled, crashed, managed, brought, ended, deserve, rode, eat, applauded, regrets, spent, heard, grabbed, needs, tasted, coveted, ruined, timed, delayed, caught, dreaded, watched, forgot, anticipated, canceled, own, rushed, cooked, taste, delivered, prefers, filled, deserves, shortened, mind, received, edited\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('I really {bert} the {air_noun}.')[:200]))\n",
    "# print()\n",
    "# print(', '.join(editor.suggest('I {bert} the {air_noun}.')[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<class 'checklist.test_types.MFT'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8e54404bb9a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# test.run(new_pp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# test.summary(n=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'individual positive words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Vocabulary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TODO_DESCRIPTION'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/checklist/checklist/test_suite.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, test, name, capability, description, format_example_fn, print_fn, overwrite)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mDIR\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DIR'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         }\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtypez\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'capability'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: <class 'checklist.test_types.MFT'>"
     ]
    }
   ],
   "source": [
    "test = MFT(pos_adj + pos_verb_present + pos_verb_past, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual positive words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(neg_adj + neg_verb_present + neg_verb_past, labels=0)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual negative words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(neutral_adj + neutral_verb_present + neutral_verb_past, labels=1)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual neutral words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=2, save=True)\n",
    "t += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=0, save=True)\n",
    "# equivalent to:\n",
    "# test = MFT(t.data, labels=t.labels, templates=t.templates)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'sentiment words in context', 'Vocabulary', 'TODO_DESCRIPTION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {be} {neutral_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{it} {be} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{i} {neutral_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'neutral words in context', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very , really , extremely , quite , pretty , absolutely , incredibly , truly , actually , a , rather , always , most , exceptionally , genuinely , surprisingly , particularly , damn , especially , amazingly , generally , extraordinarily , unbelievably , utterly , overall , equally , darn , super , exceedingly , unusually , obviously , entirely , otherwise , enormously , overwhelmingly , immensely , just , totally , insanely , absolute , altogether , undeniably , unexpectedly , completely , real , VERY , almost , all , increasingly , already\n"
     ]
    }
   ],
   "source": [
    "print(' , '.join(editor.suggest('{it} {be} {a:bert} {pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really, always, certainly, also, truly, greatly, all, definitely, highly, personally, we, thoroughly, especially, so, I, both, much, absolutely, particularly, sincerely, very, still, just, sure, genuinely, fully, people, most, strongly, you, deeply, obviously, clearly, do, have, and, never, quite, did, actually, would, totally, they, rather, seriously, dearly, again, completely, will, simply, honestly, should, too, now, extremely, guys, immediately, must, even, already, who, can, had, immensely, tremendously, generally, surely, profoundly, everyone, vastly, might, friends, does, further, may, could, to, many, REALLY, family, only, hugely, has, ..., incredibly, well, long, then, probably, ultimately, often, kindly, are, positively, therefore, indeed, sorely, that, actively, usually\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('{i} {bert} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "t += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "test = DIR(t.data, monotonic_label, templates=t.templates)\n",
    "suite.add(test, 'intensifiers', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'a little', 'a bit', 'slightly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {air_noun} {be} {pos_adj}.', '{it} {air_noun} {be} {red} {pos_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000, save=True)\n",
    "t += editor.template(['{it} {air_noun} {be} {neg_adj}.', '{it} {air_noun} {be} {red} {neg_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000, save=True)\n",
    "test = DIR(t.data, monotonic_label_down, templates=t.templates)\n",
    "suite.add(test, 'reducers', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVariance: change neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = set(\n",
    "    ['.', 'the', 'The', ',', 'a', 'A', 'and', 'of', 'to', 'it', 'that', 'in',\n",
    "     'this', 'for',  'you', 'there', 'or', 'an', 'by', 'about', 'flight', 'my',\n",
    "     'in', 'of', 'have', 'with', 'was', 'at', 'it', 'get', 'from', 'this', 'Flight', 'plane'\n",
    "    ])\n",
    "forbidden = set(['No', 'no', 'Not', 'not', 'Nothing', 'nothing', 'without'] + pos_adj + neg_adj + pos_verb_present + pos_verb_past + neg_verb_present + neg_verb_past)\n",
    "def change_neutral(d):\n",
    "#     return d.text\n",
    "    examples = []\n",
    "    subs = []\n",
    "    words_in = [x for x in d.capitalize().split() if x in neutral_words]\n",
    "    if not words_in:\n",
    "        return None\n",
    "    for w in words_in:\n",
    "        suggestions = [x for x in editor.suggest_replace(d, w, beam_size=5, words_and_sentences=True) if x[0] not in forbidden]\n",
    "        examples.extend([x[1] for x in suggestions])\n",
    "        subs.extend(['%s -> %s' % (w, x[0]) for x in suggestions])\n",
    "    if examples:\n",
    "        idxs = np.random.choice(len(examples), min(len(examples), 10), replace=False)\n",
    "        return [examples[i] for i in idxs]#, [subs[i] for i in idxs])\n",
    "# Perturb.perturb(parsed_data[:5], perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, change_neutral, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change neutral words with BERT', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = editor.template('I {pos_verb_present} you.').data\n",
    "positive += editor.template('You are {pos_adj}.').data\n",
    "positive += ['I would fly with you again.']\n",
    "positive.remove('You are happy.')\n",
    "negative = editor.template('I {neg_verb_present} you.').data\n",
    "negative += editor.template('You are {neg_adj}.').data\n",
    "negative += ['Never flying with you again.']\n",
    "def add_phrase_function(phrases):\n",
    "    def pert(d):\n",
    "        while d[-1].pos_ == 'PUNCT':\n",
    "            d = d[:-1]\n",
    "        d = d.text\n",
    "        ret = [d + '. ' + x for x in phrases]\n",
    "        idx = np.random.choice(len(ret), 10, replace=False)\n",
    "        ret = [ret[i] for i in idx]\n",
    "        return ret\n",
    "    return pert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_1 = Expect.monotonic(label=2, increasing=True, tolerance=0.1)\n",
    "monotonic_1_down = Expect.monotonic(label=2, increasing=False, tolerance=0.1)\n",
    "monotonic_0 = Expect.monotonic(label=0, increasing=True, tolerance=0.1)\n",
    "monotonic_0_down = Expect.monotonic(label=0, increasing=False, tolerance=0.1)\n",
    "goes_up = Expect.combine_and(monotonic_1, monotonic_0_down)\n",
    "goes_down = Expect.combine_and(monotonic_1_down, monotonic_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check this expectation function\n",
    "t = Perturb.perturb(parsed_data, add_phrase_function(positive), nsamples=500)\n",
    "# test = DIR(data, monotonic_1)\n",
    "test = DIR(t.data, goes_up)\n",
    "suite.add(test, 'add positive phrases', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp, overwrite=True)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check this expectation function\n",
    "t = Perturb.perturb(parsed_data, add_phrase_function(negative), nsamples=500)\n",
    "# test = DIR(data, monotonic_1_down)\n",
    "test = DIR(t.data, goes_down)\n",
    "suite.add(test, 'add negative phrases', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp, overwrite=True)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: robustness\n",
    "### INVariance: adding irrelevant stuff before and after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def random_string(n):\n",
    "    return ''.join(np.random.choice([x for x in string.ascii_letters + string.digits], n))\n",
    "def random_url(n=6):\n",
    "    return 'https://t.co/%s' % random_string(n)\n",
    "def random_handle(n=6):\n",
    "    return '@%s' % random_string(n)\n",
    "\n",
    "# data['sentence']\n",
    "\n",
    "def add_irrelevant(sentence):\n",
    "    urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]\n",
    "    irrelevant_before = ['@airline '] + urls_and_handles\n",
    "    irrelevant_after = urls_and_handles \n",
    "    rets = ['%s %s' % (x, sentence) for x in irrelevant_before ]\n",
    "    rets += ['%s %s' % (sentence, x) for x in irrelevant_after]\n",
    "    return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, add_irrelevant, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'add urls and handles', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation, contractions, typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.punctuation, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'punctuation', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=1)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'typos', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=2)\n",
    "test = INV(t.data)\n",
    "suite.add(test, '2 typos', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.contractions, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'contractions', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change names', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_location, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change locations', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_number, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change numbers', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: temporal awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate', 'dislike', 'regret', 'abhor', 'dread', 'despise']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{neg_verb_present}').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = ['but', 'even though', 'although', '']\n",
    "t = editor.template(['I used to think this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} I used to think it was {neg_adj}.',\n",
    "                                 'In the past I thought this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} in the past I thought it was {neg_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500, save=True, labels=2)\n",
    "t += editor.template(['I used to {neg_verb_present} this airline, {change} now I {pos_verb_present} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} I used to {neg_verb_present} it.',\n",
    "                                 'In the past I would {neg_verb_present} this airline, {change} now I {pos_verb} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} in the past I would {neg_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500, save=True, labels=2)\n",
    "\n",
    "t += editor.template(['I used to think this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} I used to think it was {pos_adj}.',\n",
    "                                 'In the past I thought this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} in the past I thought it was {pos_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500, save=True, labels=0)\n",
    "t += editor.template(['I used to {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} I used to {pos_verb_present} it.',\n",
    "                                 'In the past I would {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} in the past I would {pos_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500, save=True, labels=0)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'used to, but now', 'Temporal', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used to should reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {be} {a:adj} {air_noun}.', 'I used to think {it} {be} {a:adj} {air_noun}.'], it=['it', 'this', 'that'], be=['is', 'was'], adj=editor.lexicons['pos_adj'] + editor.lexicons['neg_adj'], save=True)\n",
    "t += editor.template(['{i} {verb} {the} {air_noun}.', '{i} used to {verb} {the} {air_noun}.'], i=['I', 'We'], the=['this', 'that', 'the'], verb=editor.lexicons['pos_verb_present'] + editor.lexicons['neg_verb_present'], save=True)\n",
    "test = DIR(t.data, monotonic_label_down, templates=t.templates)\n",
    "# test = MFT(data, labels=2)\n",
    "suite.add(test, '\"used to\" should reduce', 'Temporal', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect:fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['male', 'female', 'first_name', 'first_pronoun', 'last_name', 'country', 'nationality', 'city', 'religion', 'religion_adj', 'sexual_adj', 'air_noun', 'pos_adj', 'neg_adj', 'neutral_adj', 'pos_verb_present', 'neg_verb_present', 'neutral_verb_present', 'pos_verb_past', 'neg_verb_past', 'neutral_verb_past', 'pos_verb', 'neg_verb', 'neutral_verb'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a Christian',\n",
       " 'a Protestant',\n",
       " 'a Roman Catholic',\n",
       " 'an Eastern Orthodox',\n",
       " 'an Anglican',\n",
       " 'a Jew',\n",
       " 'an Orthodox Jew',\n",
       " 'a Muslim',\n",
       " 'a Sunni',\n",
       " \"a Shi'a\",\n",
       " 'an Ahmadiyya',\n",
       " 'a Buddhist',\n",
       " 'a Zoroastrian',\n",
       " 'a Hindu',\n",
       " 'a Sikh',\n",
       " 'a Shinto',\n",
       " \"a Baha'i\",\n",
       " 'a Taoist',\n",
       " 'a Confucian',\n",
       " 'a Jain',\n",
       " 'an Atheist',\n",
       " 'an Agnostic']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{a:religion_adj}').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = {\n",
    "    'race': ['a black','a hispanic', 'a white', 'an asian'],\n",
    "    'sexual': editor.template('{a:sexual_adj}').data,\n",
    "    'religion': editor.template('{a:religion_adj}').data,\n",
    "    'nationality': editor.template('{a:nationality}').data[:20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "sexual\n",
      "religion\n",
      "nationality\n"
     ]
    }
   ],
   "source": [
    "for p, vals in protected.items():\n",
    "    print(p)\n",
    "    t = editor.template(['{male} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=300, save=True)\n",
    "    t += editor.template(['{female} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=300, save=True)\n",
    "    test = INV(t.data, threshold=0.1, templates=t.templates)\n",
    "    suite.add(test, 'protected: %s' % p, 'Fairness', 'TODO_DESCRIPTION')\n",
    "#     test.run(new_pp)\n",
    "#     test.summary(n=3)\n",
    "#     print()\n",
    "#     preds = np.array(test.results.preds)\n",
    "#     for i, x in enumerate(vals):\n",
    "#         print('%.2f %s' % (preds[:, i].mean(), vals[i]))\n",
    "#     print()\n",
    "#     print()\n",
    "#     print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {pos_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "t += editor.template('No one {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'simple negations: negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {neg_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {neg_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "t += editor.template('No one {neg_verb_present}s {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "is_not_0 = lambda x, pred, *args: pred != 0\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'simple negations: not negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {neutral_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {neutral_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'simple negations: not neutral is still neutral', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {pos_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {pos_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'simple negations: but I did not (negative)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {neg_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {neg_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'simple negations: but I did not (not negative)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {neutral_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {neutral_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'simple negations: but it was not (neutral)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harder: negation with neutral in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neg = neg[:-1]\n",
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {pos_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:pos_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {pos_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neg_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:neg_adj} {air_noun}.',neutral=neutral,  neg=['i don\\'t think', 'i can\\'t say', 'i wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {neg_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['i don\\'t think', 'i can\\'t say', 'i wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle, not negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neutral_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:neutral_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {neutral_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle, neutral', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: SRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my opinion is more important than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = [' but', '']\n",
    "templates = ['Some people think you are {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} some people think you are {neg_adj}.',\n",
    "             'I had heard you were {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} I had heard you were {neg_adj}.',\n",
    "             ]\n",
    "t = editor.template(templates, change=change, unroll=True, labels=2, save=True)\n",
    "templates = ['{others} {neg_verb_present} you,{change} I {pos_verb_present} you.',\n",
    "             'I {pos_verb_present} you,{change} {others} {neg_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "t += editor.template(templates, others=others, change=change, unroll=True, labels=2, save=True)\n",
    "\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} some people think you are {pos_adj}.',\n",
    "             'I had heard you were {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} I had heard you were {pos_adj}.',\n",
    "             ]\n",
    "t += editor.template(templates, change=change, unroll=True, labels=0, save=True)\n",
    "templates = ['{others} {pos_verb_present} you,{change} I {neg_verb_present} you.',\n",
    "             'I {neg_verb_present} you,{change} {others} {pos_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "t += editor.template(templates, others=others, change=change, unroll=True, labels=0, save=True)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'my opinion is what matters', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q & a form: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=2)\n",
    "t += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=2)\n",
    "t += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=2)\n",
    "t += editor.template('Do I think {it} {air_noun} {be} {neg_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=0)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'Q & A: yes', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'Q & A: yes (neutral)', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {air_noun} {be} {neg_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=1)\n",
    "t += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=1)\n",
    "t += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=1)\n",
    "allow_for_neutral = lambda x, pred, _, label, _2 : pred != 0 if label == 1 else pred == label\n",
    "test = MFT(t.data, Expect.single(allow_for_neutral), labels=t.labels, templates=t.templates)\n",
    "suite.add(test, 'Q & A: no', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'Q & A: no (neutral)', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.save('/home/marcotcr/tmp/sentiment_suite.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running individual positive words\n",
      "Predicting 34 examples\n",
      "Running individual negative words\n",
      "Predicting 37 examples\n",
      "Running individual neutral words\n",
      "Predicting 13 examples\n",
      "Running sentiment words in context\n",
      "Predicting 8970 examples\n",
      "Running neutral words in context\n",
      "Predicting 1716 examples\n",
      "Running intensifiers\n",
      "Predicting 4000 examples\n",
      "Running reducers\n",
      "Predicting 4000 examples\n",
      "Running change neutral words with BERT\n",
      "Predicting 4917 examples\n",
      "Running add positive phrases\n",
      "Predicting 5500 examples\n",
      "Running add negative phrases\n",
      "Predicting 5500 examples\n",
      "Running add urls and handles\n",
      "Predicting 11000 examples\n",
      "Running punctuation\n",
      "Predicting 1159 examples\n",
      "Running typos\n",
      "Predicting 1000 examples\n",
      "Running 2 typos\n",
      "Predicting 1000 examples\n",
      "Running contractions\n",
      "Predicting 2079 examples\n",
      "Running change names\n",
      "Predicting 3641 examples\n",
      "Running change locations\n",
      "Predicting 9999 examples\n",
      "Running change numbers\n",
      "Predicting 11000 examples\n",
      "Running used to, but now\n",
      "Predicting 8000 examples\n",
      "Running \"used to\" should reduce\n",
      "Predicting 9048 examples\n",
      "Running protected: race\n",
      "Predicting 2400 examples\n",
      "Running protected: sexual\n",
      "Predicting 8400 examples\n",
      "Running protected: religion\n",
      "Predicting 13200 examples\n",
      "Running protected: nationality\n",
      "Predicting 12000 examples\n",
      "Running simple negations: negative\n",
      "Predicting 6318 examples\n",
      "Running simple negations: not negative\n",
      "Predicting 7254 examples\n",
      "Running simple negations: not neutral is still neutral\n",
      "Predicting 2496 examples\n",
      "Running simple negations: but I did not (negative)\n",
      "Predicting 2106 examples\n",
      "Running simple negations: but I did not (not negative)\n",
      "Predicting 2418 examples\n",
      "Running simple negations: but it was not (neutral)\n",
      "Predicting 858 examples\n",
      "Running negation with neutral in the middle\n",
      "Predicting 1000 examples\n",
      "Running negation with neutral in the middle, not negative\n",
      "Predicting 1000 examples\n",
      "Running negation with neutral in the middle, neutral\n",
      "Predicting 1000 examples\n",
      "Running my opinion is what matters\n",
      "Predicting 9136 examples\n",
      "Running Q & A: yes\n",
      "Predicting 7956 examples\n",
      "Running Q & A: yes (neutral)\n",
      "Predicting 1560 examples\n",
      "Running Q & A: no\n",
      "Predicting 7956 examples\n",
      "Running Q & A: no (neutral)\n",
      "Predicting 1560 examples\n"
     ]
    }
   ],
   "source": [
    "suite.run(new_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "individual positive words\n",
      "Test cases:      34\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "individual negative words\n",
      "Test cases:      37\n",
      "Fails (rate):    2 (5.4%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) tough\n",
      "----\n",
      "2 (1.0) aggressive\n",
      "----\n",
      "\n",
      "\n",
      "individual neutral words\n",
      "Test cases:      13\n",
      "Fails (rate):    13 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) commercial\n",
      "----\n",
      "2 (1.0) found\n",
      "----\n",
      "2 (1.0) Indian\n",
      "----\n",
      "\n",
      "\n",
      "sentiment words in context\n",
      "Test cases:      8970\n",
      "Fails (rate):    173 (1.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) The aircraft was aggressive.\n",
      "----\n",
      "2 (1.0) The crew was aggressive.\n",
      "----\n",
      "1 (0.8) It is an average plane.\n",
      "----\n",
      "\n",
      "\n",
      "neutral words in context\n",
      "Test cases:      1716\n",
      "Fails (rate):    1632 (95.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) This customer service was commercial.\n",
      "----\n",
      "2 (1.0) This pilot was international.\n",
      "----\n",
      "0 (1.0) This is a commercial cabin crew.\n",
      "----\n",
      "\n",
      "\n",
      "intensifiers\n",
      "Test cases:      2000\n",
      "After filtering: 1996 (99.8%)\n",
      "Fails (rate):    20 (1.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) It was a hard cabin crew.\n",
      "2 (1.0) It was an amazingly hard cabin crew.\n",
      "\n",
      "----\n",
      "0 (1.0) This is a tough crew.\n",
      "2 (1.0) This is an exceedingly tough crew.\n",
      "\n",
      "----\n",
      "0 (1.0) It was a tough service.\n",
      "2 (1.0) It was an unbelievably tough service.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "reducers\n",
      "Test cases:      2000\n",
      "After filtering: 6 (0.3%)\n",
      "Fails (rate):    2 (33.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) That crew is tough.\n",
      "0 (1.0) That crew is kinda tough.\n",
      "\n",
      "----\n",
      "0 (0.8) This crew was tough.\n",
      "0 (1.0) This crew was probably tough.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change neutral words with BERT\n",
      "Test cases:      500\n",
      "Fails (rate):    42 (8.4%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) @united rarely ceases to amaze...for the worse.  i hope this is the last time i \"have\" to fly with you.\n",
      "1 (0.9) @united rarely ceases to amaze...for the worse.  i hope this is the last time i \"have\" to fly before you.\n",
      "0 (0.8) @united rarely ceases to amaze...for the worse.  i hope this is the last time i \"have\" to fly over you.\n",
      "\n",
      "----\n",
      "2 (1.0) @AmericanAir it's always nice coming home but I wish you'd fly LAX-MAD and keep me away from Iberia 😜✈️ #GoingForGreat\n",
      "1 (0.9) @AmericanAir it's always nice coming home but I wish you'd fly LAX-MAD or keep me away from Iberia 😜✈️ #GoingForGreat\n",
      "\n",
      "----\n",
      "0 (0.8) @SouthwestAir took delivery of N8661A, a new Boeing 737-8H4 yesterday. http://t.co/5z9STyUQJ3 #DFW #DAL #airlines\n",
      "1 (0.6) @SouthwestAir took delivery of N8661A, our new Boeing 737-8H4 yesterday. http://t.co/5z9STyUQJ3 #DFW #DAL #airlines\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "add positive phrases\n",
      "Test cases:      500\n",
      "After filtering: 152 (30.4%)\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "add negative phrases\n",
      "Test cases:      500\n",
      "After filtering: 385 (77.0%)\n",
      "Fails (rate):    68 (17.7%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) @SouthwestAir flying back to Denver tomorrow. Should I rebook?\n",
      "1 (0.5) @SouthwestAir flying back to Denver tomorrow. Should I rebook. You are aggressive.\n",
      "\n",
      "----\n",
      "0 (0.8) @united #787 to begin Newark-London Heathrow route on May 6 @PANYNJ @BoeingAirplanes @FroschTravel http://t.co/RdJ1MwFLg1\n",
      "2 (1.0) @united #787 to begin Newark-London Heathrow route on May 6 @PANYNJ @BoeingAirplanes @FroschTravel http://t.co/RdJ1MwFLg1. You are aggressive.\n",
      "\n",
      "----\n",
      "0 (1.0) @united please explain why I need to pay bag fees twice, equally $1200, because a delayed flight resulted in bags being rechecked overnight?\n",
      "0 (0.8) @united please explain why I need to pay bag fees twice, equally $1200, because a delayed flight resulted in bags being rechecked overnight. You are tough.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robustness\n",
      "\n",
      "add urls and handles\n",
      "Test cases:      500\n",
      "Fails (rate):    65 (13.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) @SouthwestAir may I have my Companion pass please.\n",
      "1 (0.9) @iFwGEM @SouthwestAir may I have my Companion pass please.\n",
      "1 (0.6) @SouthwestAir may I have my Companion pass please. @Y5Vdc4\n",
      "\n",
      "----\n",
      "1 (0.9) @jetblue it's time for a direct flight from #JFK to #PITT.... @ Official JetBlue Terminal 5 - New York… http://t.co/QygXGmd3Sn\n",
      "0 (0.7) @qDu0Si @jetblue it's time for a direct flight from #JFK to #PITT.... @ Official JetBlue Terminal 5 - New York… http://t.co/QygXGmd3Sn\n",
      "0 (1.0) https://t.co/mxPfF5 @jetblue it's time for a direct flight from #JFK to #PITT.... @ Official JetBlue Terminal 5 - New York… http://t.co/QygXGmd3Sn\n",
      "\n",
      "----\n",
      "2 (0.7) @SouthwestAir F5R3ZZ\n",
      "0 (1.0) https://t.co/Sh0JZK @SouthwestAir F5R3ZZ\n",
      "0 (1.0) https://t.co/6bYhKM @SouthwestAir F5R3ZZ\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "punctuation\n",
      "Test cases:      500\n",
      "Fails (rate):    22 (4.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) @AmericanAir I have a Cancelled Flighted flight tomorrow morning and the 800 number did nothing! Please! I just want to go home!\n",
      "1 (1.0) @AmericanAir I have a Cancelled Flighted flight tomorrow morning and the 800 number did nothing! Please! I just want to go home.\n",
      "\n",
      "----\n",
      "0 (0.9) @USAirways \"Airport snow removal method #22...\"\n",
      "Keep up the good work folks, this is where Cessna's become 747's! http://t.co/0JJT4X3YxG\n",
      "2 (0.7) @USAirways \"Airport snow removal method #22...\"\n",
      "Keep up the good work folks, this is where Cessna's become 747's\n",
      "1 (0.7) @USAirways \"Airport snow removal method #22...\"\n",
      "Keep up the good work folks, this is where Cessna's become 747's.\n",
      "\n",
      "----\n",
      "0 (0.8) @USAirways it still says that I can't check into my flight because the information is incorrect but everything is entered correctly\n",
      "1 (0.6) @USAirways it still says that I can't check into my flight because the information is incorrect but everything is entered correctly.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "typos\n",
      "Test cases:      500\n",
      "Fails (rate):    23 (4.6%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.6) @SouthwestAir LUV! your new Luv Television Commercials. Traveled on your airline last year return trip from NYC...#feltthelove\n",
      "0 (1.0) @SouthwestAir LUV! your new Luv Television Commercials. Tarveled on your airline last year return trip from NYC...#feltthelove\n",
      "\n",
      "----\n",
      "2 (1.0) @USAirways thanks !!!\n",
      "0 (1.0) @USAirways thakns !!!\n",
      "\n",
      "----\n",
      "1 (0.8) @united okay ase24766m. Find our luggage.\n",
      "2 (0.9) @united okay as2e4766m. Find our luggage.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "2 typos\n",
      "Test cases:      500\n",
      "Fails (rate):    51 (10.2%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) @JetBlue just touched down in #NewOrleans for the annual @HeinekenUSACorp national distributor conference! #livethelegend\n",
      "0 (1.0) @JetBlue just touhced down in #eNwOrleans for the annual @HeinekenUSACorp national distributor conference! #livethelegend\n",
      "\n",
      "----\n",
      "2 (1.0) @united that would be great - 1k us651621\n",
      "0 (0.9) @united that would be graet - 1ku s651621\n",
      "\n",
      "----\n",
      "0 (0.7) @USAirways   my miles will expire on 2/29 and it could take someone 10 days to respond...I have over 150000 miles that I do not lose i❤usair\n",
      "1 (0.7) @USAirways   my miles will expier on 2/29 and it could take someone 10 days to respond...I have over 150000 miles that I do not lose i❤suair\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "contractions\n",
      "Test cases:      1000\n",
      "Fails (rate):    30 (3.0%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.7) @VirginAmerica please provide status for flight 769.  I cant imagine it's on time as the Web indicates  dude to weather here and/or Dallas\n",
      "0 (0.9) @VirginAmerica please provide status for flight 769.  I cant imagine it is on time as the Web indicates  dude to weather here and/or Dallas\n",
      "\n",
      "----\n",
      "2 (0.8) @AmericanAir upgraded to 1st class. Inquired about red wine.  \"Sauvignon blanc &amp; I can't pronounce the other one.\"  Couldn't describe...\n",
      "1 (0.6) @AmericanAir upgraded to 1st class. Inquired about red wine.  \"Sauvignon blanc &amp; I cannot pronounce the other one.\"  Could not describe...\n",
      "\n",
      "----\n",
      "2 (1.0) @JetBlue thanks for listening. Doesn't mean I don't appreciate you!\n",
      "1 (0.6) @JetBlue thanks for listening. Does not mean I do not appreciate you!\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporal\n",
      "\n",
      "used to, but now\n",
      "Test cases:      8000\n",
      "Fails (rate):    1489 (18.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I think this airline is lousy, but in the past I thought it was beautiful.\n",
      "----\n",
      "1 (0.6) I abhor this airline,  I used to like it.\n",
      "----\n",
      "2 (1.0) I used to enjoy this airline, even though now I regret it.\n",
      "----\n",
      "\n",
      "\n",
      "\"used to\" should reduce\n",
      "Test cases:      4524\n",
      "After filtering: 15 (0.3%)\n",
      "Fails (rate):    8 (53.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.7) this was an average aircraft.\n",
      "0 (1.0) I used to think this was an average aircraft.\n",
      "\n",
      "----\n",
      "0 (0.8) this is an average flight.\n",
      "0 (1.0) I used to think this is an average flight.\n",
      "\n",
      "----\n",
      "0 (0.9) it is an average food.\n",
      "0 (1.0) I used to think it is an average food.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness\n",
      "\n",
      "protected: race\n",
      "Test cases:      600\n",
      "Fails (rate):    533 (88.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) Rachel is a black American.\n",
      "2 (1.0) Rachel is a hispanic American.\n",
      "2 (1.0) Rachel is an asian American.\n",
      "\n",
      "----\n",
      "0 (1.0) Cynthia is a black experience.\n",
      "2 (1.0) Cynthia is a hispanic experience.\n",
      "2 (1.0) Cynthia is an asian experience.\n",
      "\n",
      "----\n",
      "0 (1.0) Sarah is a black mother.\n",
      "2 (1.0) Sarah is a hispanic mother.\n",
      "2 (1.0) Sarah is an asian mother.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: sexual\n",
      "Test cases:      600\n",
      "Fails (rate):    556 (92.7%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Jason is an asexual friend.\n",
      "0 (0.8) Jason is a homosexual friend.\n",
      "\n",
      "----\n",
      "2 (1.0) Cody is an asexual boyfriend.\n",
      "0 (1.0) Cody is a homosexual boyfriend.\n",
      "0 (0.9) Cody is a gay boyfriend.\n",
      "\n",
      "----\n",
      "2 (1.0) Taylor is an asexual mom.\n",
      "1 (1.0) Taylor is a trans mom.\n",
      "1 (0.8) Taylor is a transgender mom.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: religion\n",
      "Test cases:      600\n",
      "Fails (rate):    570 (95.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Rachel is a Christian fighter.\n",
      "0 (1.0) Rachel is an Agnostic fighter.\n",
      "0 (1.0) Rachel is an Atheist fighter.\n",
      "\n",
      "----\n",
      "2 (1.0) Kathryn is a Christian guru.\n",
      "0 (1.0) Kathryn is an Atheist guru.\n",
      "0 (1.0) Kathryn is an Agnostic guru.\n",
      "\n",
      "----\n",
      "2 (1.0) Elizabeth is a Christian Woman.\n",
      "0 (1.0) Elizabeth is an Atheist Woman.\n",
      "0 (1.0) Elizabeth is an Agnostic Woman.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: nationality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      600\n",
      "Fails (rate):    90 (15.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) Elijah is a Chinese anarchist.\n",
      "1 (0.5) Elijah is an American anarchist.\n",
      "2 (0.7) Elijah is an Indian anarchist.\n",
      "\n",
      "----\n",
      "0 (0.9) Destiny is a Chinese refugee.\n",
      "1 (0.6) Destiny is a French refugee.\n",
      "1 (0.6) Destiny is an Egyptian refugee.\n",
      "\n",
      "----\n",
      "1 (0.6) Jesus is a Chinese passport.\n",
      "2 (0.9) Jesus is a French passport.\n",
      "2 (0.8) Jesus is an Indian passport.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRL\n",
      "\n",
      "my opinion is what matters\n",
      "Test cases:      9136\n",
      "Fails (rate):    3374 (36.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I dislike you, some people enjoy you.\n",
      "----\n",
      "2 (0.9) Some people think you are amazing, I think you are difficult.\n",
      "----\n",
      "2 (1.0) I think you are tough, but some people think you are great.\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes\n",
      "Test cases:      7956\n",
      "Fails (rate):    226 (2.8%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Did we dread the seat? Yes\n",
      "----\n",
      "2 (0.8) Did I dread this pilot? Yes\n",
      "----\n",
      "1 (0.5) Do I think that airline was adorable? Yes\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes (neutral)\n",
      "Test cases:      1560\n",
      "Fails (rate):    1541 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) Do I think the plane is international? Yes\n",
      "----\n",
      "0 (1.0) Do I think this is a commercial aircraft? Yes\n",
      "----\n",
      "0 (1.0) Do I think it was an Australian service? Yes\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: no\n",
      "Test cases:      7956\n",
      "Fails (rate):    4371 (54.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think the company was unhappy? No\n",
      "----\n",
      "0 (1.0) Do I think it is an average staff? No\n",
      "----\n",
      "0 (1.0) Do I think this pilot is tough? No\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: no (neutral)\n",
      "Test cases:      1560\n",
      "Fails (rate):    1560 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that is an Australian food? No\n",
      "----\n",
      "0 (1.0) Do I think this crew is Italian? No\n",
      "----\n",
      "0 (1.0) Do I think the customer service is Indian? No\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negation\n",
      "\n",
      "simple negations: negative\n",
      "Test cases:      6318\n",
      "Fails (rate):    580 (9.2%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I can't say I recommend this cabin crew.\n",
      "----\n",
      "2 (1.0) I would never say I welcome this company.\n",
      "----\n",
      "2 (1.0) I can't say I enjoy this cabin crew.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not negative\n",
      "Test cases:      7254\n",
      "Fails (rate):    1255 (17.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I would never say I hate the airline.\n",
      "----\n",
      "0 (1.0) I would never say I despise the aircraft.\n",
      "----\n",
      "0 (0.8) I don't think I abhor the aircraft.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not neutral is still neutral\n",
      "Test cases:      2496\n",
      "Fails (rate):    2466 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) That is not a commercial staff.\n",
      "----\n",
      "0 (1.0) This was not a commercial company.\n",
      "----\n",
      "2 (1.0) It isn't an Indian seat.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but I did not (negative)\n",
      "Test cases:      2106\n",
      "Fails (rate):    32 (1.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.7) I thought I would love that aircraft, but I did not.\n",
      "----\n",
      "1 (0.6) I thought I would love this flight, but I did not.\n",
      "----\n",
      "2 (0.7) I thought I would love that service, but I did not.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but I did not (not negative)\n",
      "Test cases:      2418\n",
      "Fails (rate):    2082 (86.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) I thought this plane would be horrible, but it wasn't.\n",
      "----\n",
      "0 (1.0) I thought that crew would be tough, but it wasn't.\n",
      "----\n",
      "0 (1.0) I thought that seat would be tough, but it was not.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but it was not (neutral)\n",
      "Test cases:      858\n",
      "Fails (rate):    844 (98.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I thought that seat would be Indian, but it was not.\n",
      "----\n",
      "0 (1.0) I thought that staff would be Australian, but it was not.\n",
      "----\n",
      "0 (1.0) I thought this staff would be international, but it wasn't.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle\n",
      "Test cases:      1000\n",
      "Fails (rate):    709 (70.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I wouldn't say, given all that I've seen over the years, that that pilot is nice.\n",
      "----\n",
      "2 (1.0) I can't say, given the time that I've been flying, that the airline is incredible.\n",
      "----\n",
      "2 (1.0) I can't say, given the time that I've been flying, that the was a brilliant staff.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle, not negative\n",
      "Test cases:      1000\n",
      "Fails (rate):    989 (98.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I don't think, given that I am from Brazil, that that airline was dreadful.\n",
      "----\n",
      "0 (1.0) i wouldn't say, given all that I've seen over the years, that I despise that plane.\n",
      "----\n",
      "0 (1.0) i don't think, given my history with airplanes, that this is a sad service.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle, neutral\n",
      "Test cases:      1000\n",
      "Fails (rate):    976 (97.6%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) I can't say, given my history with airplanes, that that is a British food.\n",
      "----\n",
      "0 (1.0) I wouldn't say, given the time that I've been flying, that the is an Italian cabin crew.\n",
      "----\n",
      "0 (0.9) I wouldn't say, given that I am from Brazil, that we find this company.\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NER\n",
      "\n",
      "change names\n",
      "Test cases:      331\n",
      "Fails (rate):    22 (6.6%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.6) @VirginAmerica to start 5xweekly #A319 flights from to #Dallas @DallasLoveField #Austin on 28APR #avgeek\n",
      "0 (0.7) @VirginAmerica to start 5xweekly #A319 flights from to #Dallas @DallasLoveField #Chad on 28APR #avgeek\n",
      "\n",
      "----\n",
      "2 (0.9) @SouthwestAir Flight 2078 to Balt hit turbulence, babies cried, kids vomited. Chaos. Flight attendant Caroline was a superhero.\n",
      "0 (0.8) @SouthwestAir Flight 2078 to Balt hit turbulence, babies cried, kids vomited. Chaos. Flight attendant Diana was a superhero.\n",
      "\n",
      "----\n",
      "1 (0.6) @VirginAmerica Debbie Baldwin gave a #rockstar performance of the safety demo this evening on VX919 #LAS2SFO #BestCrew #SheRocks\n",
      "2 (0.9) @VirginAmerica Jessica Rodriguez gave a #rockstar performance of the safety demo this evening on VX919 #LAS2SFO #BestCrew #SheRocks\n",
      "2 (0.9) @VirginAmerica Ashley Gomez gave a #rockstar performance of the safety demo this evening on VX919 #LAS2SFO #BestCrew #SheRocks\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change locations\n",
      "Test cases:      909\n",
      "Fails (rate):    56 (6.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) @united The Opal Dragon book The Dragon (ALI) has woven his murdering ways from the Philippines to Australia http://t.co/N2fvElcYgz\n",
      "1 (0.8) @united The Opal Dragon book The Dragon (ALI) has woven his murdering ways from the Barbados to Australia http://t.co/N2fvElcYgz\n",
      "1 (0.6) @united The Opal Dragon book The Dragon (ALI) has woven his murdering ways from the Libya to Australia http://t.co/N2fvElcYgz\n",
      "\n",
      "----\n",
      "2 (0.9) @USAirways sitting on the ground in Charlotte with no gate or comms for flight deck great happy Friday hope i make my connection\n",
      "1 (0.5) @USAirways sitting on the ground in Norman with no gate or comms for flight deck great happy Friday hope i make my connection\n",
      "\n",
      "----\n",
      "2 (1.0) @united landing in Chicago - saw it once we were already airborne.\n",
      "1 (0.9) @united landing in Rancho Santa Margarita - saw it once we were already airborne.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change numbers\n",
      "Test cases:      1000\n",
      "Fails (rate):    28 (2.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.7) @VirginAmerica Can you give me Silver Status for 12 months?\n",
      "1 (0.8) @VirginAmerica Can you give me Silver Status for 9 months?\n",
      "1 (0.8) @VirginAmerica Can you give me Silver Status for 9 months?\n",
      "\n",
      "----\n",
      "1 (0.7) @USAirways  really u gave 6 scotch on the rocks to an out of control drunk and would not let my sister change seats when he was grouping her\n",
      "2 (0.7) @USAirways  really u gave 8 scotch on the rocks to an out of control drunk and would not let my sister change seats when he was grouping her\n",
      "2 (0.7) @USAirways  really u gave 8 scotch on the rocks to an out of control drunk and would not let my sister change seats when he was grouping her\n",
      "\n",
      "----\n",
      "1 (0.9) @USAirways wanted to connect to discuss sponsorship opportunities with http://t.co/q6Xedzvhh9, silicon valleys only 10 day film festival\n",
      "2 (0.7) @USAirways wanted to connect to discuss sponsorship opportunities with http://t.co/q6Xedzvhh9, silicon valleys only 13 day film festival\n",
      "2 (0.7) @USAirways wanted to connect to discuss sponsorship opportunities with http://t.co/q6Xedzvhh9, silicon valleys only 13 day film festival\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
