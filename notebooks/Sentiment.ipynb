{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import Mft\n",
    "from checklist.inv_dir import Inv, Dir\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = checklist.editor.Editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good, great, excellent, amazing, new, terrible, awful, old, interesting, unusual, small, incredible, extraordinary, expensive, important, experimental, bad, awesome, big, ordinary, wonderful, huge, fantastic, American, nice, enormous, exceptional, odd, large, unfamiliar, remarkable, professional, empty, skeleton, easy, open, happy, young, outstanding, exemplary\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('It was {a:bert} {air_noun}.')[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'tough', 'weird', 'aggressive', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.tg.unmask('I <mask> the food.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liked, enjoyed, like, appreciate, appreciated, enjoy, loved, love, miss, missed, recommend, wanted, got, needed, likes, prefer, hate, need, admired, want, admire, value, enjoying, enjoys, dislike, dig, use, trust, respect, respected, liking, did, used, get, understand, found, adore, underestimated, preferred, was, valued, feel, have, ,, tried, helped, took, mean, regret, disliked, praised, noticed, dug, hated, loves, had, compliment, remember, cherish, do, rate, about, LOVE, felt, saw, supported, support, applaud, commend, thank, bought, treasure, improved, see, understood, welcome, hit, left, is, beat, all, impressed, know, think, thought, for, believe, thanks, trusted, credit, underestimate, help, take, met, tested, wish, experienced, lost, recommended, blame, owe, leave, in, reviewed, picked, welcomed, made, Love, packed, chose, misses, finished, values, received, delivered, praise, embrace, upgraded, consider, to, changed, worked, into, started, considered, envy, improve, try, fancy, are, follow, drove, experience, sold, meant, respects, needs, compliments, regretted, thanked, rocked, salute, deserved, told, ordered, rode, meet, pleased, surprised, handled, saved, flew, hope, forgive, recruited, complement, brought, hired, loving, doubt, recognize, stressed, just, embraced, survived, crashed, cleaned, spoiled, congratulate, on, choose, notice, watched, favor, remembered, pushed, own, joined, find, owned, earned, cherished, drive, applauded, heard, offer, followed, expanded, deliver, utilize, emphasize, provide, crave, uses, learned, mentioned, ride, filled, gave, moved\n",
      "\n",
      "and, of, love, for, to, ,, like, loved, about, liked, on, found, was, with, left, blame, 's, got, hate, is, get, ing, called, miss, had, ating, remember, have, ., need, took, changed, in, know, :, trust, saw, want, at, are, wanted, see, started, asked, needed, appreciate, appreciated, missed, did, enjoyed\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('I really {bert} the {air_noun}.')[:200]))\n",
    "print()\n",
    "print(', '.join(editor.suggest('I {bert} the {air_noun}.')[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Mft(pos_adj + pos_verb_present + pos_verb_past, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 38 examples\n",
      "Test cases:     38\n",
      "Failure rate:   5.3%\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) tough\n",
      "2 (1.0) aggressive\n"
     ]
    }
   ],
   "source": [
    "test = Mft(neg_adj + neg_verb_present + neg_verb_past, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 12 examples\n",
      "Test cases:     12\n",
      "Failure rate:   100.0%\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) British\n",
      "2 (1.0) American\n",
      "2 (1.0) found\n"
     ]
    }
   ],
   "source": [
    "test = Mft(neutral_adj + neutral_verb_present + neutral_verb_past, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3510, 4992)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template('I {pos_verb_present} {bert} {air_noun}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8970 examples\n",
      "Test cases:      8970\n",
      "Fails (rate):    169 (1.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) That was an aggressive airline.\n",
      "2 (1.0) This service was aggressive.\n",
      "2 (1.0) That was an aggressive plane.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels = [2] * len(data)\n",
    "data += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1716 examples\n",
      "Test cases:      1716\n",
      "Fails (rate):    1626 (94.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) It airline was Indian.\n",
      "2 (1.0) The seat was private.\n",
      "2 (1.0) This food was international.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {be} {neutral_adj}.', it=['That', 'This', 'The'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {neutral_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very , really , absolutely , truly , extremely , quite , most , pretty , incredibly , just , almost , actually , ... , amazingly , especially , exceptionally , surprisingly , unbelievably , equally , utterly , extraordinarily , absolute , overall , otherwise , damn , unusually , obviously , simply , exceedingly , generally , real , a , entirely , overwhelmingly , undeniably , amazing , rather , already , totally , increasingly , altogether , unexpectedly , … , particularly , all , insanely , incredible , fairly , actual , apparently\n"
     ]
    }
   ],
   "source": [
    "print(' , '.join(editor.suggest('{it} {be} {a:bert} {pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I, really, always, also, highly, definitely, all, personally, truly, certainly, actually, absolutely, still, just, thoroughly, very, greatly, especially, particularly, much, we, quite, so, totally, generally, clearly, seriously, never, 'd, genuinely, rather, honestly, do, people, strongly, both, sure, obviously, did, you, fully, REALLY, ly, sincerely, only, most, guys, already, initely, would, completely, deeply, 've, even, Really, and, simply, ,, too, have, they, will, kids, 's, 'll, mostly, can, should, immediately, must, again, extremely, had, ..., now, least, he, dearly, might, to, everyone, fucking, could, kinda, 't, probably, usually, immensely, may, family, hardly, o, does, specifically, many, students, boys, secretly, more, everybody\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('{i} {bert} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:      2000\n",
      "After filtering: 1996 (99.8%)\n",
      "Fails (rate):    26 (1.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) This is a commercial plane.\n",
      "2 (1.0) This is an exceedingly commercial plane.\n",
      "\n",
      "0 (0.9) It is a commercial aircraft.\n",
      "2 (1.0) It is a really commercial aircraft.\n",
      "\n",
      "0 (1.0) This was a creepy seat.\n",
      "2 (1.0) This was an amazingly creepy seat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "data += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "test = Dir(data, monotonic_label)\n",
    "test.run(new_pp)\n",
    "test.set_monotonic_print(increasing=True)\n",
    "test.summary(3)\n",
    "# test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'little', 'bit', 'slightly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:     2000\n",
      "Cases filtered out: 1987 (99.3%)\n",
      "Failure rate:   30.8%\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) It was an average plane.\n",
      "0 (1.0) It was a kinda average plane.\n",
      "\n",
      "0 (0.9) It is an average flight.\n",
      "0 (1.0) It is a probably average flight.\n",
      "\n",
      "0 (0.9) It is a creepy food.\n",
      "0 (1.0) It is a probably creepy food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:red} {pos_adj} {air_noun}.'] , red=reducer_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "data += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:red} {neg_adj} {air_noun}.'] , red=reducer_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "test = Dir(data, monotonic_label_down)\n",
    "test.run(new_pp)\n",
    "test.set_monotonic_print(increasing=False)\n",
    "test.summary(3)\n",
    "# test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invariance: change neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = set(\n",
    "    ['.', 'the', 'The', ',', 'a', 'A', 'and', 'of', 'to', 'it', 'that', 'in',\n",
    "     'this', 'for',  'you', 'there', 'or', 'an', 'by', 'about', 'flight', 'my',\n",
    "     'in', 'of', 'have', 'with', 'was', 'at', 'it', 'get', 'from', 'this', 'Flight', 'plane'\n",
    "    ])\n",
    "forbidden = set(['No', 'no', 'Not', 'not', 'Nothing', 'nothing', 'without'] + pos_adj + neg_adj + pos_verb_present + pos_verb_past + neg_verb_present + neg_verb_past)\n",
    "def change_neutral(d):\n",
    "#     return d.text\n",
    "    examples = []\n",
    "    words_in = [x for x in d.capitalize().split() if x in neutral_words]\n",
    "    if not words_in:\n",
    "        return None\n",
    "    for w in words_in:\n",
    "        examples.extend([x[1] for x in editor.suggest_replace(d, w, beam_size=5, words_and_sentences=True) if x[0] not in forbidden])\n",
    "    if examples:\n",
    "        idxs = np.random.choice(len(examples), min(len(examples), 10), replace=False)\n",
    "        return [examples[i] for i in idxs]\n",
    "# Perturb.perturb(parsed_data[:5], perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Perturb.perturb(sentences, change_neutral, nsamples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5009 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    64 (12.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) @SouthwestAir how do I get a companion pass\n",
      "1 (0.9) @SouthwestAir how do I add a companion pass\n",
      "\n",
      "2 (0.9) @united well perhaps you could highlight where that is stated.  We travel between Canada &amp; USA on United with only Nexus\n",
      "1 (0.8) @united well perhaps they could highlight where that is stated.  We travel between Canada &amp; USA on United with only Nexus\n",
      "1 (0.9) @united well perhaps someone could highlight where that is stated.  We travel between Canada &amp; USA on United with only Nexus\n",
      "\n",
      "2 (0.9) @JetBlue sadly, no! I have the app, but it also is experiencing difficulties. The flight information boards are keeping me updated.\n",
      "0 (0.7) @JetBlue sadly, no! I checked the app, but it also is experiencing difficulties. The flight information boards are keeping me updated.\n",
      "0 (0.7) @JetBlue sadly, no! I have the app, but it also is experiencing difficulties. The airline information boards are keeping me updated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = editor.template('I {pos_verb_present} you.')\n",
    "positive += editor.template('You are {pos_adj}.')\n",
    "positive += ['I would fly with you again.']\n",
    "positive.remove('You are happy.')\n",
    "negative = editor.template('I {neg_verb_present} you.')\n",
    "negative += editor.template('You are {neg_adj}.')\n",
    "negative += ['Never flying with you again.']\n",
    "def add_phrase_function(phrases):\n",
    "    def pert(d):\n",
    "        while d[-1].pos_ == 'PUNCT':\n",
    "            d = d[:-1]\n",
    "        d = d.text\n",
    "        ret = [d + '. ' + x for x in phrases]\n",
    "        idx = np.random.choice(len(ret), 10, replace=False)\n",
    "        ret = [ret[i] for i in idx]\n",
    "        return ret\n",
    "    return pert\n",
    "\n",
    "# perturbed = PerturbFactory.perturb_key(small, 'sentence', add_phrase_function(positive))\n",
    "# test = mltest.Test(perturbed, expectation_fn = mon_increasing)\n",
    "# r = test.run(model.predict_and_confidence, is_binary=False, n=500)\n",
    "# r.summary(5, format_fn=format_perturb_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_1 = Expect.monotonic(label=2, increasing=True, tolerance=0.1)\n",
    "monotonic_1_down = Expect.monotonic(label=2, increasing=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5500 examples\n",
      "Test cases:      500\n",
      "After filtering: 150 (30.0%)\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, add_phrase_function(positive), nsamples=500)\n",
    "test = Dir(data, monotonic_1)\n",
    "test.run(new_pp, overwrite=True)\n",
    "test.set_monotonic_print(label=2, increasing=True)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5500 examples\n",
      "Test cases:      500\n",
      "After filtering: 370 (74.0%)\n",
      "Fails (rate):    83 (22.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) @USAirways Nice try.  Doesn't apply to Philly.\n",
      "2 (1.0) @USAirways Nice try.  Doesn't apply to Philly. You are tough.\n",
      "0 (0.8) @USAirways Nice try.  Doesn't apply to Philly. You are aggressive.\n",
      "\n",
      "0 (0.8) @united maybe one day you'll be the one quoted on http://t.co/mJkpgVXmPC\n",
      "2 (1.0) @united maybe one day you'll be the one quoted on http://t.co/mJkpgVXmPC. You are aggressive.\n",
      "\n",
      "0 (1.0) @SouthwestAir schedule is open through the end of October, got Columbus day wknd in New England for ~50K points for 4! Check your calendar!\n",
      "2 (0.9) @SouthwestAir schedule is open through the end of October, got Columbus day wknd in New England for ~50K points for 4! Check your calendar. You are aggressive.\n",
      "0 (1.0) @SouthwestAir schedule is open through the end of October, got Columbus day wknd in New England for ~50K points for 4! Check your calendar. You are tough.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, add_phrase_function(negative), nsamples=500)\n",
    "test = Dir(data, monotonic_1_down)\n",
    "test.run(new_pp, overwrite=True)\n",
    "test.set_monotonic_print(label=2, increasing=False)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: robustness\n",
    "### Invariance: adding irrelevant stuff before and after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def random_string(n):\n",
    "    return ''.join(np.random.choice([x for x in string.ascii_letters + string.digits], n))\n",
    "def random_url(n=6):\n",
    "    return 'https://t.co/%s' % random_string(n)\n",
    "def random_handle(n=6):\n",
    "    return '@%s' % random_string(n)\n",
    "\n",
    "# data['sentence']\n",
    "\n",
    "def add_irrelevant(sentence):\n",
    "    urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]\n",
    "    irrelevant_before = ['@airline '] + urls_and_handles\n",
    "    irrelevant_after = urls_and_handles \n",
    "    rets = ['%s %s' % (x, sentence) for x in irrelevant_before ]\n",
    "    rets += ['%s %s' % (sentence, x) for x in irrelevant_after]\n",
    "    return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 11000 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    56 (11.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) @united well. As of yet, our checked bag has already vanished and we haven't left the airport yet.\n",
      "2 (0.7) @united well. As of yet, our checked bag has already vanished and we haven't left the airport yet. @lUVsAS\n",
      "2 (0.9) @united well. As of yet, our checked bag has already vanished and we haven't left the airport yet. @N6GErs\n",
      "\n",
      "1 (0.7) @JetBlue's new CEO seeks the right balance to please passengers and Wall ... - Daily Journal http://t.co/9bzqZQx8DC\n",
      "2 (0.8) @airline  @JetBlue's new CEO seeks the right balance to please passengers and Wall ... - Daily Journal http://t.co/9bzqZQx8DC\n",
      "0 (1.0) https://t.co/Y7VByc @JetBlue's new CEO seeks the right balance to please passengers and Wall ... - Daily Journal http://t.co/9bzqZQx8DC\n",
      "\n",
      "2 (1.0) @AmericanAir come on I just want to go home I can't miss another day of work #stuckinmemphis #texasisclosed\n",
      "1 (0.5) @LwpRRX @AmericanAir come on I just want to go home I can't miss another day of work #stuckinmemphis #texasisclosed\n",
      "1 (0.6) @kxE3XI @AmericanAir come on I just want to go home I can't miss another day of work #stuckinmemphis #texasisclosed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, add_irrelevant, nsamples=500)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation, contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1162 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    15 (3.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.7) @SouthwestAir F5R3ZZ\n",
      "0 (0.8) @SouthwestAir F5R3ZZ.\n",
      "\n",
      "0 (0.8) @USAirways it still says that I can't check into my flight because the information is incorrect but everything is entered correctly\n",
      "1 (0.6) @USAirways it still says that I can't check into my flight because the information is incorrect but everything is entered correctly.\n",
      "\n",
      "0 (0.9) @usairways I get extended hold times due to the weather...but 2-1/2 hours on hold (and counting). HELP!!\n",
      "2 (0.8) @usairways I get extended hold times due to the weather...but 2-1/2 hours on hold (and counting). HELP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.punctuation, nsamples=500)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2087 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    29 (2.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.7) @SouthwestAir    How's life @ the NOC?\n",
      "0 (0.9) @SouthwestAir    How is life @ the NOC?\n",
      "\n",
      "2 (0.9) @AmericanAir it's flight 5348\n",
      "1 (0.6) @AmericanAir it is flight 5348\n",
      "\n",
      "1 (0.9) @USAirways any update on this flight?It's more than three hours delayed. Thank you.\n",
      "2 (0.7) @USAirways any update on this flight?It is more than three hours delayed. Thank you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, Perturb.contractions, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 3641 examples\n",
      "Test cases:      331\n",
      "Fails (rate):    25 (7.6%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) @USAirways owes Tammy from the Winston-Salem call center for keeping me as a customer!\n",
      "2 (0.8) @USAirways owes Chelsea from the Winston-Salem call center for keeping me as a customer!\n",
      "2 (0.8) @USAirways owes Hannah from the Winston-Salem call center for keeping me as a customer!\n",
      "\n",
      "0 (0.9) @united Hey so many time changes for UA 1534. We going tonight or what? MIA - EWR\n",
      "2 (0.7) @united Hey so many time changes for UA 1534. We going tonight or what? Amanda Green\n",
      "1 (0.8) @united Hey so many time changes for UA 1534. We going tonight or what? Stephanie Robinson\n",
      "\n",
      "2 (0.9) @JetBlue your employee Charles cave at the gate at MSY went above and beyond to help try to help me find my glasses. Thought u should know\n",
      "0 (0.9) @JetBlue your employee Jesus cave at the gate at MSY went above and beyond to help try to help me find my glasses. Thought u should know\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9999 examples\n",
      "Test cases:      909\n",
      "Fails (rate):    61 (6.7%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.7) @USAirways us2118\n",
      "My wife in Boston says no snow right now.\n",
      "1 (0.6) @USAirways us2118\n",
      "My wife in Greenville says no snow right now.\n",
      "0 (0.8) @USAirways us2118\n",
      "My wife in Park Ridge says no snow right now.\n",
      "\n",
      "1 (0.5) @SouthwestAir to start daily #B737-700 flights from #Columbus OH to #Oakland on 8AUG #avgeek\n",
      "0 (0.7) @SouthwestAir to start daily #B737-700 flights from #Columbus OH to #Warner Robins on 8AUG #avgeek\n",
      "0 (0.7) @SouthwestAir to start daily #B737-700 flights from #Columbus OH to #Richmond on 8AUG #avgeek\n",
      "\n",
      "2 (0.9) @SouthwestAir we have to stay in Chicago overnight and meet up with our bags there. I hope they stay put and don't get put on a flight.\n",
      "1 (0.5) @SouthwestAir we have to stay in Cranston overnight and meet up with our bags there. I hope they stay put and don't get put on a flight.\n",
      "1 (0.7) @SouthwestAir we have to stay in Dunwoody overnight and meet up with our bags there. I hope they stay put and don't get put on a flight.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_location, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 11000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    21 (2.1%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.9) @americanair the best is your 800 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "2 (0.9) @americanair the best is your 704 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "2 (1.0) @americanair the best is your 670 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "\n",
      "0 (0.9) @VirginAmerica save some for 871 tomorrow AM!\n",
      "1 (0.6) @VirginAmerica save some for 826 tomorrow AM!\n",
      "\n",
      "0 (0.7) @USAirways   my miles will expire on 2/29 and it could take someone 10 days to respond...I have over 150000 miles that I do not lose i❤usair\n",
      "1 (0.6) @USAirways   my miles will expire on 2/29 and it could take someone 10 days to respond...I have over 134350 miles that I do not lose i❤usair\n",
      "1 (0.6) @USAirways   my miles will expire on 2/29 and it could take someone 10 days to respond...I have over 178595 miles that I do not lose i❤usair\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_number, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: temporal awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate', 'dislike', 'regret', 'abhor', 'dread', 'despise']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{neg_verb_present}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8000 examples\n",
      "Test cases:      8000\n",
      "Fails (rate):    1531 (19.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) I think this airline is great, but I used to think it was difficult.\n",
      "2 (1.0) I abhor this airline, but in the past I would love it.\n",
      "2 (1.0) I used to admire this airline, even though now I despise it.\n"
     ]
    }
   ],
   "source": [
    "change = ['but', 'even though', 'although', '']\n",
    "data = editor.template(['I used to think this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} I used to think it was {neg_adj}.',\n",
    "                                 'In the past I thought this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} in the past I thought it was {neg_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500)\n",
    "data += editor.template(['I used to {neg_verb_present} this airline, {change} now I {pos_verb_present} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} I used to {neg_verb_present} it.',\n",
    "                                 'In the past I would {neg_verb_present} this airline, {change} now I {pos_verb} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} in the past I would {neg_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500)\n",
    "labels = [2] * len(data)\n",
    "\n",
    "data += editor.template(['I used to think this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} I used to think it was {pos_adj}.',\n",
    "                                 'In the past I thought this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} in the past I thought it was {pos_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500)\n",
    "data += editor.template(['I used to {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} I used to {pos_verb_present} it.',\n",
    "                                 'In the past I would {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} in the past I would {pos_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500)\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = Mft(data, labels=labels)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used to should reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9204 examples\n",
      "Test cases:      4602\n",
      "After filtering: 27 (0.6%)\n",
      "Fails (rate):    14 (51.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) this was a commercial flight.\n",
      "0 (1.0) I used to think this was a commercial flight.\n",
      "\n",
      "0 (0.9) it is an average food.\n",
      "0 (1.0) I used to think it is an average food.\n",
      "\n",
      "0 (0.9) that was a commercial company.\n",
      "0 (1.0) I used to think that was a commercial company.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:adj} {air_noun}.', 'I used to think {it} {be} {a:adj} {air_noun}.'], it=['it', 'this', 'that'], be=['is', 'was'], adj=editor.lexicons['pos_adj'] + editor.lexicons['neg_adj'])\n",
    "data += editor.template(['{i} {verb} {the} {air_noun}.', '{i} used to {verb} {the} {air_noun}.'], i=['I', 'We'], the=['this', 'that', 'the'], verb=editor.lexicons['pos_verb_present'] + editor.lexicons['neg_verb_present'])\n",
    "test = Dir(data, monotonic_label_down)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect:fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['male', 'female', 'first_name', 'first_pronoun', 'last_name', 'country', 'nationality', 'city', 'religion', 'religion_adj', 'sexual_adj', 'air_noun', 'neutral_noun', 'pos_adj', 'neg_adj', 'pos_verb_present', 'neg_verb_present', 'neutral_verb_present', 'pos_verb_past', 'neg_verb_past', 'neutral_verb_past', 'neutral_adj', 'pos_verb', 'neg_verb', 'neutral_verb'])"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a Christian',\n",
       "  'a Protestant',\n",
       "  'a Roman Catholic',\n",
       "  'an Eastern Orthodox',\n",
       "  'an Anglican',\n",
       "  'a Jew',\n",
       "  'an Orthodox Jew',\n",
       "  'a Muslim',\n",
       "  'a Sunni',\n",
       "  \"a Shi'a\",\n",
       "  'an Ahmadiyya',\n",
       "  'a Buddhist',\n",
       "  'a Zoroastrian',\n",
       "  'a Hindu',\n",
       "  'a Sikh',\n",
       "  'a Shinto',\n",
       "  \"a Baha'i\",\n",
       "  'a Taoist',\n",
       "  'a Confucian',\n",
       "  'a Jain',\n",
       "  'an Atheist',\n",
       "  'an Agnostic'],)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{a:religion_adj}'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = {\n",
    "    'race': ['a black','a hispanic', 'a white', 'an asian'],\n",
    "    'sexual': editor.template('{a:sexual_adj}'),\n",
    "    'religion': editor.template('{a:religion_adj}'),\n",
    "    'nationality': editor.template('{a:nationality}')[:20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "Predicting 800 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    182 (91.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) Leslie is a black girl.\n",
      "2 (1.0) Leslie is a hispanic girl.\n",
      "2 (1.0) Leslie is an asian girl.\n",
      "\n",
      "0 (0.9) Caroline is a black American.\n",
      "2 (1.0) Caroline is an asian American.\n",
      "2 (1.0) Caroline is a hispanic American.\n",
      "\n",
      "0 (0.7) Victoria is a black boy.\n",
      "2 (1.0) Victoria is a hispanic boy.\n",
      "2 (1.0) Victoria is an asian boy.\n",
      "\n",
      "\n",
      "0.30 a black\n",
      "1.96 a hispanic\n",
      "0.51 a white\n",
      "2.00 an asian\n",
      "\n",
      "\n",
      "-------------------------\n",
      "sexual\n",
      "Predicting 2800 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    200 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Connor is an asexual woman.\n",
      "0 (0.8) Connor is a lesbian woman.\n",
      "0 (0.8) Connor is a trans woman.\n",
      "\n",
      "2 (1.0) Danielle is an asexual feminist.\n",
      "0 (0.7) Danielle is a transgender feminist.\n",
      "0 (0.7) Danielle is a heterosexual feminist.\n",
      "\n",
      "2 (1.0) Diana is an asexual person.\n",
      "1 (0.5) Diana is a cisgender person.\n",
      "0 (0.7) Diana is a bisexual person.\n",
      "\n",
      "\n",
      "1.85 an asexual\n",
      "1.41 a bisexual\n",
      "0.64 a heterosexual\n",
      "0.00 a homosexual\n",
      "1.85 a pansexual\n",
      "1.10 a queer\n",
      "1.61 a transsexual\n",
      "0.32 a trans\n",
      "0.01 a gay\n",
      "1.84 a straight\n",
      "0.13 a transgender\n",
      "0.17 a lesbian\n",
      "1.97 a non-binary\n",
      "1.75 a cisgender\n",
      "\n",
      "\n",
      "-------------------------\n",
      "religion\n",
      "Predicting 4400 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    198 (99.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Melissa is a Christian writer.\n",
      "0 (1.0) Melissa is an Agnostic writer.\n",
      "0 (1.0) Melissa is an Atheist writer.\n",
      "\n",
      "2 (0.9) Edward is a Christian Muslim.\n",
      "0 (0.8) Edward is a Jew Muslim.\n",
      "0 (0.9) Edward is an Agnostic Muslim.\n",
      "\n",
      "2 (1.0) Jesus is a Christian convert.\n",
      "0 (0.8) Jesus is a Jew convert.\n",
      "0 (1.0) Jesus is an Atheist convert.\n",
      "\n",
      "\n",
      "1.97 a Christian\n",
      "1.95 a Protestant\n",
      "1.92 a Roman Catholic\n",
      "2.00 an Eastern Orthodox\n",
      "1.96 an Anglican\n",
      "1.60 a Jew\n",
      "1.98 an Orthodox Jew\n",
      "1.77 a Muslim\n",
      "1.95 a Sunni\n",
      "1.97 a Shi'a\n",
      "1.98 an Ahmadiyya\n",
      "1.86 a Buddhist\n",
      "2.00 a Zoroastrian\n",
      "1.88 a Hindu\n",
      "1.99 a Sikh\n",
      "1.96 a Shinto\n",
      "1.99 a Baha'i\n",
      "2.00 a Taoist\n",
      "1.89 a Confucian\n",
      "2.00 a Jain\n",
      "0.04 an Atheist\n",
      "0.33 an Agnostic\n",
      "\n",
      "\n",
      "-------------------------\n",
      "nationality\n",
      "Predicting 4000 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    18 (9.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) Nathan is a Chinese Muslim.\n",
      "1 (0.6) Nathan is a German Muslim.\n",
      "\n",
      "2 (0.8) Edward is a Chinese Muslim.\n",
      "0 (0.7) Edward is a German Muslim.\n",
      "1 (0.8) Edward is a Vietnamese Muslim.\n",
      "\n",
      "2 (0.9) Cynthia is a Chinese immigrant.\n",
      "1 (0.5) Cynthia is a Turkish immigrant.\n",
      "1 (0.7) Cynthia is a Nigerian immigrant.\n",
      "\n",
      "\n",
      "1.98 a Chinese\n",
      "2.00 an Indian\n",
      "2.00 an American\n",
      "2.00 an Indonesian\n",
      "1.94 a Pakistani\n",
      "2.00 a Brazilian\n",
      "2.00 a Nigerian\n",
      "2.00 a Bangladeshi\n",
      "1.97 a Russian\n",
      "1.98 a Japanese\n",
      "1.96 a Mexican\n",
      "2.00 an Ethiopian\n",
      "2.00 a Philippine\n",
      "2.00 an Egyptian\n",
      "1.96 a Vietnamese\n",
      "1.88 a German\n",
      "2.00 a Turkish\n",
      "2.00 an Iranian\n",
      "2.00 a Thai\n",
      "1.99 a French\n",
      "\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for p, vals in protected.items():\n",
    "    print(p)\n",
    "    d = editor.template(['{male} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=100)\n",
    "    d += editor.template(['{female} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=100)\n",
    "    test = Inv(d, threshold=0.1)\n",
    "    test.run(new_pp)\n",
    "    test.summary(n=3)\n",
    "    print()\n",
    "    preds = np.array(test.results.preds)\n",
    "    for i, x in enumerate(vals):\n",
    "        print('%.2f %s' % (preds[:, i].mean(), vals[i]))\n",
    "    print()\n",
    "    print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2964 examples\n",
      "Test cases:      2964\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {pos_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "data += editor.template('No one {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7254 examples\n",
      "Test cases:      7254\n",
      "Fails (rate):    1255 (17.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) No one hates the pilot.\n",
      "0 (0.9) That seat isn't aggressive.\n",
      "0 (1.0) I don't abhor that aircraft.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {neg_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {neg_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "data += editor.template('No one {neg_verb_present}s {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "is_not_0 = lambda x, pred, *args: pred != 0\n",
    "test = Mft(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2496 examples\n",
      "Test cases:      2496\n",
      "Fails (rate):    2466 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) That is not an Italian aircraft.\n",
      "0 (0.8) It isn't an Israeli seat.\n",
      "0 (1.0) This plane isn't private.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {neutral_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {neutral_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Different templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2106 examples\n",
      "Test cases:      2106\n",
      "Fails (rate):    32 (1.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) I thought I would admire this aircraft, but I did not.\n",
      "1 (0.7) I thought I would admire this seat, but I did not.\n",
      "2 (0.8) I thought I would love that plane, but I did not.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {pos_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {pos_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2418 examples\n",
      "Test cases:      2418\n",
      "Fails (rate):    2082 (86.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) I thought the staff would be ugly, but it wasn't.\n",
      "0 (1.0) I thought that flight would be frustrating, but it was not.\n",
      "0 (1.0) I thought the plane would be dreadful, but it wasn't.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {neg_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {neg_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "test = Mft(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 858 examples\n",
      "Test cases:      858\n",
      "Fails (rate):    844 (98.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I thought this food would be Australian, but it wasn't.\n",
      "0 (1.0) I thought this flight would be American, but it was not.\n",
      "2 (0.9) I thought I would see this flight, but I did not.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {neutral_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {neutral_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harder: negation with neutral in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    706 (70.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I wouldn't say, given the time that I've been flying, that that was a sweet customer service.\n",
      "1 (0.9) I wouldn't say, given that I am from Brazil, that this was a happy plane.\n",
      "2 (1.0) I can't say, given my history with airplanes, that this flight is fantastic.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {pos_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:pos_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {pos_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = Mft(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    992 (99.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I wouldn't say, given it's a Tuesday, that this was a lame airline.\n",
      "0 (1.0) I can't say, given that I am from Brazil, that that was a poor airline.\n",
      "0 (1.0) I can't say, given it's a Tuesday, that that was a frustrating company.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neg_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neg_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {neg_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = Mft(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    984 (98.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I don't think, given all that I've seen over the years, that this service was Australian.\n",
      "0 (1.0) I wouldn't say, given the time that I've been flying, that this seat was Australian.\n",
      "0 (1.0) I can't say, given my history with airplanes, that this is a commercial staff.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neutral_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neutral_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {neutral_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: SRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my opinion is more important than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9136 examples\n",
      "Test cases:      9136\n",
      "Fails (rate):    3374 (36.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I think you are incredible, but some people think you are lame.\n",
      "0 (1.0) I think you are sweet, but some people think you are bad.\n",
      "2 (1.0) I think you are horrible, but some people think you are awesome.\n"
     ]
    }
   ],
   "source": [
    "change = [' but', '']\n",
    "templates = ['Some people think you are {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} some people think you are {neg_adj}.',\n",
    "             'I had heard you were {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} I had heard you were {neg_adj}.',\n",
    "             ]\n",
    "data = editor.template(templates, change=change, unroll=True)\n",
    "templates = ['{others} {neg_verb_present} you,{change} I {pos_verb_present} you.',\n",
    "             'I {pos_verb_present} you,{change} {others} {neg_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "labels = [2] * len(data)\n",
    "\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} some people think you are {pos_adj}.',\n",
    "             'I had heard you were {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} I had heard you were {pos_adj}.',\n",
    "             ]\n",
    "data += editor.template(templates, change=change, unroll=True)\n",
    "templates = ['{others} {pos_verb_present} you,{change} I {neg_verb_present} you.',\n",
    "             'I {neg_verb_present} you,{change} {others} {pos_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = Mft(data, labels=labels)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q & a form: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7956 examples\n",
      "Test cases:      7956\n",
      "Fails (rate):    226 (2.8%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.6) Do I think this was a nice staff? Yes\n",
      "2 (0.9) Did we regret this seat? Yes\n",
      "0 (1.0) Do I think the cabin crew was nice? Yes\n"
     ]
    }
   ],
   "source": [
    "temp = editor.template('Do I think {it} {air_noun} {be} {pos_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp += editor.template('Did {i} {pos_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = [x + ' Yes' for x in temp]\n",
    "labels = [2] * len(data)\n",
    "temp2 = editor.template('Do I think {it} {air_noun} {be} {neg_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp2 += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp2 += editor.template('Did {i} {neg_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data += [x + ' Yes' for x in temp2]\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "\n",
    "test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1560 examples\n",
      "Test cases:      1560\n",
      "Fails (rate):    1541 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that is an Italian food? Yes\n",
      "0 (1.0) Do I think this company is Indian? Yes\n",
      "0 (1.0) Do I think that airline was Indian? Yes\n"
     ]
    }
   ],
   "source": [
    "temp3 = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp3 += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp3 += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = [x + ' Yes' for x in temp3]\n",
    "test = Mft(data, labels=1)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7956 examples\n",
      "Test cases:      7956\n",
      "Fails (rate):    4371 (54.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think the service was nasty? No\n",
      "0 (1.0) Do I think the airline was creepy? No\n",
      "0 (1.0) Do I think this was a rough service? No\n"
     ]
    }
   ],
   "source": [
    "data = [x + ' No' for x in temp]\n",
    "labels = [0] * len(data)\n",
    "data += [x + ' No' for x in temp2]\n",
    "labels += [1] * (len(data) - len(labels))\n",
    "\n",
    "allow_for_neutral = lambda x, pred, _, label, _2 : pred != 0 if label == 1 else pred == label\n",
    "\n",
    "test = Mft(data, Expect.single(allow_for_neutral), labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1560 examples\n",
      "Test cases:      1560\n",
      "Fails (rate):    1560 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think this plane is international? No\n",
      "0 (1.0) Do I think that food was Italian? No\n",
      "0 (1.0) Do I think that crew is American? No\n"
     ]
    }
   ],
   "source": [
    "data = [x + ' No' for x in temp3]\n",
    "test = Mft(data, labels=1)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
