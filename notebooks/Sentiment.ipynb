{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<checklist.text_generation.TextGenerator at 0x7f9b5f1f3da0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "editor.tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template((('{first_name} is a good guy', 'Who is a good guy?'), '{first_name}', '3') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great, good, excellent, amazing, bad, terrible, incredible, awful, expensive, extraordinary, new, interesting, wonderful, nice, fantastic, real, important, awesome, unusual, American, different, poor, exceptional, beautiful, ordinary, little, impressive, special, enormous, actual, easy, big, horrible, huge, fine, lousy, perfect, international, decent, terrific\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('It was {a:bert} {air_noun}.')[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'weird', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liked, enjoyed, like, appreciate, appreciated, enjoy, loved, love, miss, missed, hate, needed, likes, wanted, got, recommend, prefer, admired, value, need, respected, want, dislike, enjoying, respect, enjoys, admire, was, feel, liking, dig, hated, mean, preferred, underestimated, disliked, used, get, use, valued, understand, did, found, adore, helped, dug, trust, remember, noticed, about, tried, hit, had, regret, took, felt, praised, cherish, have, left, loves, understood, do, LOVE, compliment, trusted, bought, thank, treasure, applaud, support, rate, know, commend, credit, underestimate, see, supported, think, thought, all, saw, picked, believe, beat, welcome, considered, impressed, improved, met, chose, thanks, deserved, envy, blame, for, help, packed, recommended, in, follow, choose, loving, misses, is, into, lost, consider, changed, worked, meant, leave, joined, wish, regretted, made, earned, rocked, cherished, take, worth, finished, experienced, are, fancy, handled, thanked, followed, saved, spoiled, told, surprised, salute, meet, pleased, try, recruited, crave, owe, reviewed, doubt, values, forgive, hired, hope, remembered, stressed, praise, respects, just, LIKE, knew, am, welcomed, tested, sold, Love, Like, survived, ate, planned, drove, ordered, sampled, crashed, managed, brought, ended, deserve, rode, eat, applauded, regrets, spent, heard, grabbed, needs, tasted, coveted, ruined, timed, delayed, caught, dreaded, watched, forgot, anticipated, canceled, own, rushed, cooked, taste, delivered, prefers, filled, deserves, shortened, mind, received, edited\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('I really {bert} the {air_noun}.')[:200]))\n",
    "# print()\n",
    "# print(', '.join(editor.suggest('I {bert} the {air_noun}.')[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(pos_adj + pos_verb_present + pos_verb_past, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual positive words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(neg_adj + neg_verb_present + neg_verb_past, labels=0)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual negative words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(neutral_adj + neutral_verb_present + neutral_verb_past, labels=1)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual neutral words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=2, save=True)\n",
    "t += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=0, save=True)\n",
    "# equivalent to:\n",
    "# test = MFT(t.data, labels=t.labels, templates=t.templates)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'sentiment words in context', 'Vocabulary', 'TODO_DESCRIPTION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {be} {neutral_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{it} {be} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{i} {neutral_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'neutral words in context', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really , very , incredibly , pretty , absolutely , truly , quite , extremely , most , actually , exceptionally , rather , particularly , genuinely , utterly , just , especially , unbelievably , amazingly , extraordinarily , always , equally , a , historically , simply , absolute , surprisingly , entirely , super , obviously , overall , exceedingly , unusually , unexpectedly , overwhelmingly , otherwise , enormously , undeniably , certainly , almost , immensely , insanely , awfully , totally , seriously , all , frankly , extraordinary , completely , real\n"
     ]
    }
   ],
   "source": [
    "print(' , '.join(editor.suggest('{it} {be} {a:bert} {pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really, always, truly, also, greatly, just, all, absolutely, certainly, both, definitely, personally, actually, so, especially, still, particularly, thoroughly, never, very, I, sincerely, quite, most, simply, totally, genuinely, much, honestly, obviously, sure, have, clearly, generally, highly, seriously, too, we, REALLY, strongly, had, deeply, people, rather, completely, only, dearly, mostly, already, fully, immediately, even, and, guys, do, hugely, he, family, boys, immensely, long, often, tremendously, did, ever, extremely, many, incredibly, they, surely, literally, specifically, would, sorely, kids, secretly, everyone, probably, profoundly, desperately, instantly, again, usually, then, each, heavily, has, ultimately, who, forever, vastly, fucking, enormously, constantly, Really, kinda, students, further, almost, parents\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('{i} {bert} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "t += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "test = DIR(t.data, monotonic_label, templates=t.templates)\n",
    "suite.add(test, 'intensifiers', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'a little', 'a bit', 'slightly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {air_noun} {be} {pos_adj}.', '{it} {air_noun} {be} {red} {pos_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000, save=True)\n",
    "t += editor.template(['{it} {air_noun} {be} {neg_adj}.', '{it} {air_noun} {be} {red} {neg_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000, save=True)\n",
    "test = DIR(t.data, monotonic_label_down, templates=t.templates)\n",
    "suite.add(test, 'reducers', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVariance: change neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = set(\n",
    "    ['.', 'the', 'The', ',', 'a', 'A', 'and', 'of', 'to', 'it', 'that', 'in',\n",
    "     'this', 'for',  'you', 'there', 'or', 'an', 'by', 'about', 'flight', 'my',\n",
    "     'in', 'of', 'have', 'with', 'was', 'at', 'it', 'get', 'from', 'this', 'Flight', 'plane'\n",
    "    ])\n",
    "forbidden = set(['No', 'no', 'Not', 'not', 'Nothing', 'nothing', 'without'] + pos_adj + neg_adj + pos_verb_present + pos_verb_past + neg_verb_present + neg_verb_past)\n",
    "def change_neutral(d):\n",
    "#     return d.text\n",
    "    examples = []\n",
    "    subs = []\n",
    "    words_in = [x for x in d.capitalize().split() if x in neutral_words]\n",
    "    if not words_in:\n",
    "        return None\n",
    "    for w in words_in:\n",
    "        suggestions = [x for x in editor.suggest_replace(d, w, beam_size=5, words_and_sentences=True) if x[0] not in forbidden]\n",
    "        examples.extend([x[1] for x in suggestions])\n",
    "        subs.extend(['%s -> %s' % (w, x[0]) for x in suggestions])\n",
    "    if examples:\n",
    "        idxs = np.random.choice(len(examples), min(len(examples), 10), replace=False)\n",
    "        return [examples[i] for i in idxs]#, [subs[i] for i in idxs])\n",
    "# Perturb.perturb(parsed_data[:5], perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, change_neutral, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change neutral words with BERT', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = editor.template('I {pos_verb_present} you.').data\n",
    "positive += editor.template('You are {pos_adj}.').data\n",
    "positive += ['I would fly with you again.']\n",
    "positive.remove('You are happy.')\n",
    "negative = editor.template('I {neg_verb_present} you.').data\n",
    "negative += editor.template('You are {neg_adj}.').data\n",
    "negative += ['Never flying with you again.']\n",
    "def add_phrase_function(phrases):\n",
    "    def pert(d):\n",
    "        while d[-1].pos_ == 'PUNCT':\n",
    "            d = d[:-1]\n",
    "        d = d.text\n",
    "        ret = [d + '. ' + x for x in phrases]\n",
    "        idx = np.random.choice(len(ret), 10, replace=False)\n",
    "        ret = [ret[i] for i in idx]\n",
    "        return ret\n",
    "    return pert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_1 = Expect.monotonic(label=2, increasing=True, tolerance=0.1)\n",
    "monotonic_1_down = Expect.monotonic(label=2, increasing=False, tolerance=0.1)\n",
    "monotonic_0 = Expect.monotonic(label=0, increasing=True, tolerance=0.1)\n",
    "monotonic_0_down = Expect.monotonic(label=0, increasing=False, tolerance=0.1)\n",
    "# goes_up = Expect.combine_and(monotonic_1, monotonic_0_down)\n",
    "# goes_down = Expect.combine_and(monotonic_1_down, monotonic_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, add_phrase_function(positive), nsamples=500)\n",
    "test = DIR(t.data, monotonic_1)\n",
    "# test = DIR(t.data, goes_up)\n",
    "suite.add(test, 'add positive phrases', 'Vocabulary', 'TODO_DESCRIPTION', overwrite=True)\n",
    "# test.run(new_pp, overwrite=True)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check this expectation function\n",
    "t = Perturb.perturb(parsed_data, add_phrase_function(negative), nsamples=500)\n",
    "test = DIR(t.data, monotonic_1_down)\n",
    "# test = DIR(t.data, goes_down)\n",
    "suite.add(test, 'add negative phrases', 'Vocabulary', 'TODO_DESCRIPTION', overwrite=True)\n",
    "# test.run(new_pp, overwrite=True)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: robustness\n",
    "### INVariance: adding irrelevant stuff before and after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def random_string(n):\n",
    "    return ''.join(np.random.choice([x for x in string.ascii_letters + string.digits], n))\n",
    "def random_url(n=6):\n",
    "    return 'https://t.co/%s' % random_string(n)\n",
    "def random_handle(n=6):\n",
    "    return '@%s' % random_string(n)\n",
    "\n",
    "# data['sentence']\n",
    "\n",
    "def add_irrelevant(sentence):\n",
    "    urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]\n",
    "    irrelevant_before = ['@airline '] + urls_and_handles\n",
    "    irrelevant_after = urls_and_handles \n",
    "    rets = ['%s %s' % (x, sentence) for x in irrelevant_before ]\n",
    "    rets += ['%s %s' % (sentence, x) for x in irrelevant_after]\n",
    "    return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, add_irrelevant, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'add urls and handles', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation, contractions, typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.punctuation, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'punctuation', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=1)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'typos', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=2)\n",
    "test = INV(t.data)\n",
    "suite.add(test, '2 typos', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.contractions, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'contractions', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change names', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_location, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change locations', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_number, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change numbers', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: temporal awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate', 'dislike', 'regret', 'abhor', 'dread', 'despise']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{neg_verb_present}').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = ['but', 'even though', 'although', '']\n",
    "t = editor.template(['I used to think this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} I used to think it was {neg_adj}.',\n",
    "                                 'In the past I thought this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} in the past I thought it was {neg_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500, save=True, labels=2)\n",
    "t += editor.template(['I used to {neg_verb_present} this airline, {change} now I {pos_verb_present} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} I used to {neg_verb_present} it.',\n",
    "                                 'In the past I would {neg_verb_present} this airline, {change} now I {pos_verb} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} in the past I would {neg_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500, save=True, labels=2)\n",
    "\n",
    "t += editor.template(['I used to think this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} I used to think it was {pos_adj}.',\n",
    "                                 'In the past I thought this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} in the past I thought it was {pos_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500, save=True, labels=0)\n",
    "t += editor.template(['I used to {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} I used to {pos_verb_present} it.',\n",
    "                                 'In the past I would {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} in the past I would {pos_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500, save=True, labels=0)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'used to, but now', 'Temporal', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used to should reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {be} {a:adj} {air_noun}.', 'I used to think {it} {be} {a:adj} {air_noun}.'], it=['it', 'this', 'that'], be=['is', 'was'], adj=editor.lexicons['pos_adj'] + editor.lexicons['neg_adj'], save=True)\n",
    "t += editor.template(['{i} {verb} {the} {air_noun}.', '{i} used to {verb} {the} {air_noun}.'], i=['I', 'We'], the=['this', 'that', 'the'], verb=editor.lexicons['pos_verb_present'] + editor.lexicons['neg_verb_present'], save=True)\n",
    "test = DIR(t.data, monotonic_label_down, templates=t.templates)\n",
    "# test = MFT(data, labels=2)\n",
    "suite.add(test, '\"used to\" should reduce', 'Temporal', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect:fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['male', 'female', 'first_name', 'first_pronoun', 'last_name', 'country', 'nationality', 'city', 'religion', 'religion_adj', 'sexual_adj', 'air_noun', 'pos_adj', 'neg_adj', 'neutral_adj', 'pos_verb_present', 'neg_verb_present', 'neutral_verb_present', 'pos_verb_past', 'neg_verb_past', 'neutral_verb_past', 'pos_verb', 'neg_verb', 'neutral_verb'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a Christian',\n",
       " 'a Protestant',\n",
       " 'a Roman Catholic',\n",
       " 'an Eastern Orthodox',\n",
       " 'an Anglican',\n",
       " 'a Jew',\n",
       " 'an Orthodox Jew',\n",
       " 'a Muslim',\n",
       " 'a Sunni',\n",
       " \"a Shi'a\",\n",
       " 'an Ahmadiyya',\n",
       " 'a Buddhist',\n",
       " 'a Zoroastrian',\n",
       " 'a Hindu',\n",
       " 'a Sikh',\n",
       " 'a Shinto',\n",
       " \"a Baha'i\",\n",
       " 'a Taoist',\n",
       " 'a Confucian',\n",
       " 'a Jain',\n",
       " 'an Atheist',\n",
       " 'an Agnostic']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{a:religion_adj}').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = {\n",
    "    'race': ['a black','a hispanic', 'a white', 'an asian'],\n",
    "    'sexual': editor.template('{a:sexual_adj}').data,\n",
    "    'religion': editor.template('{a:religion_adj}').data,\n",
    "    'nationality': editor.template('{a:nationality}').data[:20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "sexual\n",
      "religion\n",
      "nationality\n"
     ]
    }
   ],
   "source": [
    "for p, vals in protected.items():\n",
    "    print(p)\n",
    "    t = editor.template(['{male} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=300, save=True)\n",
    "    t += editor.template(['{female} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=300, save=True)\n",
    "    test = INV(t.data, threshold=0.1, templates=t.templates)\n",
    "    suite.add(test, 'protected: %s' % p, 'Fairness', 'TODO_DESCRIPTION')\n",
    "#     test.run(new_pp)\n",
    "#     test.summary(n=3)\n",
    "#     print()\n",
    "#     preds = np.array(test.results.preds)\n",
    "#     for i, x in enumerate(vals):\n",
    "#         print('%.2f %s' % (preds[:, i].mean(), vals[i]))\n",
    "#     print()\n",
    "#     print()\n",
    "#     print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {pos_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "t += editor.template('No one {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'simple negations: negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {neg_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {neg_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "t += editor.template('No one {neg_verb_present}s {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "is_not_0 = lambda x, pred, *args: pred != 0\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'simple negations: not negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {neutral_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {neutral_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'simple negations: not neutral is still neutral', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun_it = [x for x in editor.lexicons['air_noun'] if x != 'pilot']\n",
    "t = editor.template('I thought {it} {air_noun} would be {pos_adj}, but it {neg}.', air_noun=air_noun_it, neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {pos_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'simple negations: but I did not (negative)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2124 examples\n",
      "Test cases:      2124\n",
      "Fails (rate):    1794 (84.5%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 I thought the staff would be lousy, but it wasn't.\n",
      "----\n",
      "0.9 0.0 0.1 I thought the flight would be unhappy, but it was not.\n",
      "----\n",
      "1.0 0.0 0.0 I thought this aircraft would be nasty, but it wasn't.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {neg_adj}, but it {neg}.', air_noun=air_noun_it, neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {neg_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "# suite.add(test, 'simple negations: but I did not (not negative)', 'Negation', 'TODO_DESCRIPTION')\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {neutral_adj}, but it {neg}.', air_noun=air_noun_it, neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {neutral_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'simple negations: but it was not (neutral)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harder: negation with neutral in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neg = neg[:-1]\n",
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {pos_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:pos_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {pos_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neg_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:neg_adj} {air_noun}.',neutral=neutral,  neg=['i don\\'t think', 'i can\\'t say', 'i wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {neg_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['i don\\'t think', 'i can\\'t say', 'i wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle, not negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neutral_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:neutral_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {neutral_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle, neutral', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: SRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my opinion is more important than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = [' but', '']\n",
    "templates = ['Some people think you are {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} some people think you are {neg_adj}.',\n",
    "             'I had heard you were {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} I had heard you were {neg_adj}.',\n",
    "             ]\n",
    "t = editor.template(templates, change=change, unroll=True, labels=2, save=True)\n",
    "templates = ['{others} {neg_verb_present} you,{change} I {pos_verb_present} you.',\n",
    "             'I {pos_verb_present} you,{change} {others} {neg_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "t += editor.template(templates, others=others, change=change, unroll=True, labels=2, save=True)\n",
    "\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} some people think you are {pos_adj}.',\n",
    "             'I had heard you were {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} I had heard you were {pos_adj}.',\n",
    "             ]\n",
    "t += editor.template(templates, change=change, unroll=True, labels=0, save=True)\n",
    "templates = ['{others} {pos_verb_present} you,{change} I {neg_verb_present} you.',\n",
    "             'I {neg_verb_present} you,{change} {others} {pos_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "t += editor.template(templates, others=others, change=change, unroll=True, labels=0, save=True)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'my opinion is what matters', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q & a form: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=2)\n",
    "t += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=2)\n",
    "t += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=2)\n",
    "t += editor.template('Do I think {it} {air_noun} {be} {neg_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=0)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'Q & A: yes', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'Q & A: yes (neutral)', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {air_noun} {be} {neg_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=1)\n",
    "t += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=1)\n",
    "t += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=1)\n",
    "allow_for_neutral = lambda x, pred, _, label, _2 : pred != 0 if label == 1 else pred == label\n",
    "test = MFT(t.data, Expect.single(allow_for_neutral), labels=t.labels, templates=t.templates)\n",
    "suite.add(test, 'Q & A: no', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'Q & A: no (neutral)', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.save('/home/marcotcr/tmp/sentiment_suite.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite.from_file('/home/marcotcr/tmp/sentiment_suite.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.to_raw_file('/tmp/a', n=300, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = open('/tmp/a', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53303"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1545786"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, confs = new_pp(texts)\n",
    "open('/tmp/b', 'w').write('\\n'.join(['%d %f %f %f' % (pred, *c) for pred, c in zip(preds, confs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "individual positive words\n",
      "Test cases:      34\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "individual negative words\n",
      "Test cases:      35\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "individual neutral words\n",
      "Test cases:      13\n",
      "Fails (rate):    13 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 see\n",
      "----\n",
      "0.0 0.0 1.0 American\n",
      "----\n",
      "0.0 0.0 1.0 Italian\n",
      "----\n",
      "\n",
      "\n",
      "sentiment words in context\n",
      "Test cases:      8658\n",
      "Test cases run:  300\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "neutral words in context\n",
      "Test cases:      1716\n",
      "Test cases run:  300\n",
      "Fails (rate):    287 (95.7%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 That pilot was American.\n",
      "----\n",
      "0.0 0.0 1.0 That plane is Australian.\n",
      "----\n",
      "0.1 0.0 0.9 It was an American aircraft.\n",
      "----\n",
      "\n",
      "\n",
      "intensifiers\n",
      "Test cases:      2000\n",
      "Test cases run:  300\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "reducers\n",
      "Test cases:      2000\n",
      "Test cases run:  300\n",
      "After filtering: 0 (0.0%)\n",
      "\n",
      "\n",
      "change neutral words with BERT\n",
      "Test cases:      500\n",
      "Test cases run:  300\n",
      "Fails (rate):    37 (12.3%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 @JetBlue Something to think about when you are dealing with PEOPLE!!!!!  Just wrong.  No excuses. Figure it out &amp; expect the unexpected\n",
      "0.1 0.9 0.0 @JetBlue Something to think of when you are dealing with PEOPLE!!!!!  Just wrong.  No excuses. Figure it out &amp; expect the unexpected\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 @united annnnddddd I'm going to lose my first class seat on the flight they shuffle me to\n",
      "0.0 0.0 1.0 @united annnnddddd I'm going t lose my first class seat on the flight they shuffle me t\n",
      "\n",
      "----\n",
      "0.1 0.0 0.9 @united well for a start it would be nice if your ticket was actually in the system when you change a flight\n",
      "0.9 0.0 0.1 @united well for a start but would be nice if your ticket was actually in the system when you change a flight\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "add positive phrases\n",
      "Test cases:      500\n",
      "Test cases run:  300\n",
      "After filtering: 83 (27.7%)\n",
      "Fails (rate):    2 (2.4%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.0 0.9 @united - I think she was having a rough moment w/ a bad passenger from an earlier flight. Things got considerably better. Thanks!\n",
      "0.9 0.0 0.1 @united - I think she was having a rough moment w/ a bad passenger from an earlier flight. Things got considerably better. Thanks. I would fly with you again.\n",
      "\n",
      "----\n",
      "0.1 0.0 0.9 @united - I think she was having a rough moment w/ a bad passenger from an earlier flight. Things got considerably better. Thanks!\n",
      "0.9 0.0 0.1 @united - I think she was having a rough moment w/ a bad passenger from an earlier flight. Things got considerably better. Thanks. I would fly with you again.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "add negative phrases\n",
      "Test cases:      500\n",
      "Test cases run:  300\n",
      "After filtering: 224 (74.7%)\n",
      "Fails (rate):    2 (0.9%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.5 0.5 @united Hi, I am flying domestic first from SEA to HNL. Can I pay to use the lounge before I fly or this is just for international? Thanks\n",
      "0.3 0.0 0.7 @united Hi, I am flying domestic first from SEA to HNL. Can I pay to use the lounge before I fly or this is just for international? Thanks. You are rough.\n",
      "\n",
      "----\n",
      "0.0 0.5 0.5 @united Hi, I am flying domestic first from SEA to HNL. Can I pay to use the lounge before I fly or this is just for international? Thanks\n",
      "0.3 0.0 0.7 @united Hi, I am flying domestic first from SEA to HNL. Can I pay to use the lounge before I fly or this is just for international? Thanks. You are rough.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robustness\n",
      "\n",
      "add urls and handles\n",
      "Test cases:      500\n",
      "Test cases run:  300\n",
      "Fails (rate):    26 (8.7%)\n",
      "\n",
      "Example fails:\n",
      "0.9 0.0 0.1 @united I know. I’m sure you hear it all the time — don’t like the new seats. I’m a small guy, 5’ 6” and I feel the new planes are tight\n",
      "0.0 1.0 0.0 @XtDsRD @united I know. I’m sure you hear it all the time — don’t like the new seats. I’m a small guy, 5’ 6” and I feel the new planes are tight\n",
      "0.4 0.6 0.0 @FKzO6o @united I know. I’m sure you hear it all the time — don’t like the new seats. I’m a small guy, 5’ 6” and I feel the new planes are tight\n",
      "\n",
      "----\n",
      "0.0 1.0 0.0 @JetBlue Can I bring my dog on board?\n",
      "0.8 0.0 0.2 @EnZqhQ @JetBlue Can I bring my dog on board?\n",
      "0.9 0.0 0.1 @JetBlue Can I bring my dog on board? @iG7mxr\n",
      "\n",
      "----\n",
      "0.8 0.0 0.2 @SouthwestAir yep. 99.99999999% certain it was on that flight.\n",
      "0.4 0.6 0.0 @airline  @SouthwestAir yep. 99.99999999% certain it was on that flight.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "punctuation\n",
      "Test cases:      500\n",
      "Test cases run:  300\n",
      "Fails (rate):    16 (5.3%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 @JetBlue Well, thankfully they've got a nice food court here...When will an update be posted?\n",
      "0.2 0.8 0.0 @JetBlue Well, thankfully they've got a nice food court here...When will an update be posted.\n",
      "\n",
      "----\n",
      "0.7 0.0 0.3 @united @SCVPools \n",
      "Please call me at 310-795-2210.\n",
      "0.5 0.5 0.0 @united @SCVPools \n",
      "Please call me at 310-795-2210\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 @united right... Are you guys charging for the air we breathe next?\n",
      "0.0 0.0 1.0 @united right... Are you guys charging for the air we breathe next\n",
      "0.0 0.0 1.0 @united right... Are you guys charging for the air we breathe next.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "typos\n",
      "Test cases:      500\n",
      "Test cases run:  300\n",
      "Fails (rate):    31 (10.3%)\n",
      "\n",
      "Example fails:\n",
      "0.3 0.7 0.0 @VirginAmerica any plans to start flying direct from DAL to LAS?\n",
      "0.8 0.0 0.2 @VirginAmerica any plans ot start flying direct from DAL to LAS?\n",
      "\n",
      "----\n",
      "0.9 0.0 0.1 @USAirways 3818 delayed. blue skies all over. #whyisusairALWAYSDELAYED???\n",
      "0.3 0.7 0.0 @USAirways 3818 delayed. blue skies all over. #wyhisusairALWAYSDELAYED???\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 @JetBlue shocker my flight delayed an hour. Thanks for reminding me why I switched to delta.\n",
      "0.0 0.0 1.0 @JetBlue shocker my flight delaeyd an hour. Thanks for reminding me why I switched to delta.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "2 typos\n",
      "Test cases:      500\n",
      "Test cases run:  300\n",
      "Fails (rate):    25 (8.3%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.9 0.1 @VirginAmerica are you ready!? Let's say it together.. 'Noooo turbulence today!' 😘\n",
      "0.1 0.0 0.9 @VirginAmerica are yuo ready!? Let's say it together.. 'oNooo turbulence today!' 😘\n",
      "\n",
      "----\n",
      "0.1 0.0 0.9 .@JetBlue process begins 6:48\n",
      "0.9 0.0 0.1 .@JetBlue processb egnis 6:48\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 @united is by far THE WORST airline I've ever had the misfortune of flying with in all the 44 countries I've flown to!!! It's outrageous!\n",
      "0.0 0.0 1.0 @united is by far THE WORTS ailrine I've ever had the misfortune of flying with in all the 44 countries I've flown to!!! It's outrageous!\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "contractions\n",
      "Test cases:      1000\n",
      "Test cases run:  300\n",
      "Fails (rate):    5 (1.7%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.5 0.5 @united Hi, I am flying domestic first from SEA to HNL. Can I pay to use the lounge before I fly or this is just for international? Thanks\n",
      "0.3 0.0 0.7 @united Hi, I'm flying domestic first from SEA to HNL. Can I pay to use the lounge before I fly or this is just for international? Thanks\n",
      "\n",
      "----\n",
      "0.1 0.0 0.9 @united Don't ask me to be patient without offering something in return.\n",
      "0.9 0.0 0.1 @united Do not ask me to be patient without offering something in return.\n",
      "\n",
      "----\n",
      "0.0 0.6 0.4 @JetBlue if you want to be helpful, find us a seat. Don't quote terms and conditions. #jetbluefail\n",
      "0.7 0.0 0.3 @JetBlue if you want to be helpful, find us a seat. Do not quote terms and conditions. #jetbluefail\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporal\n",
      "\n",
      "used to, but now\n",
      "Test cases:      8000\n",
      "Test cases run:  300\n",
      "Fails (rate):    59 (19.7%)\n",
      "\n",
      "Example fails:\n",
      "0.2 0.0 0.8 I think this airline is creepy,  I used to think it was adorable.\n",
      "----\n",
      "0.0 0.0 1.0 I think this airline is weird, but I used to think it was perfect.\n",
      "----\n",
      "1.0 0.0 0.0 I like this airline,  I used to dislike it.\n",
      "----\n",
      "\n",
      "\n",
      "\"used to\" should reduce\n",
      "Test cases:      4368\n",
      "Test cases run:  300\n",
      "After filtering: 1 (0.3%)\n",
      "Fails (rate):    1 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.7 0.0 0.3 it was an average seat.\n",
      "1.0 0.0 0.0 I used to think it was an average seat.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness\n",
      "\n",
      "protected: race\n",
      "Test cases:      600\n",
      "Test cases run:  300\n",
      "Fails (rate):    260 (86.7%)\n",
      "\n",
      "Example fails:\n",
      "0.8 0.0 0.2 Amy is a black Canadian.\n",
      "0.0 0.0 1.0 Amy is an asian Canadian.\n",
      "0.1 0.0 0.9 Amy is a hispanic Canadian.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 Hailey is a black community.\n",
      "0.0 0.0 1.0 Hailey is a hispanic community.\n",
      "0.0 0.0 1.0 Hailey is an asian community.\n",
      "\n",
      "----\n",
      "0.9 0.0 0.1 Adrian is a black American.\n",
      "0.0 0.0 1.0 Adrian is a hispanic American.\n",
      "0.0 0.0 1.0 Adrian is an asian American.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: sexual\n",
      "Test cases:      600\n",
      "Test cases run:  300\n",
      "Fails (rate):    259 (86.3%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 Stephen is an asexual parent.\n",
      "1.0 0.0 0.0 Stephen is a gay parent.\n",
      "1.0 0.0 0.0 Stephen is a homosexual parent.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 April is an asexual person.\n",
      "0.4 0.6 0.0 April is a transsexual person.\n",
      "1.0 0.0 0.0 April is a trans person.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Avery is an asexual mother.\n",
      "0.2 0.8 0.0 Avery is a transgender mother.\n",
      "1.0 0.0 0.0 Avery is a gay mother.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: religion\n",
      "Test cases:      600\n",
      "Test cases run:  300\n",
      "Fails (rate):    277 (92.3%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 Evan is a Christian citizen.\n",
      "1.0 0.0 0.0 Evan is an Atheist citizen.\n",
      "1.0 0.0 0.0 Evan is an Agnostic citizen.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Anna is a Christian grandmother.\n",
      "0.9 0.0 0.1 Anna is an Atheist grandmother.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Destiny is a Christian minister.\n",
      "1.0 0.0 0.0 Destiny is an Atheist minister.\n",
      "1.0 0.0 0.0 Destiny is an Agnostic minister.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: nationality\n",
      "Test cases:      600\n",
      "Test cases run:  300\n",
      "Fails (rate):    47 (15.7%)\n",
      "\n",
      "Example fails:\n",
      "0.3 0.7 0.0 Mark is a Chinese Jew.\n",
      "0.1 0.0 0.9 Mark is an Indian Jew.\n",
      "0.1 0.0 0.9 Mark is an American Jew.\n",
      "\n",
      "----\n",
      "0.2 0.0 0.8 Jacqueline is a Chinese blogger.\n",
      "0.0 0.6 0.4 Jacqueline is a German blogger.\n",
      "\n",
      "----\n",
      "0.3 0.7 0.0 Mark is a Chinese Jew.\n",
      "0.1 0.0 0.9 Mark is an Indian Jew.\n",
      "0.1 0.0 0.9 Mark is an American Jew.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRL\n",
      "\n",
      "my opinion is what matters\n",
      "Test cases:      8528\n",
      "Test cases run:  300\n",
      "Fails (rate):    103 (34.3%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 I hate you, my parents recommend you.\n",
      "----\n",
      "1.0 0.0 0.0 I think you are amazing, but I had heard you were poor.\n",
      "----\n",
      "0.0 0.0 1.0 I think you are difficult, I had heard you were sweet.\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes\n",
      "Test cases:      7644\n",
      "Test cases run:  300\n",
      "Fails (rate):    10 (3.3%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 Do I think this customer service was nice? Yes\n",
      "----\n",
      "0.1 0.0 0.9 Did I dread the seat? Yes\n",
      "----\n",
      "0.4 0.6 0.0 Did we dislike this flight? Yes\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes (neutral)\n",
      "Test cases:      1560\n",
      "Test cases run:  300\n",
      "Fails (rate):    294 (98.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 Do I think that was a private food? Yes\n",
      "----\n",
      "1.0 0.0 0.0 Do I think that is an Australian company? Yes\n",
      "----\n",
      "1.0 0.0 0.0 Do I think that was a private food? Yes\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: no\n",
      "Test cases:      7644\n",
      "Test cases run:  300\n",
      "Fails (rate):    150 (50.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 Do I think this staff is unhappy? No\n",
      "----\n",
      "1.0 0.0 0.0 Do I think the airline is unpleasant? No\n",
      "----\n",
      "1.0 0.0 0.0 Do I think that was a lousy airline? No\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: no (neutral)\n",
      "Test cases:      1560\n",
      "Test cases run:  300\n",
      "Fails (rate):    300 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 Do I think it is an Israeli aircraft? No\n",
      "----\n",
      "1.0 0.0 0.0 Do I think the staff was international? No\n",
      "----\n",
      "1.0 0.0 0.0 Do I think that is an Australian crew? No\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negation\n",
      "\n",
      "simple negations: negative\n",
      "Test cases:      6318\n",
      "Test cases run:  300\n",
      "Fails (rate):    39 (13.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 I can't say I enjoy this cabin crew.\n",
      "----\n",
      "0.0 0.0 1.0 I would never say I appreciate the crew.\n",
      "----\n",
      "0.0 0.0 1.0 I can't say I welcome the plane.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not negative\n",
      "Test cases:      6786\n",
      "Test cases run:  300\n",
      "Fails (rate):    32 (10.7%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 I would never say I dread that pilot.\n",
      "----\n",
      "0.8 0.0 0.2 I didn't dislike this cabin crew.\n",
      "----\n",
      "1.0 0.0 0.0 I can't say I despise that crew.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not neutral is still neutral\n",
      "Test cases:      2496\n",
      "Test cases run:  300\n",
      "Fails (rate):    298 (99.3%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 This wasn't an Italian flight.\n",
      "----\n",
      "1.0 0.0 0.0 This aircraft is not American.\n",
      "----\n",
      "1.0 0.0 0.0 I didn't see the flight.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but I did not (negative)\n",
      "Test cases:      1992\n",
      "Test cases run:  300\n",
      "Fails (rate):    5 (1.7%)\n",
      "\n",
      "Example fails:\n",
      "0.2 0.0 0.8 I thought I would admire that service, but I did not.\n",
      "----\n",
      "0.0 0.6 0.4 I thought I would admire the plane, but I did not.\n",
      "----\n",
      "0.2 0.0 0.8 I thought I would admire the flight, but I did not.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but it was not (neutral)\n",
      "Test cases:      804\n",
      "Test cases run:  300\n",
      "Fails (rate):    297 (99.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 I thought that crew would be British, but it wasn't.\n",
      "----\n",
      "1.0 0.0 0.0 I thought the plane would be private, but it wasn't.\n",
      "----\n",
      "1.0 0.0 0.0 I thought the flight would be Indian, but it wasn't.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle\n",
      "Test cases:      1000\n",
      "Test cases run:  300\n",
      "Fails (rate):    220 (73.3%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 I can't say, given that I am from Brazil, that that cabin crew is wonderful.\n",
      "----\n",
      "0.0 0.0 1.0 I can't say, given the time that I've been flying, that this service was sweet.\n",
      "----\n",
      "0.0 0.0 1.0 I can't say, given it's a Tuesday, that the was a brilliant company.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle, not negative\n",
      "Test cases:      1000\n",
      "Test cases run:  300\n",
      "Fails (rate):    300 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 i don't think, given my history with airplanes, that this was a terrible aircraft.\n",
      "----\n",
      "1.0 0.0 0.0 I can't say, given all that I've seen over the years, that the staff was awful.\n",
      "----\n",
      "1.0 0.0 0.0 I don't think, given my history with airplanes, that that cabin crew was unhappy.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle, neutral\n",
      "Test cases:      1000\n",
      "Test cases run:  300\n",
      "Fails (rate):    292 (97.3%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 I don't think, given my history with airplanes, that this crew is Australian.\n",
      "----\n",
      "0.9 0.0 0.1 I can't say, given all that I've seen over the years, that this is an Italian cabin crew.\n",
      "----\n",
      "1.0 0.0 0.0 I can't say, given my history with airplanes, that the was an American plane.\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NER\n",
      "\n",
      "change names\n",
      "Test cases:      331\n",
      "Test cases run:  300\n",
      "Fails (rate):    26 (8.7%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.0 0.9 @USAirways on your website and on your boards at Logan it said it was on time, so we went through security and got to the gate (2)\n",
      "0.0 0.5 0.5 @USAirways on your website and on your boards at Jeremiah it said it was on time, so we went through security and got to the gate (2)\n",
      "\n",
      "----\n",
      "0.2 0.8 0.0 @AmericanAir it wasn't ' disrupted' it was Cancelled Flightled. Airport agents were horrendous. Sharon was your saviour\n",
      "0.8 0.0 0.2 @AmericanAir it wasn't ' disrupted' it was Cancelled Flightled. Airport agents were horrendous. Brittany was your saviour\n",
      "0.7 0.0 0.3 @AmericanAir it wasn't ' disrupted' it was Cancelled Flightled. Airport agents were horrendous. Courtney was your saviour\n",
      "\n",
      "----\n",
      "0.2 0.0 0.8 @united always makes our cross country flights rad. @hemispheresmag here's baby flier Charlotte! #8thtime5MonthsOld http://t.co/kCqnwIXUCm\n",
      "0.0 0.5 0.5 @united always makes our cross country flights rad. @hemispheresmag here's baby flier Monica! #8thtime5MonthsOld http://t.co/kCqnwIXUCm\n",
      "0.0 0.5 0.5 @united always makes our cross country flights rad. @hemispheresmag here's baby flier Madison! #8thtime5MonthsOld http://t.co/kCqnwIXUCm\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change locations\n",
      "Test cases:      909\n",
      "Test cases run:  300\n",
      "Fails (rate):    20 (6.7%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 @JetBlue what's good with a Miami terminal?\n",
      "0.4 0.6 0.0 @JetBlue what's good with a Indio terminal?\n",
      "0.9 0.0 0.1 @JetBlue what's good with a State College terminal?\n",
      "\n",
      "----\n",
      "0.8 0.0 0.2 @united The Opal Dragon book The Dragon (ALI) has woven his murdering ways from the Philippines to Australia http://t.co/N2fvElcYgz\n",
      "0.3 0.0 0.7 @united The Opal Dragon book The Dragon (ALI) has woven his murdering ways from the St. Vincent and the Grenadines to Australia http://t.co/N2fvElcYgz\n",
      "0.3 0.7 0.0 @united The Opal Dragon book The Dragon (ALI) has woven his murdering ways from the Denmark to Australia http://t.co/N2fvElcYgz\n",
      "\n",
      "----\n",
      "0.8 0.0 0.2 @united can you ask your guys with flight 1146 to BWI to wait for us to get off a delayed flight from San Diego? Pretty please?\n",
      "0.0 0.9 0.1 @united can you ask your guys with flight 1146 to BWI to wait for us to get off a delayed flight from Palm Beach Gardens? Pretty please?\n",
      "0.2 0.8 0.0 @united can you ask your guys with flight 1146 to BWI to wait for us to get off a delayed flight from Flower Mound? Pretty please?\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change numbers\n",
      "Test cases:      1000\n",
      "Test cases run:  300\n",
      "Fails (rate):    8 (2.7%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.6 0.4 @SouthwestAir #3854 ATL to RDU. Snow forecasted for Raleigh this evening.\n",
      "0.3 0.0 0.7 @SouthwestAir #4508 ATL to RDU. Snow forecasted for Raleigh this evening.\n",
      "\n",
      "----\n",
      "0.0 0.9 0.1 @americanair the best is your 800 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "0.3 0.0 0.7 @americanair the best is your 705 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "0.0 0.0 1.0 @americanair the best is your 839 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "\n",
      "----\n",
      "0.7 0.0 0.3 @USAirways   my miles will expire on 2/29 and it could take someone 10 days to respond...I have over 150000 miles that I do not lose i❤usair\n",
      "0.3 0.7 0.0 @USAirways   my miles will expire on 2/29 and it could take someone 10 days to respond...I have over 148251 miles that I do not lose i❤usair\n",
      "0.4 0.6 0.0 @USAirways   my miles will expire on 2/29 and it could take someone 10 days to respond...I have over 140534 miles that I do not lose i❤usair\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.run_from_file('/tmp/b', file_format='pred_and_softmax', overwrite=True)\n",
    "suite.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suite.run_from_file('/tmp/azure', file_format='pred_and_softmax', overwrite=True)\n",
    "# suite.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suite.run_from_file('/tmp/google', file_format='pred_and_softmax', overwrite=True)\n",
    "# suite.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# suite.run_from_file('/tmp/amazon', file_format='pred_and_softmax', overwrite=True)\n",
    "# suite.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suite.run(new_pp, n=300, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# suite.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
