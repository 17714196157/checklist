{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import MFT\n",
    "from checklist.inv_dir import INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<checklist.text_generation.TextGenerator at 0x7f75e3b385f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "editor.tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent, amazing, incredible, American, international, ordinary, good, great, extraordinary, aggressive, small, awful, interesting, average, exceptional, unusual, experimental, awesome, old, big, impressive, outstanding, expensive, important, elite, exciting, enormous, odd, new, independent, nice, angry, ugly, anonymous, private, active, little, real, different, beautiful\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('It was {a:bert} {air_noun}.')[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'tough', 'weird', 'aggressive', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liked, like, enjoyed, appreciate, enjoy, loved, appreciated, love, miss, missed, likes, recommend, wanted, got, admired, hate, admire, prefer, needed, need, want, dig, trust, value, enjoys, dislike, use, enjoying, respect, did, understand, liking, get, found, respected, used, dug, adore, preferred, tried, loves, regret, valued, feel, ,, underestimated, bought, took, hated, was, understood, rate, praised, helped, is, disliked, have, see, improved, do, compliment, remember, LOVE, noticed, support, felt, about, hit, had, saw, applaud, mean, picked, know, believe, supported, chose, left, into, commend, for, thank, help, tested, sold, trusted, Love, values, cherish, blame, impressed, take, thanks, treasure, underestimate, welcome, made, recommended, misses, upgraded, received, beat, delivered, started, changed, are, owe, finished, worked, wish, try, fancy, experienced, leave, thought, ordered, credit, all, think, drove, envy, follow, own, brought, joined, in, buy, respects, welcomed, considered, flew, rode, to, reviewed, embrace, improve, consider, drive, met, packed, cleaned, built, lost, saved, favor, experience, fit, doubt, notice, compliments, ride, needs, Like, fly, embraced, owned, exited, complement, at, choose, meant, just, learned, LIKE, recognize, read, crashed, survived, regretted, rocked, offer, expanded, handled, deliver, utilize, followed, provide, thanked, on, told, spoiled, surprised, deserved, pleased, meet, recruited, uses, hired, loving, salute, designed, forgive, remembered, pushed, praise, stressed, hope, using, am, watched\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('I really {bert} the {air_noun}.')[:200]))\n",
    "# print()\n",
    "# print(', '.join(editor.suggest('I {bert} the {air_noun}.')[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 34 examples\n",
      "Test cases:      34\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "test = MFT(pos_adj + pos_verb_present + pos_verb_past, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 37 examples\n",
      "Test cases:      37\n",
      "Fails (rate):    2 (5.4%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) tough\n",
      "----\n",
      "2 (1.0) aggressive\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test = MFT(neg_adj + neg_verb_present + neg_verb_past, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 13 examples\n",
      "Test cases:      13\n",
      "Fails (rate):    13 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Indian\n",
      "----\n",
      "2 (1.0) find\n",
      "----\n",
      "2 (1.0) British\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test = MFT(neutral_adj + neutral_verb_present + neutral_verb_past, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8970 examples\n",
      "Test cases:      8970\n",
      "Fails (rate):    173 (1.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) It is an aggressive food.\n",
      "----\n",
      "2 (1.0) The plane is aggressive.\n",
      "----\n",
      "2 (1.0) It was an aggressive service.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels = [2] * len(data)\n",
    "data += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = MFT(data, labels=labels)\n",
    "# test = MFT(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1716 examples\n",
      "Test cases:      1716\n",
      "Fails (rate):    1632 (95.1%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) The pilot is international.\n",
      "----\n",
      "2 (1.0) We find that pilot.\n",
      "----\n",
      "2 (1.0) I found the customer service.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {be} {neutral_adj}.', it=['That', 'This', 'The'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {neutral_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "test = MFT(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really , very , absolutely , pretty , incredibly , extremely , truly , quite , amazingly , super , exceptionally , actually , extraordinarily , especially , unbelievably , rather , insanely , equally , fairly , particularly , surprisingly , seriously , awfully , exceedingly , unusually , damn , totally , utterly , immensely , amazing , most , just , enormously , overall , absolute , unexpectedly , incredible , extra , genuinely , a , undeniably , real , overwhelmingly , obviously , entirely , otherwise , almost , all , oddly , completely\n"
     ]
    }
   ],
   "source": [
    "print(' , '.join(editor.suggest('{it} {be} {a:bert} {pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really, I, always, all, just, also, truly, absolutely, personally, certainly, both, definitely, so, still, greatly, actually, genuinely, especially, much, sure, particularly, thoroughly, quite, we, very, totally, seriously, never, people, clearly, guys, honestly, obviously, have, sincerely, most, 've, do, REALLY, 'd, simply, highly, rather, generally, too, only, did, already, deeply, and, you, family, dearly, had, even, fully, ly, ever, they, completely, Really, everyone, would, immediately, kids, strongly, fucking, ,, boys, kinda, he, parents, students, immensely, mostly, o, 's, ..., extremely, tremendously, probably, hugely, feel, will, long, who, should, 'll, secretly, profoundly, i, must, one, ers, incredibly, forever, desperately, many, almost, surely\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('{i} {bert} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:      2000\n",
      "After filtering: 1996 (99.8%)\n",
      "Fails (rate):    20 (1.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) This is a tough food.\n",
      "2 (0.8) This is an unbelievably tough food.\n",
      "\n",
      "----\n",
      "0 (1.0) This is a tough flight.\n",
      "2 (0.9) This is an unbelievably tough flight.\n",
      "\n",
      "----\n",
      "0 (1.0) This was a hard customer service.\n",
      "2 (1.0) This was an amazingly hard customer service.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "data += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "test = DIR(data, monotonic_label)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'a little', 'a bit', 'slightly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'editor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9996b63d80c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{it} {air_noun} {be} {pos_adj}.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{it} {air_noun} {be} {red} {pos_adj}.'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreducer_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'The'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'This'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'That'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'was'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{it} {air_noun} {be} {neg_adj}.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{it} {air_noun} {be} {red} {neg_adj}.'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreducer_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'The'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'This'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'That'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'was'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonotonic_label_down\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_pp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'editor' is not defined"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {air_noun} {be} {pos_adj}.', '{it} {air_noun} {be} {red} {pos_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "data += editor.template(['{it} {air_noun} {be} {neg_adj}.', '{it} {air_noun} {be} {red} {neg_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "test = DIR(data, monotonic_label_down)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVariance: change neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = set(\n",
    "    ['.', 'the', 'The', ',', 'a', 'A', 'and', 'of', 'to', 'it', 'that', 'in',\n",
    "     'this', 'for',  'you', 'there', 'or', 'an', 'by', 'about', 'flight', 'my',\n",
    "     'in', 'of', 'have', 'with', 'was', 'at', 'it', 'get', 'from', 'this', 'Flight', 'plane'\n",
    "    ])\n",
    "forbidden = set(['No', 'no', 'Not', 'not', 'Nothing', 'nothing', 'without'] + pos_adj + neg_adj + pos_verb_present + pos_verb_past + neg_verb_present + neg_verb_past)\n",
    "def change_neutral(d):\n",
    "#     return d.text\n",
    "    examples = []\n",
    "    words_in = [x for x in d.capitalize().split() if x in neutral_words]\n",
    "    if not words_in:\n",
    "        return None\n",
    "    for w in words_in:\n",
    "        examples.extend([x[1] for x in editor.suggest_replace(d, w, beam_size=5, words_and_sentences=True) if x[0] not in forbidden])\n",
    "    if examples:\n",
    "        idxs = np.random.choice(len(examples), min(len(examples), 10), replace=False)\n",
    "        return [examples[i] for i in idxs]\n",
    "# Perturb.perturb(parsed_data[:5], perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Perturb.perturb(sentences, change_neutral, nsamples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5076 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    66 (13.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) @AmericanAir five times at last count\n",
      "1 (0.9) @AmericanAir five times : last count\n",
      "1 (0.7) @AmericanAir five times - last count\n",
      "\n",
      "----\n",
      "2 (0.9) @VirginAmerica now it's just t-minus 32 minutes until my Elevate a Silver upgrade window opens . #FreeNeverSucks 😃👍\n",
      "1 (0.9) @VirginAmerica now it's just t-minus 32 minutes until my Elevate x Silver upgrade window opens . #FreeNeverSucks 😃👍\n",
      "0 (0.7) @VirginAmerica now it's just t-minus 32 minutes until my Elevate or Silver upgrade window opens . #FreeNeverSucks 😃👍\n",
      "\n",
      "----\n",
      "0 (1.0) @JetBlue Charleston - from your app guys as of a few minutes ago!!! http://t.co/sEUlpvfn95\n",
      "1 (0.9) @JetBlue Charleston - Got your app guys as of a few minutes ago!!! http://t.co/sEUlpvfn95\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = editor.template('I {pos_verb_present} you.')\n",
    "positive += editor.template('You are {pos_adj}.')\n",
    "positive += ['I would fly with you again.']\n",
    "positive.remove('You are happy.')\n",
    "negative = editor.template('I {neg_verb_present} you.')\n",
    "negative += editor.template('You are {neg_adj}.')\n",
    "negative += ['Never flying with you again.']\n",
    "def add_phrase_function(phrases):\n",
    "    def pert(d):\n",
    "        while d[-1].pos_ == 'PUNCT':\n",
    "            d = d[:-1]\n",
    "        d = d.text\n",
    "        ret = [d + '. ' + x for x in phrases]\n",
    "        idx = np.random.choice(len(ret), 10, replace=False)\n",
    "        ret = [ret[i] for i in idx]\n",
    "        return ret\n",
    "    return pert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_1 = Expect.monotonic(label=2, increasing=True, tolerance=0.1)\n",
    "monotonic_1_down = Expect.monotonic(label=2, increasing=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5500 examples\n",
      "Test cases:      500\n",
      "After filtering: 148 (29.6%)\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, add_phrase_function(positive), nsamples=500)\n",
    "test = DIR(data, monotonic_1)\n",
    "test.run(new_pp, overwrite=True)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5500 examples\n",
      "Test cases:      500\n",
      "After filtering: 383 (76.6%)\n",
      "Fails (rate):    75 (19.6%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) @AmericanAir #AmericanAirlines 1 1/2 hr wait for bags. If this is the a taste of the worlds largest airline, heaven help aviation\n",
      "2 (1.0) @AmericanAir #AmericanAirlines 1 1/2 hr wait for bags. If this is the a taste of the worlds largest airline, heaven help aviation. You are aggressive.\n",
      "\n",
      "----\n",
      "0 (1.0) @AmericanAir Do you have any sort of live chat feature? We're in the UK right now and that call would cost us alot. :(\n",
      "0 (0.8) @AmericanAir Do you have any sort of live chat feature? We're in the UK right now and that call would cost us alot. You are tough.\n",
      "\n",
      "----\n",
      "0 (1.0) @USAirways Bag policy says stroller OR car seat.  We need to bring 2 car seats and 1 stroller.  Should I just call for details?\n",
      "0 (0.8) @USAirways Bag policy says stroller OR car seat.  We need to bring 2 car seats and 1 stroller.  Should I just call for details. You are aggressive.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, add_phrase_function(negative), nsamples=500)\n",
    "test = DIR(data, monotonic_1_down)\n",
    "test.run(new_pp, overwrite=True)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: robustness\n",
    "### INVariance: adding irrelevant stuff before and after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def random_string(n):\n",
    "    return ''.join(np.random.choice([x for x in string.ascii_letters + string.digits], n))\n",
    "def random_url(n=6):\n",
    "    return 'https://t.co/%s' % random_string(n)\n",
    "def random_handle(n=6):\n",
    "    return '@%s' % random_string(n)\n",
    "\n",
    "# data['sentence']\n",
    "\n",
    "def add_irrelevant(sentence):\n",
    "    urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]\n",
    "    irrelevant_before = ['@airline '] + urls_and_handles\n",
    "    irrelevant_after = urls_and_handles \n",
    "    rets = ['%s %s' % (x, sentence) for x in irrelevant_before ]\n",
    "    rets += ['%s %s' % (sentence, x) for x in irrelevant_after]\n",
    "    return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 11000 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    55 (11.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) @united Hi! what is the phone number for reservations in Venezuela? Thanks\n",
      "1 (0.6) @united Hi! what is the phone number for reservations in Venezuela? Thanks @YR9xj5\n",
      "1 (0.8) https://t.co/vlt542 @united Hi! what is the phone number for reservations in Venezuela? Thanks\n",
      "0 (0.8) https://t.co/dlRGYB @united Hi! what is the phone number for reservations in Venezuela? Thanks\n",
      "\n",
      "----\n",
      "2 (0.7) @JetBlue Hey any chance you have an update on Flight 99 Hartford to D.C.?\n",
      "1 (0.5) https://t.co/9oOPkN @JetBlue Hey any chance you have an update on Flight 99 Hartford to D.C.?\n",
      "1 (0.7) https://t.co/W0ommu @JetBlue Hey any chance you have an update on Flight 99 Hartford to D.C.?\n",
      "1 (0.9) https://t.co/c4OtLX @JetBlue Hey any chance you have an update on Flight 99 Hartford to D.C.?\n",
      "\n",
      "----\n",
      "2 (1.0) @united wont transfer flight ticket to accompany an 11 yr old who's active military mom had to have emergency brain surgery? WOW!!\n",
      "0 (0.9) @united wont transfer flight ticket to accompany an 11 yr old who's active military mom had to have emergency brain surgery? WOW!! https://t.co/Q51bIj\n",
      "1 (0.8) @united wont transfer flight ticket to accompany an 11 yr old who's active military mom had to have emergency brain surgery? WOW!! @N85lPW\n",
      "0 (0.8) @united wont transfer flight ticket to accompany an 11 yr old who's active military mom had to have emergency brain surgery? WOW!! https://t.co/wd5QqM\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, add_irrelevant, nsamples=500)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation, contractions, typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1176 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    31 (6.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) @JetBlue is that one on the picture http://t.co/lxwbsfxfj0\n",
      "2 (0.9) @JetBlue is that one on the picture\n",
      "2 (1.0) @JetBlue is that one on the picture.\n",
      "\n",
      "2 (0.8) @JetBlue with the free wifi #impressive #FlyFi http://t.co/T1RYpzEBc8\n",
      "1 (0.7) @JetBlue with the free wifi #impressive #FlyFi http://t.co/T1RYpzEBc8.\n",
      "\n",
      "0 (0.8) @united they had record of it being at Denver on the concourse prior to me gettin on the shuttle. I just want to confirm its location\n",
      "1 (0.7) @united they had record of it being at Denver on the concourse prior to me gettin on the shuttle. I just want to confirm its location.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.punctuation, nsamples=500)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    37 (7.4%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.7) @USAirways thanks for the seat that doesn't recline. I'm shocked I'm not being asked to serve everyone drinks on the plane.  #DoBetter\n",
      "1 (0.9) @USAirways thanks for the seat that doesn't recline. I'm shocked I'm not being asked to serve veeryone drinks on the plane.  #DoBetter\n",
      "\n",
      "2 (0.7) @AmericanAir on Feb. 15th your rep gave me the record locator and told me I'd be receiving an email with the itinerary and confirmation.\n",
      "0 (0.7) @AmericanAir on Feb. 15t hyour rep gave me the record locator and told me I'd be receiving an email with the itinerary and confirmation.\n",
      "\n",
      "0 (1.0) @AmericanAir this has to be the absolute WORST EXPERIENCE EVER!\n",
      "2 (1.0) @AmericanAir this has to be the absolute OWRST EXPERIENCE EVER!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=1)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    44 (8.8%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) .@USAirways I did but the more eyes I have looking for Pandu the better chance I have of bringing him home.\n",
      "0 (1.0) .@USAirways I did but the more eyes I have looking for Pandu hte better cahnce I have of bringing him home.\n",
      "\n",
      "1 (0.7) @USAirways - done :)\n",
      "0 (1.0) @USAirwasy -d one :)\n",
      "\n",
      "2 (1.0) @united Just sent! Thanks :)\n",
      "0 (1.0) @united Just snet! hTanks :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=2)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2076 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    26 (2.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) @JetBlue spoken to 2 reps. Once I'm allowed to check my bag and through the TSA checkpoint, I guarantee I will be talking to someone.\n",
      "1 (1.0) @JetBlue spoken to 2 reps. Once I'm allowed to check my bag and through the TSA checkpoint, I guarantee I'll be talking to someone.\n",
      "\n",
      "0 (0.8) @united I would appreciate a response regarding the pressurization failure on flight 1109. You seem to be responding to less serious issues\n",
      "1 (0.8) @united I'd appreciate a response regarding the pressurization failure on flight 1109. You seem to be responding to less serious issues\n",
      "\n",
      "1 (0.6) @AmericanAir what's the best number to use?\n",
      "0 (1.0) @AmericanAir what is the best number to use?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, Perturb.contractions, nsamples=1000)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 3641 examples\n",
      "Test cases:      331\n",
      "Fails (rate):    22 (6.6%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.6) @united Want to book a multi-city fare. Have miles for 1of 2 flights. An option to pay for 1st flight with miles +2nd with UMP Visa? Thanks!\n",
      "2 (0.8) @united Want to book a multi-city fare. Have John for 1of 2 flights. An option to pay for 1st flight with John +2nd with UMP Visa? Thanks!\n",
      "2 (0.9) @united Want to book a multi-city fare. Have Scott for 1of 2 flights. An option to pay for 1st flight with Scott +2nd with UMP Visa? Thanks!\n",
      "\n",
      "----\n",
      "1 (0.6) “@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Andrew. What can we do for you?” SMA\n",
      "0 (0.8) “@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Aiden. What can we do for you?” SMA\n",
      "0 (0.7) “@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Luis. What can we do for you?” SMA\n",
      "\n",
      "----\n",
      "1 (0.9) Too little, too Late Flight. You suck RT @USAirways: @mitchsunderland Oh no, Mitchell. Our agents are happy to offer you available options.\n",
      "0 (0.7) Too little, too Late Flight. You suck RT @USAirways: @mitchsunderland Oh no, Carlos. Our agents are happy to offer you available options.\n",
      "0 (0.8) Too little, too Late Flight. You suck RT @USAirways: @mitchsunderland Oh no, Jesus. Our agents are happy to offer you available options.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9999 examples\n",
      "Test cases:      909\n",
      "Fails (rate):    62 (6.8%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) @USAirways I didn't even leave the airport and you sent 2 of my bags to Philadelphia!\n",
      "0 (0.7) @USAirways I didn't even leave the airport and you sent 2 of my bags to St. Paul!\n",
      "1 (0.7) @USAirways I didn't even leave the airport and you sent 2 of my bags to Diamond Bar!\n",
      "\n",
      "2 (0.9) @SouthwestAir my friends from Boston stuck in Denver. Her name Jane. @RnCahill  Please contact her.\n",
      "1 (0.8) @SouthwestAir my friends from Boston stuck in Fresno. Her name Jane. @RnCahill  Please contact her.\n",
      "\n",
      "0 (0.7) @USAirways your delayed flight out of Wilmington made me miss my flight out of Charlotte. Figure out how to take off and arrive on time.\n",
      "1 (0.6) @USAirways your delayed flight out of Wilmington made me miss my flight out of Monterey Park. Figure out how to take off and arrive on time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_location, nsamples=1000)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 11000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    19 (1.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) @VirginAmerica Can I get some help with a support ticket? It's been 15 days.... Incident: 150202-000419 Thank you!\n",
      "1 (0.8) @VirginAmerica Can I get some help with a support ticket? It's been 15 days.... Incident: 150202-442 Thank you!\n",
      "\n",
      "2 (0.8) @united MIA-EWR #384 😄😄😄 excellent crew. EWR-IAD #3589 😡😡😡 No crew to load bags - waiting w/ door open freezing. 20 mins past departure.\n",
      "1 (0.7) @united MIA-EWR #318 😄😄😄 excellent crew. EWR-IAD #3589 😡😡😡 No crew to load bags - waiting w/ door open freezing. 20 mins past departure.\n",
      "\n",
      "1 (0.8) @USAirways it doesn't take 6 days to respond to an already open case!\n",
      "0 (0.7) @USAirways it doesn't take 5 days to respond to an already open case!\n",
      "0 (0.9) @USAirways it doesn't take 7 days to respond to an already open case!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_number, nsamples=1000)\n",
    "test = INV(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: temporal awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate', 'dislike', 'regret', 'abhor', 'dread', 'despise']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{neg_verb_present}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8000 examples\n",
      "Test cases:      8000\n",
      "Fails (rate):    1473 (18.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I think this airline is great, but I used to think it was awful.\n",
      "----\n",
      "0 (1.0) I welcome this airline, but I used to hate it.\n",
      "----\n",
      "0 (1.0) I think this airline is amazing, but I used to think it was ugly.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "change = ['but', 'even though', 'although', '']\n",
    "data = editor.template(['I used to think this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} I used to think it was {neg_adj}.',\n",
    "                                 'In the past I thought this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} in the past I thought it was {neg_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500)\n",
    "data += editor.template(['I used to {neg_verb_present} this airline, {change} now I {pos_verb_present} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} I used to {neg_verb_present} it.',\n",
    "                                 'In the past I would {neg_verb_present} this airline, {change} now I {pos_verb} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} in the past I would {neg_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500)\n",
    "labels = [2] * len(data)\n",
    "\n",
    "data += editor.template(['I used to think this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} I used to think it was {pos_adj}.',\n",
    "                                 'In the past I thought this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} in the past I thought it was {pos_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500)\n",
    "data += editor.template(['I used to {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} I used to {pos_verb_present} it.',\n",
    "                                 'In the past I would {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} in the past I would {pos_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500)\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = MFT(data, labels=labels)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used to should reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9048 examples\n",
      "Test cases:      4524\n",
      "After filtering: 15 (0.3%)\n",
      "Fails (rate):    8 (53.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) this is an average plane.\n",
      "0 (1.0) I used to think this is an average plane.\n",
      "\n",
      "----\n",
      "0 (0.7) this was an average aircraft.\n",
      "0 (1.0) I used to think this was an average aircraft.\n",
      "\n",
      "----\n",
      "0 (0.9) it is an average food.\n",
      "0 (1.0) I used to think it is an average food.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:adj} {air_noun}.', 'I used to think {it} {be} {a:adj} {air_noun}.'], it=['it', 'this', 'that'], be=['is', 'was'], adj=editor.lexicons['pos_adj'] + editor.lexicons['neg_adj'])\n",
    "data += editor.template(['{i} {verb} {the} {air_noun}.', '{i} used to {verb} {the} {air_noun}.'], i=['I', 'We'], the=['this', 'that', 'the'], verb=editor.lexicons['pos_verb_present'] + editor.lexicons['neg_verb_present'])\n",
    "test = DIR(data, monotonic_label_down)\n",
    "# test = MFT(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect:fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['male', 'female', 'first_name', 'first_pronoun', 'last_name', 'country', 'nationality', 'city', 'religion', 'religion_adj', 'sexual_adj', 'air_noun', 'pos_adj', 'neg_adj', 'neutral_adj', 'pos_verb_present', 'neg_verb_present', 'neutral_verb_present', 'pos_verb_past', 'neg_verb_past', 'neutral_verb_past', 'pos_verb', 'neg_verb', 'neutral_verb'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a Christian',\n",
       "  'a Protestant',\n",
       "  'a Roman Catholic',\n",
       "  'an Eastern Orthodox',\n",
       "  'an Anglican',\n",
       "  'a Jew',\n",
       "  'an Orthodox Jew',\n",
       "  'a Muslim',\n",
       "  'a Sunni',\n",
       "  \"a Shi'a\",\n",
       "  'an Ahmadiyya',\n",
       "  'a Buddhist',\n",
       "  'a Zoroastrian',\n",
       "  'a Hindu',\n",
       "  'a Sikh',\n",
       "  'a Shinto',\n",
       "  \"a Baha'i\",\n",
       "  'a Taoist',\n",
       "  'a Confucian',\n",
       "  'a Jain',\n",
       "  'an Atheist',\n",
       "  'an Agnostic'],)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{a:religion_adj}'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = {\n",
    "    'race': ['a black','a hispanic', 'a white', 'an asian'],\n",
    "    'sexual': editor.template('{a:sexual_adj}'),\n",
    "    'religion': editor.template('{a:religion_adj}'),\n",
    "    'nationality': editor.template('{a:nationality}')[:20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "Predicting 800 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    183 (91.5%)\n",
      "\n",
      "Example fails:\n",
      "1 (1.0) Vanessa is a black immigrant.\n",
      "2 (1.0) Vanessa is a hispanic immigrant.\n",
      "2 (1.0) Vanessa is an asian immigrant.\n",
      "\n",
      "----\n",
      "0 (1.0) Timothy is a black character.\n",
      "2 (1.0) Timothy is a hispanic character.\n",
      "2 (0.9) Timothy is an asian character.\n",
      "\n",
      "----\n",
      "0 (0.7) Christopher is a black boy.\n",
      "1 (0.9) Christopher is a white boy.\n",
      "2 (1.0) Christopher is a hispanic boy.\n",
      "\n",
      "----\n",
      "\n",
      "0.30 a black\n",
      "1.95 a hispanic\n",
      "0.45 a white\n",
      "2.00 an asian\n",
      "\n",
      "\n",
      "-------------------------\n",
      "sexual\n",
      "Predicting 2800 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    199 (99.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Isabella is an asexual man.\n",
      "0 (1.0) Isabella is a queer man.\n",
      "0 (1.0) Isabella is a trans man.\n",
      "\n",
      "----\n",
      "2 (1.0) Lucas is an asexual male.\n",
      "1 (0.8) Lucas is a heterosexual male.\n",
      "1 (0.6) Lucas is a transsexual male.\n",
      "\n",
      "----\n",
      "2 (1.0) Sara is an asexual girl.\n",
      "1 (0.9) Sara is a trans girl.\n",
      "0 (0.9) Sara is a transgender girl.\n",
      "\n",
      "----\n",
      "\n",
      "1.77 an asexual\n",
      "1.46 a bisexual\n",
      "0.61 a heterosexual\n",
      "0.01 a homosexual\n",
      "1.75 a pansexual\n",
      "1.07 a queer\n",
      "1.54 a transsexual\n",
      "0.32 a trans\n",
      "0.03 a gay\n",
      "1.77 a straight\n",
      "0.19 a transgender\n",
      "0.21 a lesbian\n",
      "1.92 a non-binary\n",
      "1.69 a cisgender\n",
      "\n",
      "\n",
      "-------------------------\n",
      "religion\n",
      "Predicting 4400 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    199 (99.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) Jonathan is a Christian Jew.\n",
      "0 (1.0) Jonathan is an Atheist Jew.\n",
      "0 (0.9) Jonathan is an Agnostic Jew.\n",
      "\n",
      "----\n",
      "2 (1.0) Kenneth is a Christian woman.\n",
      "0 (1.0) Kenneth is an Atheist woman.\n",
      "0 (1.0) Kenneth is an Agnostic woman.\n",
      "\n",
      "----\n",
      "2 (0.9) Megan is a Christian Muslim.\n",
      "1 (0.5) Megan is a Jew Muslim.\n",
      "0 (0.9) Megan is an Agnostic Muslim.\n",
      "\n",
      "----\n",
      "\n",
      "1.97 a Christian\n",
      "1.95 a Protestant\n",
      "1.89 a Roman Catholic\n",
      "2.00 an Eastern Orthodox\n",
      "1.95 an Anglican\n",
      "1.44 a Jew\n",
      "1.98 an Orthodox Jew\n",
      "1.69 a Muslim\n",
      "1.93 a Sunni\n",
      "1.96 a Shi'a\n",
      "1.96 an Ahmadiyya\n",
      "1.81 a Buddhist\n",
      "2.00 a Zoroastrian\n",
      "1.81 a Hindu\n",
      "1.97 a Sikh\n",
      "1.89 a Shinto\n",
      "1.99 a Baha'i\n",
      "2.00 a Taoist\n",
      "1.84 a Confucian\n",
      "2.00 a Jain\n",
      "0.01 an Atheist\n",
      "0.17 an Agnostic\n",
      "\n",
      "\n",
      "-------------------------\n",
      "nationality\n",
      "Predicting 4000 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    6 (3.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.7) Mason is a Chinese Muslim.\n",
      "1 (1.0) Mason is a Russian Muslim.\n",
      "1 (0.9) Mason is a Mexican Muslim.\n",
      "\n",
      "----\n",
      "2 (0.9) Christopher is a Chinese Muslim.\n",
      "1 (0.9) Christopher is a German Muslim.\n",
      "\n",
      "----\n",
      "2 (0.7) Kenneth is a Chinese Muslim.\n",
      "0 (0.8) Kenneth is a German Muslim.\n",
      "1 (0.7) Kenneth is a Vietnamese Muslim.\n",
      "\n",
      "----\n",
      "\n",
      "2.00 a Chinese\n",
      "2.00 an Indian\n",
      "2.00 an American\n",
      "2.00 an Indonesian\n",
      "1.98 a Pakistani\n",
      "2.00 a Brazilian\n",
      "2.00 a Nigerian\n",
      "2.00 a Bangladeshi\n",
      "1.98 a Russian\n",
      "1.99 a Japanese\n",
      "1.98 a Mexican\n",
      "2.00 an Ethiopian\n",
      "2.00 a Philippine\n",
      "2.00 an Egyptian\n",
      "1.99 a Vietnamese\n",
      "1.96 a German\n",
      "2.00 a Turkish\n",
      "2.00 an Iranian\n",
      "2.00 a Thai\n",
      "1.99 a French\n",
      "\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for p, vals in protected.items():\n",
    "    print(p)\n",
    "    d = editor.template(['{male} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=100)\n",
    "    d += editor.template(['{female} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=100)\n",
    "    test = INV(d, threshold=0.1)\n",
    "    test.run(new_pp)\n",
    "    test.summary(n=3)\n",
    "    print()\n",
    "    preds = np.array(test.results.preds)\n",
    "    for i, x in enumerate(vals):\n",
    "        print('%.2f %s' % (preds[:, i].mean(), vals[i]))\n",
    "    print()\n",
    "    print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 6318 examples\n",
      "Test cases:      6318\n",
      "Fails (rate):    580 (9.2%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I would never say I love that airline.\n",
      "2 (1.0) I can't say I appreciate that customer service.\n",
      "2 (1.0) I would never say I enjoy this crew.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {pos_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "data += editor.template('No one {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "test = MFT(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7254 examples\n",
      "Test cases:      7254\n",
      "Fails (rate):    1255 (17.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) This isn't an aggressive pilot.\n",
      "0 (1.0) No one dislikes that pilot.\n",
      "0 (1.0) I would never say I despise this company.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {neg_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {neg_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "data += editor.template('No one {neg_verb_present}s {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "is_not_0 = lambda x, pred, *args: pred != 0\n",
    "test = MFT(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2496 examples\n",
      "Test cases:      2496\n",
      "Fails (rate):    2466 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) It isn't an Indian service.\n",
      "0 (1.0) That was not an American service.\n",
      "0 (1.0) That was not an international flight.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {neutral_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {neutral_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "test = MFT(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2106 examples\n",
      "Test cases:      2106\n",
      "Fails (rate):    32 (1.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) I thought I would love this aircraft, but I did not.\n",
      "2 (0.8) I thought I would admire that service, but I did not.\n",
      "1 (0.8) I thought I would admire that seat, but I did not.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {pos_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {pos_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "test = MFT(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2418 examples\n",
      "Test cases:      2418\n",
      "Fails (rate):    2082 (86.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) I thought that pilot would be sad, but it was not.\n",
      "0 (1.0) I thought that customer service would be tough, but it was not.\n",
      "0 (1.0) I thought this food would be poor, but it wasn't.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {neg_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {neg_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 858 examples\n",
      "Test cases:      858\n",
      "Fails (rate):    844 (98.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I thought the service would be Australian, but it wasn't.\n",
      "0 (1.0) I thought that flight would be commercial, but it wasn't.\n",
      "0 (1.0) I thought the seat would be commercial, but it was not.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {neutral_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {neutral_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harder: negation with neutral in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    731 (73.1%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I can't say, given all that I've seen over the years, that that plane is fun.\n",
      "2 (1.0) I don't think I, given the time that I've been flying, that we admire that company.\n",
      "2 (1.0) I can't say, given that I am from Brazil, that that staff is beautiful.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {pos_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:pos_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {pos_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = MFT(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    994 (99.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I can't say, given the time that I've been flying, that that was an awful aircraft.\n",
      "0 (1.0) I can't say, given all that I've seen over the years, that this is a weird pilot.\n",
      "0 (1.0) I can't say, given the time that I've been flying, that that crew is horrible.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neg_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neg_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {neg_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = MFT(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    976 (97.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I can't say I, given my history with airplanes, that we see the service.\n",
      "0 (1.0) I wouldn't say, given all that I've seen over the years, that this customer service was Israeli.\n",
      "0 (1.0) I don't think, given my history with airplanes, that the is a private customer service.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neutral_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neutral_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {neutral_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = MFT(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: SRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my opinion is more important than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9136 examples\n",
      "Test cases:      9136\n",
      "Fails (rate):    3374 (36.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I think you are rough, but some people think you are adorable.\n",
      "0 (0.8) I think you are good, but I had heard you were nasty.\n",
      "0 (0.9) I think you are exceptional, but I had heard you were sad.\n"
     ]
    }
   ],
   "source": [
    "change = [' but', '']\n",
    "templates = ['Some people think you are {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} some people think you are {neg_adj}.',\n",
    "             'I had heard you were {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} I had heard you were {neg_adj}.',\n",
    "             ]\n",
    "data = editor.template(templates, change=change, unroll=True)\n",
    "templates = ['{others} {neg_verb_present} you,{change} I {pos_verb_present} you.',\n",
    "             'I {pos_verb_present} you,{change} {others} {neg_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "labels = [2] * len(data)\n",
    "\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} some people think you are {pos_adj}.',\n",
    "             'I had heard you were {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} I had heard you were {pos_adj}.',\n",
    "             ]\n",
    "data += editor.template(templates, change=change, unroll=True)\n",
    "templates = ['{others} {pos_verb_present} you,{change} I {neg_verb_present} you.',\n",
    "             'I {neg_verb_present} you,{change} {others} {pos_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = MFT(data, labels=labels)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q & a form: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7956 examples\n",
      "Test cases:      7956\n",
      "Fails (rate):    226 (2.8%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) Did we dislike this seat? Yes\n",
      "0 (0.9) Do I think this airline was nice? Yes\n",
      "2 (0.7) Did we dislike that seat? Yes\n"
     ]
    }
   ],
   "source": [
    "temp = editor.template('Do I think {it} {air_noun} {be} {pos_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp += editor.template('Did {i} {pos_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = [x + ' Yes' for x in temp]\n",
    "labels = [2] * len(data)\n",
    "temp2 = editor.template('Do I think {it} {air_noun} {be} {neg_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp2 += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp2 += editor.template('Did {i} {neg_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data += [x + ' Yes' for x in temp2]\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "\n",
    "test = MFT(data, labels=labels)\n",
    "# test = MFT(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1560 examples\n",
      "Test cases:      1560\n",
      "Fails (rate):    1541 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think it is an Israeli crew? Yes\n",
      "0 (1.0) Do I think this was an Israeli aircraft? Yes\n",
      "0 (1.0) Do I think that food is Italian? Yes\n"
     ]
    }
   ],
   "source": [
    "temp3 = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp3 += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp3 += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = [x + ' Yes' for x in temp3]\n",
    "test = MFT(data, labels=1)\n",
    "# test = MFT(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7956 examples\n",
      "Test cases:      7956\n",
      "Fails (rate):    4371 (54.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that was a terrible crew? No\n",
      "0 (1.0) Do I think that is a bad customer service? No\n",
      "0 (1.0) Did we hate this pilot? No\n"
     ]
    }
   ],
   "source": [
    "data = [x + ' No' for x in temp]\n",
    "labels = [0] * len(data)\n",
    "data += [x + ' No' for x in temp2]\n",
    "labels += [1] * (len(data) - len(labels))\n",
    "\n",
    "allow_for_neutral = lambda x, pred, _, label, _2 : pred != 0 if label == 1 else pred == label\n",
    "\n",
    "test = MFT(data, Expect.single(allow_for_neutral), labels=labels)\n",
    "# test = MFT(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1560 examples\n",
      "Test cases:      1560\n",
      "Fails (rate):    1560 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that staff was British? No\n",
      "0 (1.0) Do I think the customer service was Italian? No\n",
      "0 (1.0) Do I think that is a private customer service? No\n"
     ]
    }
   ],
   "source": [
    "data = [x + ' No' for x in temp3]\n",
    "test = MFT(data, labels=1)\n",
    "# test = MFT(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['good', 'bad', 'great']\n",
    "def expect(x):\n",
    "    return x in pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       "  'locals()'],\n",
       " '_oh': {},\n",
       " '_dh': ['/home/wtshuang/sourcetree/checklist_ui/notebooks'],\n",
       " 'In': ['',\n",
       "  \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       "  'locals()'],\n",
       " 'Out': {},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f21403e6a10>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f214207abd0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f214207abd0>,\n",
       " '_': '',\n",
       " '__': '',\n",
       " '___': '',\n",
       " '_i': \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       " '_ii': '',\n",
       " '_iii': '',\n",
       " '_i1': \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       " 'pos': ['good', 'bad', 'great'],\n",
       " 'expect': <function __main__.expect(x)>,\n",
       " '_i2': 'locals()'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       "  'locals()',\n",
       "  'locals()'],\n",
       " '_oh': {2: {...}},\n",
       " '_dh': ['/home/wtshuang/sourcetree/checklist_ui/notebooks'],\n",
       " 'In': ['',\n",
       "  \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       "  'locals()',\n",
       "  'locals()'],\n",
       " 'Out': {2: {...}},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f21403e6a10>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f214207abd0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f214207abd0>,\n",
       " '_': {...},\n",
       " '__': '',\n",
       " '___': '',\n",
       " '_i': 'locals()',\n",
       " '_ii': \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       " '_iii': '',\n",
       " '_i1': \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       " 'pos': ['good', 'bad', 'great'],\n",
       " 'expect': <function __main__.expect(x)>,\n",
       " '_i2': 'locals()',\n",
       " '_2': {...},\n",
       " '_i3': 'locals()'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       "  'locals()',\n",
       "  'locals()',\n",
       "  'globals()'],\n",
       " '_oh': {2: {...}, 3: {...}},\n",
       " '_dh': ['/home/wtshuang/sourcetree/checklist_ui/notebooks'],\n",
       " 'In': ['',\n",
       "  \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       "  'locals()',\n",
       "  'locals()',\n",
       "  'globals()'],\n",
       " 'Out': {2: {...}, 3: {...}},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f21403e6a10>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f214207abd0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f214207abd0>,\n",
       " '_': {...},\n",
       " '__': {...},\n",
       " '___': '',\n",
       " '_i': 'locals()',\n",
       " '_ii': 'locals()',\n",
       " '_iii': \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       " '_i1': \"pos = ['good', 'bad', 'great']\\ndef expect(x):\\n    return x in pos\",\n",
       " 'pos': ['good', 'bad', 'great'],\n",
       " 'expect': <function __main__.expect(x)>,\n",
       " '_i2': 'locals()',\n",
       " '_2': {...},\n",
       " '_i3': 'locals()',\n",
       " '_3': {...},\n",
       " '_i4': 'globals()'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
