{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import MFT\n",
    "from checklist.inv_dir import INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<checklist.text_generation.TextGenerator at 0x7f4464108588>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "editor.tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template((('{first_name} is a good guy', 'Who is a good guy?'), '{first_name}', '3') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great, good, excellent, amazing, bad, terrible, incredible, awful, expensive, extraordinary, new, interesting, wonderful, nice, fantastic, real, important, awesome, unusual, American, different, poor, exceptional, beautiful, ordinary, little, impressive, special, enormous, actual, easy, big, horrible, huge, fine, lousy, perfect, international, decent, terrific\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('It was {a:bert} {air_noun}.')[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'tough', 'weird', 'aggressive', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liked, enjoyed, like, appreciate, appreciated, enjoy, loved, love, miss, missed, hate, needed, likes, wanted, got, recommend, prefer, admired, value, need, respected, want, dislike, enjoying, respect, enjoys, admire, was, feel, liking, dig, hated, mean, preferred, underestimated, disliked, used, get, use, valued, understand, did, found, adore, helped, dug, trust, remember, noticed, about, tried, hit, had, regret, took, felt, praised, cherish, have, left, loves, understood, do, LOVE, compliment, trusted, bought, thank, treasure, applaud, support, rate, know, commend, credit, underestimate, see, supported, think, thought, all, saw, picked, believe, beat, welcome, considered, impressed, improved, met, chose, thanks, deserved, envy, blame, for, help, packed, recommended, in, follow, choose, loving, misses, is, into, lost, consider, changed, worked, meant, leave, joined, wish, regretted, made, earned, rocked, cherished, take, worth, finished, experienced, are, fancy, handled, thanked, followed, saved, spoiled, told, surprised, salute, meet, pleased, try, recruited, crave, owe, reviewed, doubt, values, forgive, hired, hope, remembered, stressed, praise, respects, just, LIKE, knew, am, welcomed, tested, sold, Love, Like, survived, ate, planned, drove, ordered, sampled, crashed, managed, brought, ended, deserve, rode, eat, applauded, regrets, spent, heard, grabbed, needs, tasted, coveted, ruined, timed, delayed, caught, dreaded, watched, forgot, anticipated, canceled, own, rushed, cooked, taste, delivered, prefers, filled, deserves, shortened, mind, received, edited\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('I really {bert} the {air_noun}.')[:200]))\n",
    "# print()\n",
    "# print(', '.join(editor.suggest('I {bert} the {air_noun}.')[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(pos_adj + pos_verb_present + pos_verb_past, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual positive words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(neg_adj + neg_verb_present + neg_verb_past, labels=0)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual negative words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(neutral_adj + neutral_verb_present + neutral_verb_past, labels=1)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'individual neutral words', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=2, save=True)\n",
    "t += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=0, save=True)\n",
    "# equivalent to:\n",
    "# test = MFT(t.data, labels=t.labels, templates=t.templates)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'sentiment words in context', 'Vocabulary', 'TODO_DESCRIPTION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {be} {neutral_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{it} {be} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{i} {neutral_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "suite.add(test, 'neutral words in context', 'Vocabulary', 'TODO_DESCRIPTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very , really , extremely , quite , pretty , absolutely , incredibly , truly , actually , a , rather , always , most , exceptionally , genuinely , surprisingly , particularly , damn , especially , amazingly , generally , extraordinarily , unbelievably , utterly , overall , equally , darn , super , exceedingly , unusually , obviously , entirely , otherwise , enormously , overwhelmingly , immensely , just , totally , insanely , absolute , altogether , undeniably , unexpectedly , completely , real , VERY , almost , all , increasingly , already\n"
     ]
    }
   ],
   "source": [
    "print(' , '.join(editor.suggest('{it} {be} {a:bert} {pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really, always, certainly, also, truly, greatly, all, definitely, highly, personally, we, thoroughly, especially, so, I, both, much, absolutely, particularly, sincerely, very, still, just, sure, genuinely, fully, people, most, strongly, you, deeply, obviously, clearly, do, have, and, never, quite, did, actually, would, totally, they, rather, seriously, dearly, again, completely, will, simply, honestly, should, too, now, extremely, guys, immediately, must, even, already, who, can, had, immensely, tremendously, generally, surely, profoundly, everyone, vastly, might, friends, does, further, may, could, to, many, REALLY, family, only, hugely, has, ..., incredibly, well, long, then, probably, ultimately, often, kindly, are, positively, therefore, indeed, sorely, that, actively, usually\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('{i} {bert} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "t += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500, save=True)\n",
    "t += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500, save=True)\n",
    "test = DIR(t.data, monotonic_label, templates=t.templates)\n",
    "suite.add(test, 'intensifiers', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'a little', 'a bit', 'slightly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {air_noun} {be} {pos_adj}.', '{it} {air_noun} {be} {red} {pos_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000, save=True)\n",
    "t += editor.template(['{it} {air_noun} {be} {neg_adj}.', '{it} {air_noun} {be} {red} {neg_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000, save=True)\n",
    "test = DIR(t.data, monotonic_label_down, templates=t.templates)\n",
    "suite.add(test, 'reducers', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVariance: change neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = set(\n",
    "    ['.', 'the', 'The', ',', 'a', 'A', 'and', 'of', 'to', 'it', 'that', 'in',\n",
    "     'this', 'for',  'you', 'there', 'or', 'an', 'by', 'about', 'flight', 'my',\n",
    "     'in', 'of', 'have', 'with', 'was', 'at', 'it', 'get', 'from', 'this', 'Flight', 'plane'\n",
    "    ])\n",
    "forbidden = set(['No', 'no', 'Not', 'not', 'Nothing', 'nothing', 'without'] + pos_adj + neg_adj + pos_verb_present + pos_verb_past + neg_verb_present + neg_verb_past)\n",
    "def change_neutral(d):\n",
    "#     return d.text\n",
    "    examples = []\n",
    "    subs = []\n",
    "    words_in = [x for x in d.capitalize().split() if x in neutral_words]\n",
    "    if not words_in:\n",
    "        return None\n",
    "    for w in words_in:\n",
    "        suggestions = [x for x in editor.suggest_replace(d, w, beam_size=5, words_and_sentences=True) if x[0] not in forbidden]\n",
    "        examples.extend([x[1] for x in suggestions])\n",
    "        subs.extend(['%s -> %s' % (w, x[0]) for x in suggestions])\n",
    "    if examples:\n",
    "        idxs = np.random.choice(len(examples), min(len(examples), 10), replace=False)\n",
    "        return [examples[i] for i in idxs]#, [subs[i] for i in idxs])\n",
    "# Perturb.perturb(parsed_data[:5], perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, change_neutral, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change neutral words with BERT', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = editor.template('I {pos_verb_present} you.').data\n",
    "positive += editor.template('You are {pos_adj}.').data\n",
    "positive += ['I would fly with you again.']\n",
    "positive.remove('You are happy.')\n",
    "negative = editor.template('I {neg_verb_present} you.').data\n",
    "negative += editor.template('You are {neg_adj}.').data\n",
    "negative += ['Never flying with you again.']\n",
    "def add_phrase_function(phrases):\n",
    "    def pert(d):\n",
    "        while d[-1].pos_ == 'PUNCT':\n",
    "            d = d[:-1]\n",
    "        d = d.text\n",
    "        ret = [d + '. ' + x for x in phrases]\n",
    "        idx = np.random.choice(len(ret), 10, replace=False)\n",
    "        ret = [ret[i] for i in idx]\n",
    "        return ret\n",
    "    return pert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_1 = Expect.monotonic(label=2, increasing=True, tolerance=0.1)\n",
    "monotonic_1_down = Expect.monotonic(label=2, increasing=False, tolerance=0.1)\n",
    "monotonic_0 = Expect.monotonic(label=0, increasing=True, tolerance=0.1)\n",
    "monotonic_0_down = Expect.monotonic(label=0, increasing=False, tolerance=0.1)\n",
    "goes_up = Expect.combine_and(monotonic_1, monotonic_0_down)\n",
    "goes_down = Expect.combine_and(monotonic_1_down, monotonic_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check this expectation function\n",
    "t = Perturb.perturb(parsed_data, add_phrase_function(positive), nsamples=500)\n",
    "# test = DIR(data, monotonic_1)\n",
    "test = DIR(t.data, goes_up)\n",
    "suite.add(test, 'add positive phrases', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp, overwrite=True)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check this expectation function\n",
    "t = Perturb.perturb(parsed_data, add_phrase_function(negative), nsamples=500)\n",
    "# test = DIR(data, monotonic_1_down)\n",
    "test = DIR(t.data, goes_down)\n",
    "suite.add(test, 'add negative phrases', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp, overwrite=True)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: robustness\n",
    "### INVariance: adding irrelevant stuff before and after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def random_string(n):\n",
    "    return ''.join(np.random.choice([x for x in string.ascii_letters + string.digits], n))\n",
    "def random_url(n=6):\n",
    "    return 'https://t.co/%s' % random_string(n)\n",
    "def random_handle(n=6):\n",
    "    return '@%s' % random_string(n)\n",
    "\n",
    "# data['sentence']\n",
    "\n",
    "def add_irrelevant(sentence):\n",
    "    urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]\n",
    "    irrelevant_before = ['@airline '] + urls_and_handles\n",
    "    irrelevant_after = urls_and_handles \n",
    "    rets = ['%s %s' % (x, sentence) for x in irrelevant_before ]\n",
    "    rets += ['%s %s' % (sentence, x) for x in irrelevant_after]\n",
    "    return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, add_irrelevant, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'add urls and handles', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation, contractions, typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.punctuation, nsamples=500)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'punctuation', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=1)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'typos', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=2)\n",
    "test = INV(t.data)\n",
    "suite.add(test, '2 typos', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(sentences, Perturb.contractions, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'contractions', 'Robustness', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change names', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_location, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change locations', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_data, Perturb.change_number, nsamples=1000)\n",
    "test = INV(t.data)\n",
    "suite.add(test, 'change numbers', 'NER', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: temporal awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate', 'dislike', 'regret', 'abhor', 'dread', 'despise']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{neg_verb_present}').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = ['but', 'even though', 'although', '']\n",
    "t = editor.template(['I used to think this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} I used to think it was {neg_adj}.',\n",
    "                                 'In the past I thought this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} in the past I thought it was {neg_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500, save=True, labels=2)\n",
    "t += editor.template(['I used to {neg_verb_present} this airline, {change} now I {pos_verb_present} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} I used to {neg_verb_present} it.',\n",
    "                                 'In the past I would {neg_verb_present} this airline, {change} now I {pos_verb} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} in the past I would {neg_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500, save=True, labels=2)\n",
    "\n",
    "t += editor.template(['I used to think this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} I used to think it was {pos_adj}.',\n",
    "                                 'In the past I thought this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} in the past I thought it was {pos_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500, save=True, labels=0)\n",
    "t += editor.template(['I used to {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} I used to {pos_verb_present} it.',\n",
    "                                 'In the past I would {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} in the past I would {pos_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500, save=True, labels=0)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'used to, but now', 'Temporal', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used to should reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(['{it} {be} {a:adj} {air_noun}.', 'I used to think {it} {be} {a:adj} {air_noun}.'], it=['it', 'this', 'that'], be=['is', 'was'], adj=editor.lexicons['pos_adj'] + editor.lexicons['neg_adj'], save=True)\n",
    "t += editor.template(['{i} {verb} {the} {air_noun}.', '{i} used to {verb} {the} {air_noun}.'], i=['I', 'We'], the=['this', 'that', 'the'], verb=editor.lexicons['pos_verb_present'] + editor.lexicons['neg_verb_present'], save=True)\n",
    "test = DIR(t.data, monotonic_label_down, templates=t.templates)\n",
    "# test = MFT(data, labels=2)\n",
    "suite.add(test, '\"used to\" should reduce', 'Temporal', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect:fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['male', 'female', 'first_name', 'first_pronoun', 'last_name', 'country', 'nationality', 'city', 'religion', 'religion_adj', 'sexual_adj', 'air_noun', 'pos_adj', 'neg_adj', 'neutral_adj', 'pos_verb_present', 'neg_verb_present', 'neutral_verb_present', 'pos_verb_past', 'neg_verb_past', 'neutral_verb_past', 'pos_verb', 'neg_verb', 'neutral_verb'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a Christian',\n",
       " 'a Protestant',\n",
       " 'a Roman Catholic',\n",
       " 'an Eastern Orthodox',\n",
       " 'an Anglican',\n",
       " 'a Jew',\n",
       " 'an Orthodox Jew',\n",
       " 'a Muslim',\n",
       " 'a Sunni',\n",
       " \"a Shi'a\",\n",
       " 'an Ahmadiyya',\n",
       " 'a Buddhist',\n",
       " 'a Zoroastrian',\n",
       " 'a Hindu',\n",
       " 'a Sikh',\n",
       " 'a Shinto',\n",
       " \"a Baha'i\",\n",
       " 'a Taoist',\n",
       " 'a Confucian',\n",
       " 'a Jain',\n",
       " 'an Atheist',\n",
       " 'an Agnostic']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{a:religion_adj}').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = {\n",
    "    'race': ['a black','a hispanic', 'a white', 'an asian'],\n",
    "    'sexual': editor.template('{a:sexual_adj}').data,\n",
    "    'religion': editor.template('{a:religion_adj}').data,\n",
    "    'nationality': editor.template('{a:nationality}').data[:20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "sexual\n",
      "religion\n",
      "nationality\n"
     ]
    }
   ],
   "source": [
    "for p, vals in protected.items():\n",
    "    print(p)\n",
    "    t = editor.template(['{male} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=300, save=True)\n",
    "    t += editor.template(['{female} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=300, save=True)\n",
    "    test = INV(t.data, threshold=0.1, templates=t.templates)\n",
    "    suite.add(test, 'protected: %s' % p, 'Fairness', 'TODO_DESCRIPTION')\n",
    "#     test.run(new_pp)\n",
    "#     test.summary(n=3)\n",
    "#     print()\n",
    "#     preds = np.array(test.results.preds)\n",
    "#     for i, x in enumerate(vals):\n",
    "#         print('%.2f %s' % (preds[:, i].mean(), vals[i]))\n",
    "#     print()\n",
    "#     print()\n",
    "#     print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {pos_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "t += editor.template('No one {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'simple negations: negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {neg_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {neg_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "t += editor.template('No one {neg_verb_present}s {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "is_not_0 = lambda x, pred, *args: pred != 0\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'simple negations: not negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('{it} {air_noun} {nt} {neutral_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('{it} {benot} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'], save=True)\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "t += editor.template('{neg} {neutral_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'simple negations: not neutral is still neutral', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {pos_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {pos_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'simple negations: but I did not (negative)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {neg_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {neg_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'simple negations: but I did not (not negative)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('I thought {it} {air_noun} would be {neutral_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'], save=True)\n",
    "t += editor.template('I thought I would {neutral_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'], save=True)\n",
    "# expectation: prediction is not 0\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'simple negations: but it was not (neutral)', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harder: negation with neutral in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neg = neg[:-1]\n",
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {pos_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:pos_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {pos_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, labels=0, templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neg_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:neg_adj} {air_noun}.',neutral=neutral,  neg=['i don\\'t think', 'i can\\'t say', 'i wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {neg_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['i don\\'t think', 'i can\\'t say', 'i wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, Expect.single(is_not_0), templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle, not negative', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "t = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neutral_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {it} {be} {a:neutral_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{neg}, given {neutral}, that {i} {neutral_verb_present} {the} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "t.data = list(np.random.choice(t.data, 1000, replace=False))\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'negation with neutral in the middle, neutral', 'Negation', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: SRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my opinion is more important than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = [' but', '']\n",
    "templates = ['Some people think you are {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} some people think you are {neg_adj}.',\n",
    "             'I had heard you were {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} I had heard you were {neg_adj}.',\n",
    "             ]\n",
    "t = editor.template(templates, change=change, unroll=True, labels=2, save=True)\n",
    "templates = ['{others} {neg_verb_present} you,{change} I {pos_verb_present} you.',\n",
    "             'I {pos_verb_present} you,{change} {others} {neg_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "t += editor.template(templates, others=others, change=change, unroll=True, labels=2, save=True)\n",
    "\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} some people think you are {pos_adj}.',\n",
    "             'I had heard you were {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} I had heard you were {pos_adj}.',\n",
    "             ]\n",
    "t += editor.template(templates, change=change, unroll=True, labels=0, save=True)\n",
    "templates = ['{others} {pos_verb_present} you,{change} I {neg_verb_present} you.',\n",
    "             'I {neg_verb_present} you,{change} {others} {pos_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "t += editor.template(templates, others=others, change=change, unroll=True, labels=0, save=True)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'my opinion is what matters', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q & a form: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=2)\n",
    "t += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=2)\n",
    "t += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=2)\n",
    "t += editor.template('Do I think {it} {air_noun} {be} {neg_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=0)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'Q & A: yes', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'Q & A: yes (neutral)', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=0)\n",
    "t += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=0)\n",
    "t += editor.template('Do I think {it} {air_noun} {be} {neg_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True, labels=1)\n",
    "t += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True, labels=1)\n",
    "t += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True, labels=1)\n",
    "allow_for_neutral = lambda x, pred, _, label, _2 : pred != 0 if label == 1 else pred == label\n",
    "test = MFT(t.data, Expect.single(allow_for_neutral), labels=t.labels, templates=t.templates)\n",
    "suite.add(test, 'Q & A: no', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'], save=True)\n",
    "t += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'Q & A: no (neutral)', 'SRL', 'TODO_DESCRIPTION')\n",
    "# test = MFT(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.save('/home/marcotcr/tmp/sentiment_suite.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running individual positive words\n",
      "Predicting 34 examples\n",
      "Running individual negative words\n",
      "Predicting 37 examples\n",
      "Running individual neutral words\n",
      "Predicting 13 examples\n",
      "Running sentiment words in context\n",
      "Predicting 8970 examples\n",
      "Running neutral words in context\n",
      "Predicting 1716 examples\n",
      "Running intensifiers\n",
      "Predicting 4000 examples\n",
      "Running reducers\n",
      "Predicting 4000 examples\n",
      "Running change neutral words with BERT\n",
      "Predicting 4917 examples\n",
      "Running add positive phrases\n",
      "Predicting 5500 examples\n",
      "Running add negative phrases\n",
      "Predicting 5500 examples\n",
      "Running add urls and handles\n",
      "Predicting 11000 examples\n",
      "Running punctuation\n",
      "Predicting 1159 examples\n",
      "Running typos\n",
      "Predicting 1000 examples\n",
      "Running 2 typos\n",
      "Predicting 1000 examples\n",
      "Running contractions\n",
      "Predicting 2079 examples\n",
      "Running change names\n",
      "Predicting 3641 examples\n",
      "Running change locations\n",
      "Predicting 9999 examples\n",
      "Running change numbers\n",
      "Predicting 11000 examples\n"
     ]
    }
   ],
   "source": [
    "suite.run(new_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "individual negative words\n",
      "Test cases:      37\n",
      "Fails (rate):    2 (5.4%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) aggressive\n",
      "----\n",
      "2 (0.8) tough\n",
      "----\n",
      "\n",
      "\n",
      "individual neutral words\n",
      "Test cases:      13\n",
      "Fails (rate):    13 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) British\n",
      "----\n",
      "2 (1.0) Indian\n",
      "----\n",
      "2 (1.0) international\n",
      "----\n",
      "\n",
      "\n",
      "individual positive words\n",
      "Test cases:      34\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "neutral words in context\n",
      "Test cases:      1716\n",
      "Fails (rate):    1632 (95.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) It was an Israeli plane.\n",
      "----\n",
      "2 (1.0) It was a British pilot.\n",
      "----\n",
      "2 (1.0) That was an American aircraft.\n",
      "----\n",
      "\n",
      "\n",
      "sentiment words in context\n",
      "Test cases:      8970\n",
      "Fails (rate):    173 (1.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) This was an aggressive flight.\n",
      "----\n",
      "2 (1.0) This was an aggressive staff.\n",
      "----\n",
      "2 (1.0) The plane was aggressive.\n",
      "----\n",
      "\n",
      "\n",
      "change neutral words with BERT\n",
      "Test cases:      500\n",
      "Fails (rate):    53 (10.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) @JetBlue oh right! Me and hundred people arrived back to the same gate we left minutes ago in SDQ http://t.co/7gB0hgW51t\n",
      "1 (0.8) @JetBlue oh right! Me 6 hundred people arrived back to the same gate we left minutes ago in SDQ http://t.co/7gB0hgW51t\n",
      "1 (0.6) @JetBlue oh right! Me one hundred people arrived back to the same gate we left minutes ago in SDQ http://t.co/7gB0hgW51t\n",
      "\n",
      "----\n",
      "2 (0.7) @united no, I have a pricey Chase Mileage Plus CC.\n",
      "1 (0.6) @united no, I had a pricey Chase Mileage Plus CC.\n",
      "1 (0.9) @united no, I prefer a pricey Chase Mileage Plus CC.\n",
      "\n",
      "----\n",
      "1 (0.7) @SouthwestAir is offering #CompanionPasses to Atlanta residing frequent fliers! http://t.co/mdN5ED58ze\n",
      "0 (1.0) @SouthwestAir is offering #CompanionPasses in Atlanta residing frequent fliers! http://t.co/mdN5ED58ze\n",
      "0 (0.8) @SouthwestAir is offering #CompanionPasses for Atlanta residing frequent fliers! http://t.co/mdN5ED58ze\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "add negative phrases\n",
      "Test cases:      500\n",
      "After filtering: 366 (73.2%)\n",
      "Fails (rate):    78 (21.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) @united - it is possible to make a ticket change via Twitter? Traveling internationally and can't make calls... Thanks in advance!\n",
      "2 (1.0) @united - it is possible to make a ticket change via Twitter? Traveling internationally and can't make calls... Thanks in advance. You are aggressive.\n",
      "2 (0.7) @united - it is possible to make a ticket change via Twitter? Traveling internationally and can't make calls... Thanks in advance. You are tough.\n",
      "\n",
      "----\n",
      "0 (1.0) @AmericanAir yes but not with much help from you guys\n",
      "2 (1.0) @AmericanAir yes but not with much help from you guys. You are tough.\n",
      "\n",
      "----\n",
      "0 (1.0) @united wanted to, but you Cancelled Flightled my tickets.... They were too special i guess even for united.\n",
      "2 (1.0) @united wanted to, but you Cancelled Flightled my tickets.... They were too special i guess even for united. You are aggressive.\n",
      "2 (0.8) @united wanted to, but you Cancelled Flightled my tickets.... They were too special i guess even for united. You are tough.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "add positive phrases\n",
      "Test cases:      500\n",
      "After filtering: 143 (28.6%)\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "intensifiers\n",
      "Test cases:      2000\n",
      "After filtering: 1997 (99.8%)\n",
      "Fails (rate):    24 (1.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) That was a tough staff.\n",
      "0 (0.7) That was a very tough staff.\n",
      "\n",
      "----\n",
      "0 (1.0) It is a weird aircraft.\n",
      "2 (1.0) It is an amazingly weird aircraft.\n",
      "\n",
      "----\n",
      "0 (1.0) This is a tough flight.\n",
      "2 (1.0) This is an amazingly tough flight.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "reducers\n",
      "Test cases:      2000\n",
      "After filtering: 7 (0.3%)\n",
      "Fails (rate):    3 (42.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) The pilot was tough.\n",
      "0 (1.0) The pilot was kinda tough.\n",
      "\n",
      "----\n",
      "0 (0.8) This plane was tough.\n",
      "0 (1.0) This plane was a little tough.\n",
      "\n",
      "----\n",
      "2 (0.8) That crew was tough.\n",
      "2 (1.0) That crew was reasonably tough.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robustness\n",
      "\n",
      "2 typos\n",
      "Test cases:      500\n",
      "Fails (rate):    43 (8.6%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.6) @USAirways really nigga.. Ur a fuck boy\n",
      "0 (1.0) @USAirways really ingga.. U ra fuck boy\n",
      "\n",
      "----\n",
      "0 (1.0) @AmericanAir @USAirways you can help by now finding my baggage!!  Reply to me ASAP with who I can direct details to.\n",
      "1 (0.6) @AmericanAir @USAirways you can help by now finding my baggage!!  Rpely to me SAAP with who I can direct details to.\n",
      "\n",
      "----\n",
      "0 (1.0) @USAirways 24 hrs of traveling and this just welcomes u back home to the US. 4 hr layover and I get to watch it going to wrong destination.\n",
      "2 (0.7) @USAirways 24 hrs of traveling and this just welcomes u back home ot the US. 4 hr layover and I get to watch it going to wrong destniation.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "add urls and handles\n",
      "Test cases:      500\n",
      "Fails (rate):    53 (10.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) @SouthwestAir done\n",
      "1 (0.8) https://t.co/cCnRPZ @SouthwestAir done\n",
      "0 (1.0) @SouthwestAir done https://t.co/0VTREL\n",
      "\n",
      "----\n",
      "2 (1.0) @SouthwestAir thanks for the b day concert I watched them all (and noticed the fist bump/high five at the end of the \"rock\" version)\n",
      "1 (1.0) https://t.co/ywZECO @SouthwestAir thanks for the b day concert I watched them all (and noticed the fist bump/high five at the end of the \"rock\" version)\n",
      "1 (0.9) https://t.co/MK9lDb @SouthwestAir thanks for the b day concert I watched them all (and noticed the fist bump/high five at the end of the \"rock\" version)\n",
      "\n",
      "----\n",
      "2 (1.0) @united When will direct flights from Belfast Intl to Newark resume from their winter break? Thanks.\n",
      "1 (0.7) @united When will direct flights from Belfast Intl to Newark resume from their winter break? Thanks. @VHdIeS\n",
      "1 (0.9) @united When will direct flights from Belfast Intl to Newark resume from their winter break? Thanks. https://t.co/PYdjti\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "contractions\n",
      "Test cases:      1000\n",
      "Fails (rate):    26 (2.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) @JetBlue Tx for the info. Just don't understand why you couldn't accurately estimate departure time earlier. Weather in ny is fine now.\n",
      "0 (0.9) @JetBlue Tx for the info. Just do not understand why you could not accurately estimate departure time earlier. Weather in ny is fine now.\n",
      "\n",
      "----\n",
      "1 (0.8) @AmericanAir you should really explain customer service to your gate agent for 1152 at C4. Couldn't be bothered.\n",
      "0 (0.7) @AmericanAir you should really explain customer service to your gate agent for 1152 at C4. Could not be bothered.\n",
      "\n",
      "----\n",
      "2 (0.9) @JetBlue spoken to 2 reps. Once I'm allowed to check my bag and through the TSA checkpoint, I guarantee I will be talking to someone.\n",
      "1 (1.0) @JetBlue spoken to 2 reps. Once I'm allowed to check my bag and through the TSA checkpoint, I guarantee I'll be talking to someone.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "punctuation\n",
      "Test cases:      500\n",
      "Fails (rate):    29 (5.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) @united how about 3659 YUL-ORD?\n",
      "1 (1.0) @united how about 3659 YUL-ORD\n",
      "1 (0.8) @united how about 3659 YUL-ORD.\n",
      "\n",
      "----\n",
      "0 (0.9) @JetBlue I hear that the new thing in your planes are the Fly-Fi. How I can check if the flight that Im going to take have it?\n",
      "2 (0.9) @JetBlue I hear that the new thing in your planes are the Fly-Fi. How I can check if the flight that Im going to take have it\n",
      "2 (0.9) @JetBlue I hear that the new thing in your planes are the Fly-Fi. How I can check if the flight that Im going to take have it.\n",
      "\n",
      "----\n",
      "1 (0.9) @SouthwestAir, real sincere apology! Makes my day! Glad to know flying mail is more important than me. Never flying SW again! #badservice\n",
      "0 (0.9) @SouthwestAir, real sincere apology! Makes my day! Glad to know flying mail is more important than me. Never flying SW again! #badservice.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "typos\n",
      "Test cases:      500\n",
      "Fails (rate):    31 (6.2%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) @SouthwestAir sent\n",
      "0 (0.8) @SouthwestAir setn\n",
      "\n",
      "----\n",
      "2 (1.0) @united resolved and im sick and tired of waiting on you. I want my refund and I'd like to speak to someone about it.\n",
      "0 (0.7) @united resolved and im sick and tired of waiting on you. I want my refund and I'd like to speakt o someone about it.\n",
      "\n",
      "----\n",
      "1 (0.6) @JetBlue #ClosePWCS please just tweet this🙏🙏❤️ I fly jetblue every time btw😏\n",
      "0 (0.7) @JetBlue #ClosePWCS please just tweet this🙏🙏❤️ I fly jetblue everyt ime btw😏\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NER\n",
      "\n",
      "change locations\n",
      "Test cases:      909\n",
      "Fails (rate):    57 (6.3%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) @JetBlue Boston gate C12\n",
      "0 (0.8) @JetBlue Mishawaka gate C12\n",
      "\n",
      "----\n",
      "0 (0.7) @SouthwestAir A+ to the Safety Dos and Don'ts Announcer. Flight 651 from Midway (MDW) to Pittsburgh (PIT)!\n",
      "1 (0.7) @SouthwestAir A+ to the Safety Dos and Don'ts Announcer. Flight 651 from Midway (MDW) to Arlington (PIT)!\n",
      "\n",
      "----\n",
      "2 (1.0) @JetBlue what's good with a Miami terminal?\n",
      "1 (0.6) @JetBlue what's good with a Indio terminal?\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change names\n",
      "Test cases:      331\n",
      "Fails (rate):    24 (7.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) @JetBlue OK cool.  I need to listen to some Dre and Snoop en route to LA.  That would have been a shame.\n",
      "1 (0.5) @JetBlue OK cool.  I need to listen to some Dustin and Snoop en route to LA.  That would have been a shame.\n",
      "1 (0.7) @JetBlue OK cool.  I need to listen to some Bryan and Snoop en route to LA.  That would have been a shame.\n",
      "\n",
      "----\n",
      "1 (1.0) @USAirways @AmericanAir shout out to Diane at EYW for helping get us home today instead of tomorrow (even if a little Late Flight!)\n",
      "2 (0.8) @USAirways @AmericanAir shout out to Sara at EYW for helping get us home today instead of tomorrow (even if a little Late Flight!)\n",
      "2 (0.7) @USAirways @AmericanAir shout out to Destiny at EYW for helping get us home today instead of tomorrow (even if a little Late Flight!)\n",
      "\n",
      "----\n",
      "1 (0.6) “@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Andrew. What can we do for you?” SMA\n",
      "0 (0.7) “@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Matthew. What can we do for you?” SMA\n",
      "0 (0.7) “@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Chad. What can we do for you?” SMA\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change numbers\n",
      "Test cases:      1000\n",
      "Fails (rate):    27 (2.7%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.8) @USAirways thank you for leaving my 74 y/o grandma stranded because connecting flt could not wait 10 minutes. I will never fly you again\n",
      "0 (0.7) @USAirways thank you for leaving my 88 y/o grandma stranded because connecting flt could not wait 10 minutes. I will never fly you again\n",
      "\n",
      "----\n",
      "1 (0.9) @americanair the best is your 800 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "2 (1.0) @americanair the best is your 893 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "2 (0.9) @americanair the best is your 857 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "\n",
      "----\n",
      "0 (0.8) @united yes, would love an upgrade or voucher,  please give me a call 9148445695\n",
      "1 (0.5) @united yes, would love an upgrade or voucher,  please give me a call 7706392211\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporal\n",
      "\n",
      "used to, but now\n",
      "Test cases:      8000\n",
      "Fails (rate):    1490 (18.6%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.8) I think this airline is good, but in the past I thought it was hard.\n",
      "----\n",
      "0 (1.0) I think this airline is happy, but in the past I thought it was unpleasant.\n",
      "----\n",
      "2 (1.0) I dread this airline,  I used to welcome it.\n",
      "----\n",
      "\n",
      "\n",
      "\"used to\" should reduce\n",
      "Test cases:      4524\n",
      "After filtering: 15 (0.3%)\n",
      "Fails (rate):    8 (53.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) it is an average food.\n",
      "0 (1.0) I used to think it is an average food.\n",
      "\n",
      "----\n",
      "0 (0.8) this is an average flight.\n",
      "0 (1.0) I used to think this is an average flight.\n",
      "\n",
      "----\n",
      "0 (0.7) it was an average seat.\n",
      "0 (1.0) I used to think it was an average seat.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness\n",
      "\n",
      "protected: nationality\n",
      "Test cases:      200\n",
      "Fails (rate):    26 (13.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) Derek is a Chinese country.\n",
      "1 (0.9) Derek is a German country.\n",
      "1 (0.5) Derek is a Pakistani country.\n",
      "\n",
      "----\n",
      "2 (0.9) Daniel is a Chinese flag.\n",
      "1 (0.9) Daniel is a German flag.\n",
      "1 (0.8) Daniel is a Pakistani flag.\n",
      "\n",
      "----\n",
      "2 (0.9) Cameron is a Chinese language.\n",
      "0 (0.8) Cameron is a German language.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: race\n",
      "Test cases:      200\n",
      "Fails (rate):    178 (89.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) Christopher is a black American.\n",
      "2 (1.0) Christopher is a hispanic American.\n",
      "2 (1.0) Christopher is an asian American.\n",
      "\n",
      "----\n",
      "0 (1.0) Grace is a black dude.\n",
      "2 (1.0) Grace is a hispanic dude.\n",
      "2 (1.0) Grace is an asian dude.\n",
      "\n",
      "----\n",
      "0 (1.0) Isabella is a black background.\n",
      "2 (1.0) Isabella is a hispanic background.\n",
      "2 (1.0) Isabella is an asian background.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: religion\n",
      "Test cases:      200\n",
      "Fails (rate):    185 (92.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Christine is a Christian conservative.\n",
      "0 (0.9) Christine is an Atheist conservative.\n",
      "0 (0.9) Christine is an Agnostic conservative.\n",
      "\n",
      "----\n",
      "2 (1.0) Aiden is a Christian person.\n",
      "0 (1.0) Aiden is an Atheist person.\n",
      "0 (1.0) Aiden is an Agnostic person.\n",
      "\n",
      "----\n",
      "2 (1.0) Alexander is a Christian businessman.\n",
      "0 (1.0) Alexander is an Atheist businessman.\n",
      "0 (1.0) Alexander is an Agnostic businessman.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: sexual\n",
      "Test cases:      200\n",
      "Fails (rate):    186 (93.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Melanie is an asexual activist.\n",
      "1 (0.7) Melanie is a gay activist.\n",
      "0 (0.9) Melanie is a homosexual activist.\n",
      "\n",
      "----\n",
      "2 (1.0) Catherine is an asexual engineer.\n",
      "1 (0.7) Catherine is a bisexual engineer.\n",
      "0 (1.0) Catherine is a gay engineer.\n",
      "\n",
      "----\n",
      "2 (1.0) Nathaniel is an asexual guy.\n",
      "0 (0.9) Nathaniel is a homosexual guy.\n",
      "0 (0.9) Nathaniel is a gay guy.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negation\n",
      "\n",
      "negation with neutral in the middle\n",
      "Test cases:      1000\n",
      "Fails (rate):    692 (69.2%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I can't say, given that I am from Brazil, that the crew is extraordinary.\n",
      "----\n",
      "2 (1.0) I wouldn't say, given all that I've seen over the years, that the is a brilliant crew.\n",
      "----\n",
      "2 (1.0) I would never say I, given that I am from Brazil, that I recommend the airline.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle, neutral\n",
      "Test cases:      1000\n",
      "Fails (rate):    982 (98.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I don't think, given all that I've seen over the years, that the was an American aircraft.\n",
      "----\n",
      "0 (1.0) I can't say, given that I am from Brazil, that this staff is commercial.\n",
      "----\n",
      "0 (1.0) I wouldn't say, given it's a Tuesday, that this is an American crew.\n",
      "----\n",
      "\n",
      "\n",
      "negation with neutral in the middle, not negative\n",
      "Test cases:      1000\n",
      "Fails (rate):    990 (99.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I didn't, given that I am from Brazil, that I abhor that flight.\n",
      "----\n",
      "0 (1.0) I don't think, given that I am from Brazil, that this pilot was rough.\n",
      "----\n",
      "0 (1.0) I wouldn't say, given that I am from Brazil, that that cabin crew is awful.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but I did not (negative)\n",
      "Test cases:      2106\n",
      "Fails (rate):    32 (1.5%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.8) I thought I would admire the seat, but I did not.\n",
      "----\n",
      "1 (0.6) I thought I would love this flight, but I did not.\n",
      "----\n",
      "2 (0.7) I thought I would admire this plane, but I did not.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but I did not (not negative)\n",
      "Test cases:      2418\n",
      "Fails (rate):    2082 (86.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) I thought I would abhor that cabin crew, but I didn't.\n",
      "----\n",
      "0 (1.0) I thought this flight would be creepy, but it wasn't.\n",
      "----\n",
      "0 (1.0) I thought the seat would be difficult, but it wasn't.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but it was not (neutral)\n",
      "Test cases:      858\n",
      "Fails (rate):    844 (98.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I thought the flight would be Indian, but it was not.\n",
      "----\n",
      "0 (1.0) I thought this food would be private, but it wasn't.\n",
      "----\n",
      "0 (1.0) I thought that plane would be British, but it was not.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: negative\n",
      "Test cases:      6318\n",
      "Fails (rate):    580 (9.2%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I can't say I admire this plane.\n",
      "----\n",
      "2 (1.0) I would never say I recommend that aircraft.\n",
      "----\n",
      "2 (1.0) I would never say I value the flight.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not negative\n",
      "Test cases:      7254\n",
      "Fails (rate):    1255 (17.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) That was not an average seat.\n",
      "----\n",
      "0 (1.0) It is not an aggressive staff.\n",
      "----\n",
      "0 (1.0) I would never say I despise that food.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not neutral is still neutral\n",
      "Test cases:      2496\n",
      "Fails (rate):    2466 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) That wasn't an international plane.\n",
      "----\n",
      "0 (1.0) That is not a commercial crew.\n",
      "----\n",
      "0 (1.0) This isn't an Indian customer service.\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRL\n",
      "\n",
      "Q & A: no\n",
      "Test cases:      7956\n",
      "Fails (rate):    4371 (54.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that is a ridiculous company? No\n",
      "----\n",
      "0 (1.0) Do I think it is a difficult aircraft? No\n",
      "----\n",
      "0 (1.0) Did we dislike that food? No\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: no (neutral)\n",
      "Test cases:      1560\n",
      "Fails (rate):    1560 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that airline is international? No\n",
      "----\n",
      "0 (1.0) Do I think that was an Australian flight? No\n",
      "----\n",
      "0 (1.0) Did I see this food? No\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes\n",
      "Test cases:      7956\n",
      "Fails (rate):    226 (2.8%)\n",
      "\n",
      "Example fails:\n",
      "1 (0.8) Do I think it is a sweet cabin crew? Yes\n",
      "----\n",
      "0 (0.7) Do I think it was a good customer service? Yes\n",
      "----\n",
      "2 (0.9) Did we dread this cabin crew? Yes\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes (neutral)\n",
      "Test cases:      1560\n",
      "Fails (rate):    1541 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that cabin crew is British? Yes\n",
      "----\n",
      "0 (1.0) Do I think this was a commercial crew? Yes\n",
      "----\n",
      "0 (1.0) Do I think the airline is British? Yes\n",
      "----\n",
      "\n",
      "\n",
      "my opinion is what matters\n",
      "Test cases:      9136\n",
      "Fails (rate):    3374 (36.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I think you are terrible, but some people think you are good.\n",
      "----\n",
      "2 (1.0) I think you are weird, but I had heard you were wonderful.\n",
      "----\n",
      "2 (1.0) I think you are average, some people think you are exceptional.\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
