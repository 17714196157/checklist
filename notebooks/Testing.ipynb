{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "import spacy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import Mft\n",
    "from checklist.inv_dir import Inv, Dir\n",
    "from checklist.expect import Expect\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = checklist.text_generation.TextGenerator(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tg.unmask_multiple(['I really <mask> the pilot.', 'I really <mask> the flight.'], metric='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "editor = checklist.editor.Editor()\n",
    "c = pickle.load(open('/home/marcotcr/work/ml-tests/data/common.pkl', 'rb'))\n",
    "editor.lexicons['male'] = c['male_names']\n",
    "editor.lexicons['female'] = c['female_names']\n",
    "c.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = ['flight', 'seat', 'pilot', 'staff', 'plane', 'airline', 'cabin crew', 'aircraft', 'food']\n",
    "pos = ['liked', 'enjoyed', 'loved', 'admired', 'appreciated']\n",
    "neg = ['hated', 'disliked', 'regretted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'liked', 'love', 'enjoyed', 'loved', 'appreciate', 'enjoy', 'appreciated', 'miss', 'Like', 'missed', 'needed', 'hate', 'wanted', 'likes', 'got', 'admired', 'admire', 'hated', 'recommend']\n"
     ]
    }
   ],
   "source": [
    "ts = editor.template('I really <mask> the {n}.', n=nouns)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000)][:20])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'We', 'He', 'he', 'She', 'They', 'we', 'and', '...', 'she', 'they', 'Everyone', 'i', 'People', 'You', 'Alex', 'Everybody', 'And', 'Dad', 'John']\n"
     ]
    }
   ],
   "source": [
    "ts = editor.template('<mask> really {pn} the {n}.', n=nouns, pn=pos+neg)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000)][:20])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = ['I', 'We', 'They', 'we', 'He', 'he', 'She', 'she', 'they', 'people', 'People', 'you', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'boring', 'strange', 'cold', 'slow', 'unpleasant', 'weird', 'difficult', 'uncomfortable', 'expensive', 'rough', 'noisy', 'ugly', 'poor', 'dark', 'different', 'small', 'disappointing', 'quiet', 'rude', 'confusing', 'dull', 'sad', 'odd', 'loud', 'annoying', 'simple', 'harsh', 'short', 'unusual', 'scary', 'hard', 'nice', 'hot', 'dangerous', 'aggressive', 'heavy', 'dirty', 'conservative', 'old', 'tough', 'bland', 'low', 'cheap', 'frustrating', 'depressing', 'nasty', 'long', 'crude', 'horrible', 'stupid', 'complicated', 'funny', 'fast', 'good', 'basic', 'weak', 'wrong', 'awkward', 'terrible', 'negative', 'crowded', 'plain', 'thin', 'interesting', 'awful', 'bizarre', 'intimidating', 'cool', 'primitive', 'unstable', 'flat', 'narrow', 'stressful', 'stiff', 'empty', 'chaotic', 'light', 'irritating', 'big', 'unfair', 'sick', 'similar', 'angry', 'mean', 'predictable', 'easy', 'offensive', 'foreign', 'violent', 'tight', 'comfortable', 'busy', 'challenging', 'arrogant', 'disgusting', 'inefficient', 'creepy', 'tense', 'disturbing']\n"
     ]
    }
   ],
   "source": [
    "ts = editor.template('I didn\\'t like the {n}, it was very <mask>.', n=nouns)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000, metric='avg')][:100])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'nice', 'helpful', 'comfortable', 'cool', 'efficient', 'pleasant', 'interesting', 'impressive', 'welcoming', 'professional', 'beautiful', 'exciting', 'positive', 'solid', 'amazing', 'wonderful', 'lovely']\n",
    "neg_adj = ['bad', 'boring', 'unpleasant', 'difficult', 'uncomfortable', 'ugly', 'poor', 'disappointing', 'sad', 'annoying', 'dirty', 'frustrating', 'depressing', 'nasty', 'horrible', 'stupid', 'negative', 'awful', 'stressful', 'irritating', 'disgusting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_eg = editor.template('{sub}{really} {p} the {n}.', sub=subj, really=[' really', ''], n=nouns, p=pos)\n",
    "pos_eg += editor.template('The {n} was{very} {p}.', sub=subj, very=[' very', ''], n=nouns, p=pos_adj)\n",
    "neg_eg = editor.template('{sub}{really} {p} the {n}.', sub=subj, really=[' really', ''], n=nouns, p=neg)\n",
    "neg_eg += editor.template('The {n} was{very} {p}.', sub=subj, very=[' very', ''], n=nouns, p=neg_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 1080)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_eg), len(neg_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = Mft(pos_eg, labels=1)\n",
    "test_neg = Mft(neg_eg, labels=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pos.set_expect(Expect.single(Expect.eq()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1494 examples\n",
      "Predicting 1080 examples\n"
     ]
    }
   ],
   "source": [
    "test_pos.run(pp)\n",
    "test_neg.run(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1080\n",
      "Failure rate:   1.7%\n",
      "\n",
      "Example fails:\n",
      "1 (1.0) The cabin crew was very stressful.\n",
      "1 (1.0) The flight was very stressful.\n",
      "1 (1.0) The flight was stressful.\n",
      "1 (1.0) The aircraft was stressful.\n",
      "1 (1.0) The seat was very stressful.\n"
     ]
    }
   ],
   "source": [
    "test_neg.summary(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1494\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "test_pos.summary(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = [x[0][0] for x in tg.unmask('John is a black <mask>.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = ['black', 'white', 'hispanic', 'asian']\n",
    "d = editor.template(['{male} is a %s {adj}.' % r for r in races], adj=adj, return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8000 examples\n"
     ]
    }
   ],
   "source": [
    "test = Inv(d, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     2000\n",
      "Failure rate:   95.0%\n",
      "\n",
      "Example fails:\n",
      "0 (0.1) Gabriel is a black Muslim.\n",
      "1 (0.9) Gabriel is a hispanic Muslim.\n",
      "1 (0.9) Gabriel is a asian Muslim.\n",
      "\n",
      "0 (0.0) James is a black person.\n",
      "1 (0.9) James is a hispanic person.\n",
      "1 (0.9) James is a asian person.\n",
      "\n",
      "0 (0.1) Jason is a black girl.\n",
      "1 (1.0) Jason is a hispanic girl.\n",
      "1 (1.0) Jason is a asian girl.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12 black\n",
      "0.19 white\n",
      "0.96 hispanic\n",
      "0.98 asian\n"
     ]
    }
   ],
   "source": [
    "pps = np.array([[x[i][1] for i in range(4)] for x in test.results.confs])\n",
    "for i in range(4):\n",
    "    print('%.2f %s' % (pps[:, i].mean(), races[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as MFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, m = editor.template('{male} is a {r} {adj}.', r=races,  adj=adj, return_maps=True, nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 'black', 'adj': 'man', 'male': 'Michael'}"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None], dtype=object)"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def is_fair(data, preds, confs, labels, metas):\n",
    "    rets = collections.defaultdict(lambda: [])\n",
    "    ret = np.repeat(None, len(data))\n",
    "    for c, m in zip(confs, metas):\n",
    "        rets[m['r']].append(c[1])\n",
    "    for i, r in enumerate(rets):\n",
    "        rets[r] = np.mean(rets[r])\n",
    "        ret[i] = rets[r]\n",
    "    print(rets)\n",
    "    return ret \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n",
      "defaultdict(<function is_fair.<locals>.<lambda> at 0x7f126044f598>, {'white': 0.356781789360361, 'asian': 0.9733978605270386, 'hispanic': 0.9593281026544243, 'black': 0.12038368929643184})\n"
     ]
    }
   ],
   "source": [
    "ex = Expect.test(is_fair)\n",
    "test = Mft(d, expect=ex, meta=m)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bad idea, fairness tests should just be INVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inv, dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_typo(string):\n",
    "    string = list(string)\n",
    "    swaps = 1\n",
    "    swaps = np.random.choice(len(string) - 1, swaps)\n",
    "    for swap in swaps:\n",
    "        swap = np.random.choice(len(string) - 1)\n",
    "        tmp = string[swap]\n",
    "        string[swap] = string[swap + 1]\n",
    "        string[swap + 1] = tmp\n",
    "    return ''.join(string)\n",
    "\n",
    "def add_typos(string):\n",
    "    return list(set([add_typo(string) for _ in range(10)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 100), add_typos, keep_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1043 examples\n"
     ]
    }
   ],
   "source": [
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     100\n",
      "Failure rate:   24.0%\n",
      "\n",
      "Example fails:\n",
      "0.0 @JetBlue Not for the dates or destination I'm headed 😔\n",
      "0.5 @JetBlu eNot for the dates or destination I'm headed 😔\n",
      "\n",
      "0.5 @AmericanAir will do. I also passed the website around to other passengers.\n",
      "0.7 @AmericanAir will do. I also passed the website around to other psasengers.\n",
      "0.7 @AmericanAir will do. I also passed the ewbsite around to other passengers.\n",
      "\n",
      "1.0 @united DM'ed you\n",
      "0.1 @uinted DM'ed you\n",
      "0.1 @united DM'ed yuo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2000 examples\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 1000), add_typo, keep_original=True)\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1000\n",
      "Failure rate:   4.9%\n",
      "\n",
      "Example fails:\n",
      "0.7 @united when will you offer real food in american clubs like the amazing food you offer in Heathrow?\n",
      "0.2 @uinted when will you offer real food in american clubs like the amazing food you offer in Heathrow?\n",
      "\n",
      "1.0 @united maybe on my return trip 👍\n",
      "0.1 @united maybe on my erturn trip 👍\n",
      "\n",
      "0.7 @united was in the air. Just DMd you\n",
      "0.0 @united was in the ai.r Just DMd you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get from somethwere else\n",
    "def add_negatives(string):\n",
    "    string = string.strip('.')\n",
    "    return [string + '. ' + l for l in ['I hate you', 'I despise you', 'You suck']]\n",
    "def add_positive(string):\n",
    "    string = string.strip('.')\n",
    "    return [string + '. ' + l for l in ['I love you', 'I like you', 'You are great']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:     1000\n",
      "Filtered cases: 259 (25.9%)\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 1000), add_negatives, keep_original=True)\n",
    "expect_fn = Expect.monotonic(1, increasing=False, tolerance=0.1)\n",
    "test = Dir(data, expect_fn)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=False)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper2(fn, check):\n",
    "    def fzz():\n",
    "        msg = fn()\n",
    "        print('fzz', msg, check)\n",
    "    return fzz\n",
    "def wrapper(fn, check=True):\n",
    "    def fz():\n",
    "        msg = fn() + 'fz'\n",
    "        return msg\n",
    "    return wrapper2(fz, check)\n",
    "    \n",
    "def f1(label=None, un=3, a=3, b=3):\n",
    "    def f():\n",
    "        nonlocal label\n",
    "        if label is None:\n",
    "            label = 3\n",
    "        return str(label) + str(un)\n",
    "    return wrapper(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fzz 33fz True\n"
     ]
    }
   ],
   "source": [
    "f1(3)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['woman'], ' Mary is not a man, she is a woman.', 17.750459671020508),\n",
       " (['child'], ' Mary is not a man, she is a child.', 14.719247817993164),\n",
       " (['monster'], ' Mary is not a man, she is a monster.', 14.308874130249023),\n",
       " (['girl'], ' Mary is not a man, she is a girl.', 13.8037109375),\n",
       " (['beast'], ' Mary is not a man, she is a beast.', 13.60313606262207),\n",
       " (['machine'], ' Mary is not a man, she is a machine.', 13.506260871887207),\n",
       " (['god'], ' Mary is not a man, she is a god.', 13.43105411529541),\n",
       " (['God'], ' Mary is not a man, she is a God.', 13.350319862365723),\n",
       " (['man'], ' Mary is not a man, she is a man.', 13.242627143859863),\n",
       " (['ghost'], ' Mary is not a man, she is a ghost.', 13.10810661315918),\n",
       " (['demon'], ' Mary is not a man, she is a demon.', 13.078417778015137),\n",
       " (['creature'], ' Mary is not a man, she is a creature.', 13.015871047973633),\n",
       " (['slave'], ' Mary is not a man, she is a slave.', 12.9219970703125),\n",
       " (['goddess'], ' Mary is not a man, she is a goddess.', 12.877565383911133),\n",
       " (['lady'], ' Mary is not a man, she is a lady.', 12.86233139038086),\n",
       " (['witch'], ' Mary is not a man, she is a witch.', 12.82192325592041),\n",
       " (['spirit'], ' Mary is not a man, she is a spirit.', 12.805957794189453),\n",
       " (['dog'], ' Mary is not a man, she is a dog.', 12.504145622253418),\n",
       " (['mother'], ' Mary is not a man, she is a mother.', 12.4073486328125),\n",
       " (['bird'], ' Mary is not a man, she is a bird.', 12.404468536376953)]"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.unmask('Mary is not a man, she is a <mask>.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['joke'], ' Mary is a joke.', 10.655401229858398),\n",
       " (['woman'], ' Mary is a woman.', 10.290670394897461),\n",
       " (['problem'], ' Mary is a problem.', 10.219472885131836),\n",
       " (['man'], ' Mary is a man.', 10.078128814697266),\n",
       " (['metaphor'], ' Mary is a metaphor.', 9.863449096679688),\n",
       " (['lie'], ' Mary is a lie.', 9.859661102294922),\n",
       " (['myth'], ' Mary is a myth.', 9.854726791381836),\n",
       " (['story'], ' Mary is a story.', 9.853984832763672),\n",
       " (['girl'], ' Mary is a girl.', 9.726561546325684),\n",
       " (['mess'], ' Mary is a mess.', 9.668272018432617),\n",
       " (['mistake'], ' Mary is a mistake.', 9.623100280761719),\n",
       " (['shame'], ' Mary is a shame.', 9.5949068069458),\n",
       " (['boy'], ' Mary is a boy.', 9.545333862304688),\n",
       " (['dream'], ' Mary is a dream.', 9.408576965332031),\n",
       " (['monster'], ' Mary is a monster.', 9.394660949707031),\n",
       " (['fact'], ' Mary is a fact.', 9.374675750732422),\n",
       " (['pseudonym'], ' Mary is a pseudonym.', 9.32861328125),\n",
       " (['robot'], ' Mary is a robot.', 9.310115814208984),\n",
       " (['tragedy'], ' Mary is a tragedy.', 9.306939125061035),\n",
       " (['typo'], ' Mary is a typo.', 9.275297164916992)]"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.unmask('Mary is a <mask>.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['was', 'young'], ' John was a young man.', 14.495121955871582),\n",
       " (['died', 'free'], ' John died a free man.', 14.45598316192627),\n",
       " (['was', 'good'], ' John was a good man.', 14.438433170318604),\n",
       " (['is', 'good'], ' John is a good man.', 14.233307838439941),\n",
       " (['died', 'young'], ' John died a young man.', 14.056387901306152),\n",
       " ([\"'s\", 'good'], \" John's a good man.\", 14.03753137588501),\n",
       " (['is', 'young'], ' John is a young man.', 13.927804470062256),\n",
       " (['was', 'black'], ' John was a black man.', 13.852426528930664),\n",
       " (['was', 'great'], ' John was a great man.', 13.799434185028076),\n",
       " (['was', 'free'], ' John was a free man.', 13.700941562652588),\n",
       " (['was', 'happy'], ' John was a happy man.', 13.667884826660156),\n",
       " (['was', 'rich'], ' John was a rich man.', 13.655069828033447),\n",
       " (['was', 'beautiful'], ' John was a beautiful man.', 13.651923179626465),\n",
       " (['is', 'great'], ' John is a great man.', 13.616724967956543),\n",
       " (['was', 'troubled'], ' John was a troubled man.', 13.616347789764404),\n",
       " (['became', 'better'], ' John became a better man.', 13.610582828521729),\n",
       " (['was', 'lonely'], ' John was a lonely man.', 13.610520362854004),\n",
       " (['was', 'mad'], ' John was a mad man.', 13.609537601470947),\n",
       " ([\"'s\", 'young'], \" John's a young man.\", 13.599075317382812),\n",
       " (['was', 'changed'], ' John was a changed man.', 13.59891128540039),\n",
       " (['are', 'good'], ' John are a good man.', 13.573655605316162),\n",
       " (['became', 'free'], ' John became a free man.', 13.573292255401611),\n",
       " (['was', 'powerful'], ' John was a powerful man.', 13.539580345153809),\n",
       " (['was', 'nice'], ' John was a nice man.', 13.514240741729736),\n",
       " (['was', 'big'], ' John was a big man.', 13.502318382263184),\n",
       " (['was', 'dead'], ' John was a dead man.', 13.48315954208374),\n",
       " (['is', 'beautiful'], ' John is a beautiful man.', 13.47053050994873),\n",
       " (['was', 'strong'], ' John was a strong man.', 13.466229915618896),\n",
       " (['was', 'brave'], ' John was a brave man.', 13.456400394439697),\n",
       " (['was', 'broken'], ' John was a broken man.', 13.451623916625977),\n",
       " (['was', 'poor'], ' John was a poor man.', 13.432952880859375),\n",
       " (['is', 'dead'], ' John is a dead man.', 13.380314826965332),\n",
       " (['is', 'powerful'], ' John is a powerful man.', 13.379196166992188),\n",
       " (['is', 'brave'], ' John is a brave man.', 13.366837501525879),\n",
       " ([\"'s\", 'smart'], \" John's a smart man.\", 13.345737934112549),\n",
       " (['is', 'wonderful'], ' John is a wonderful man.', 13.342327117919922),\n",
       " ([\"'s\", 'nice'], \" John's a nice man.\", 13.342204093933105),\n",
       " (['became', 'changed'], ' John became a changed man.', 13.310935497283936),\n",
       " (['is', 'smart'], ' John is a smart man.', 13.29515552520752),\n",
       " ([\"'s\", 'great'], \" John's a great man.\", 13.283424854278564),\n",
       " ([\"'s\", 'lucky'], \" John's a lucky man.\", 13.276181697845459),\n",
       " ([\"'s\", 'funny'], \" John's a funny man.\", 13.265432357788086),\n",
       " (['died', 'good'], ' John died a good man.', 13.261393547058105),\n",
       " (['died', 'happy'], ' John died a happy man.', 13.244301795959473),\n",
       " (['is', 'nice'], ' John is a nice man.', 13.229793548583984),\n",
       " (['is', 'happy'], ' John is a happy man.', 13.219892978668213),\n",
       " (['is', 'remarkable'], ' John is a remarkable man.', 13.218419075012207),\n",
       " (['is', 'lucky'], ' John is a lucky man.', 13.215352535247803),\n",
       " (['is', 'changed'], ' John is a changed man.', 13.199608325958252),\n",
       " (['died', 'homeless'], ' John died a homeless man.', 13.183060646057129),\n",
       " (['is', 'black'], ' John is a black man.', 13.15600299835205),\n",
       " (['is', 'rich'], ' John is a rich man.', 13.150885581970215),\n",
       " (['is', 'strange'], ' John is a strange man.', 13.147069931030273),\n",
       " (['is', 'dangerous'], ' John is a dangerous man.', 13.127272129058838),\n",
       " (['is', 'funny'], ' John is a funny man.', 13.12220811843872),\n",
       " (['is', 'fine'], ' John is a fine man.', 13.11849594116211),\n",
       " (['died', 'strong'], ' John died a strong man.', 13.09574031829834),\n",
       " ([\"'s\", 'big'], \" John's a big man.\", 13.09388017654419),\n",
       " ([\"'s\", 'grown'], \" John's a grown man.\", 13.012920379638672),\n",
       " (['became', 'young'], ' John became a young man.', 12.983636379241943),\n",
       " ([\"'s\", 'tough'], \" John's a tough man.\", 12.982636451721191),\n",
       " ([\"'s\", 'beautiful'], \" John's a beautiful man.\", 12.955739498138428),\n",
       " (['were', 'good'], ' John were a good man.', 12.93846845626831),\n",
       " ([\"'s\", 'happy'], \" John's a happy man.\", 12.928511142730713),\n",
       " ([',', 'good'], ' John, a good man.', 12.926117897033691),\n",
       " (['became', 'black'], ' John became a black man.', 12.910524845123291),\n",
       " ([\"'s\", 'busy'], \" John's a busy man.\", 12.903635025024414),\n",
       " (['died', 'gentle'], ' John died a gentle man.', 12.883638381958008),\n",
       " ([\"'s\", 'brave'], \" John's a brave man.\", 12.866588592529297),\n",
       " (['became', 'family'], ' John became a family man.', 12.865127563476562),\n",
       " ([\"'m\", 'gay'], \" John'm a gay man.\", 12.845515727996826),\n",
       " (['are', 'great'], ' John are a great man.', 12.820730686187744),\n",
       " (['became', 'good'], ' John became a good man.', 12.813342571258545),\n",
       " ([\"'s\", 'wonderful'], \" John's a wonderful man.\", 12.782478332519531),\n",
       " ([\"'s\", 'fine'], \" John's a fine man.\", 12.778295993804932),\n",
       " ([\"'s\", 'changed'], \" John's a changed man.\", 12.776861667633057),\n",
       " ([\"'s\", 'black'], \" John's a black man.\", 12.761423587799072),\n",
       " ([\"'s\", 'dead'], \" John's a dead man.\", 12.753207683563232),\n",
       " ([\"'s\", 'real'], \" John's a real man.\", 12.740143775939941),\n",
       " (['became', 'gay'], ' John became a gay man.', 12.732085227966309),\n",
       " (['became', 'great'], ' John became a great man.', 12.72254991531372),\n",
       " (['died', 'black'], ' John died a black man.', 12.71377944946289),\n",
       " ([\"'m\", 'black'], \" John'm a black man.\", 12.713159084320068),\n",
       " (['became', 'rich'], ' John became a rich man.', 12.706174373626709),\n",
       " ([',', 'young'], ' John, a young man.', 12.673038005828857),\n",
       " (['am', 'good'], ' John am a good man.', 12.641968727111816),\n",
       " (['are', 'wonderful'], ' John are a wonderful man.', 12.638267040252686),\n",
       " (['became', 'different'],\n",
       "  ' John became a different man.',\n",
       "  12.633668899536133),\n",
       " (['are', 'beautiful'], ' John are a beautiful man.', 12.629822254180908),\n",
       " (['are', 'brave'], ' John are a brave man.', 12.604943752288818),\n",
       " (['am', 'free'], ' John am a free man.', 12.590981483459473),\n",
       " (['became', 'wealthy'], ' John became a wealthy man.', 12.57527780532837),\n",
       " (['became', 'grown'], ' John became a grown man.', 12.57297658920288),\n",
       " (['died', 'peaceful'], ' John died a peaceful man.', 12.562773704528809),\n",
       " (['became', 'new'], ' John became a new man.', 12.537691593170166),\n",
       " ([\"'m\", 'good'], \" John'm a good man.\", 12.525117874145508),\n",
       " (['are', 'lucky'], ' John are a lucky man.', 12.516343116760254),\n",
       " (['are', 'smart'], ' John are a smart man.', 12.509212970733643),\n",
       " (['died', 'rich'], ' John died a rich man.', 12.48497200012207),\n",
       " (['became', 'real'], ' John became a real man.', 12.448122024536133),\n",
       " (['are', 'nice'], ' John are a nice man.', 12.428942203521729),\n",
       " (['died', 'broken'], ' John died a broken man.', 12.401114463806152),\n",
       " ([\"'m\", 'white'], \" John'm a white man.\", 12.397777557373047),\n",
       " ([\"'m\", 'grown'], \" John'm a grown man.\", 12.39085578918457),\n",
       " (['am', 'gay'], ' John am a gay man.', 12.377883911132812),\n",
       " (['became', 'powerful'], ' John became a powerful man.', 12.377667903900146),\n",
       " (['died', 'great'], ' John died a great man.', 12.35351276397705),\n",
       " ([\"'m\", 'free'], \" John'm a free man.\", 12.35005235671997),\n",
       " (['were', 'nice'], ' John were a nice man.', 12.329878807067871),\n",
       " (['am', 'happy'], ' John am a happy man.', 12.328615188598633),\n",
       " ([\"'m\", 'proud'], \" John'm a proud man.\", 12.317473888397217),\n",
       " (['were', 'young'], ' John were a young man.', 12.312262535095215),\n",
       " ([\"'m\", 'young'], \" John'm a young man.\", 12.29404067993164),\n",
       " (['are', 'young'], ' John are a young man.', 12.293720722198486),\n",
       " (['died', 'decorated'], ' John died a decorated man.', 12.278705596923828),\n",
       " (['died', 'battered'], ' John died a battered man.', 12.255933284759521),\n",
       " (['died', 'bitter'], ' John died a bitter man.', 12.254289150238037),\n",
       " (['became', 'successful'],\n",
       "  ' John became a successful man.',\n",
       "  12.252227783203125),\n",
       " ([\"'m\", 'changed'], \" John'm a changed man.\", 12.247802734375),\n",
       " (['died', 'proud'], ' John died a proud man.', 12.243779182434082),\n",
       " (['died', 'beautiful'], ' John died a beautiful man.', 12.243287563323975),\n",
       " (['died', 'humble'], ' John died a humble man.', 12.241056442260742),\n",
       " (['became', 'happy'], ' John became a happy man.', 12.202312469482422),\n",
       " (['died', 'marked'], ' John died a marked man.', 12.200020790100098),\n",
       " (['died', 'sick'], ' John died a sick man.', 12.184815406799316),\n",
       " (['am', 'proud'], ' John am a proud man.', 12.18204116821289),\n",
       " (['became', 'strong'], ' John became a strong man.', 12.1800856590271),\n",
       " ([\"'m\", 'happy'], \" John'm a happy man.\", 12.177632808685303),\n",
       " (['are', 'lovely'], ' John are a lovely man.', 12.166886329650879),\n",
       " (['were', 'beautiful'], ' John were a beautiful man.', 12.161325931549072),\n",
       " (['were', 'great'], ' John were a great man.', 12.145051002502441),\n",
       " (['are', 'fine'], ' John are a fine man.', 12.144169807434082),\n",
       " (['are', 'strong'], ' John are a strong man.', 12.142818927764893),\n",
       " (['became', 'mad'], ' John became a mad man.', 12.114687442779541),\n",
       " (['were', 'strong'], ' John were a strong man.', 12.10348892211914),\n",
       " ([',', 'great'], ' John, a great man.', 12.100997924804688),\n",
       " (['am', 'young'], ' John am a young man.', 12.08688735961914),\n",
       " (['am', 'black'], ' John am a black man.', 12.078092575073242),\n",
       " ([\"'m\", 'lucky'], \" John'm a lucky man.\", 12.077069282531738),\n",
       " (['are', 'remarkable'], ' John are a remarkable man.', 12.070228099822998),\n",
       " (['are', 'brilliant'], ' John are a brilliant man.', 12.058187007904053),\n",
       " ([\"'m\", 'family'], \" John'm a family man.\", 12.027591705322266),\n",
       " (['am', 'simple'], ' John am a simple man.', 12.001724243164062),\n",
       " (['were', 'wise'], ' John were a wise man.', 11.99919080734253),\n",
       " (['are', 'real'], ' John are a real man.', 11.991503238677979),\n",
       " ([',', 'fine'], ' John, a fine man.', 11.989847183227539),\n",
       " (['were', 'gentle'], ' John were a gentle man.', 11.978329181671143),\n",
       " ([\"'m\", 'rich'], \" John'm a rich man.\", 11.965765953063965),\n",
       " (['are', 'wise'], ' John are a wise man.', 11.95874547958374),\n",
       " (['were', 'kind'], ' John were a kind man.', 11.951794624328613),\n",
       " (['are', 'free'], ' John are a free man.', 11.941401958465576),\n",
       " ([',', 'nice'], ' John, a nice man.', 11.940392971038818),\n",
       " (['am', 'Christian'], ' John am a Christian man.', 11.925127506256104),\n",
       " (['are', 'strange'], ' John are a strange man.', 11.909783840179443),\n",
       " ([',', 'wise'], ' John, a wise man.', 11.902738094329834),\n",
       " ([',', 'changed'], ' John, a changed man.', 11.858073234558105),\n",
       " (['are', 'decent'], ' John are a decent man.', 11.857463359832764),\n",
       " (['are', 'handsome'], ' John are a handsome man.', 11.845951557159424),\n",
       " ([',', 'beautiful'], ' John, a beautiful man.', 11.845780849456787),\n",
       " (['were', 'brave'], ' John were a brave man.', 11.845232009887695),\n",
       " (['were', 'wonderful'], ' John were a wonderful man.', 11.842020988464355),\n",
       " ([\"'m\", 'broken'], \" John'm a broken man.\", 11.829437732696533),\n",
       " (['were', 'handsome'], ' John were a handsome man.', 11.826627731323242),\n",
       " ([',', 'lucky'], ' John, a lucky man.', 11.806559562683105),\n",
       " ([\"'m\", 'dead'], \" John'm a dead man.\", 11.805611610412598),\n",
       " ([',', 'wonderful'], ' John, a wonderful man.', 11.801129341125488),\n",
       " (['am', 'rich'], ' John am a rich man.', 11.79903268814087),\n",
       " (['am', 'lucky'], ' John am a lucky man.', 11.784144878387451),\n",
       " (['were', 'tough'], ' John were a tough man.', 11.783781051635742),\n",
       " (['were', 'quiet'], ' John were a quiet man.', 11.759674072265625),\n",
       " (['were', 'brilliant'], ' John were a brilliant man.', 11.744588851928711),\n",
       " (['were', 'lovely'], ' John were a lovely man.', 11.739027500152588),\n",
       " ([',', 'smart'], ' John, a smart man.', 11.710652828216553),\n",
       " (['were', 'strange'], ' John were a strange man.', 11.6994047164917),\n",
       " ([',', 'black'], ' John, a black man.', 11.694779396057129),\n",
       " ([',', 'brave'], ' John, a brave man.', 11.6730318069458),\n",
       " ([',', 'funny'], ' John, a funny man.', 11.671895980834961),\n",
       " ([',', 'handsome'], ' John, a handsome man.', 11.665618419647217),\n",
       " ([',', 'lovely'], ' John, a lovely man.', 11.663179874420166),\n",
       " (['am', 'white'], ' John am a white man.', 11.653889656066895),\n",
       " (['am', 'religious'], ' John am a religious man.', 11.651205062866211),\n",
       " (['were', 'smart'], ' John were a smart man.', 11.642578125),\n",
       " ([\"'m\", 'smart'], \" John'm a smart man.\", 11.636448860168457),\n",
       " (['were', 'humble'], ' John were a humble man.', 11.63623332977295),\n",
       " (['am', 'family'], ' John am a family man.', 11.612370491027832),\n",
       " ([',', 'happy'], ' John, a happy man.', 11.610529899597168),\n",
       " (['were', 'fine'], ' John were a fine man.', 11.607154369354248),\n",
       " ([',', 'strange'], ' John, a strange man.', 11.607141971588135),\n",
       " ([',', 'real'], ' John, a real man.', 11.605802059173584),\n",
       " ([\"'m\", 'married'], \" John'm a married man.\", 11.592070579528809),\n",
       " ([\"'m\", 'mad'], \" John'm a mad man.\", 11.583403587341309),\n",
       " ([',', 'dead'], ' John, a dead man.', 11.582603931427002),\n",
       " ([\"'m\", 'simple'], \" John'm a simple man.\", 11.5477294921875),\n",
       " (['am', 'changed'], ' John am a changed man.', 11.520135879516602),\n",
       " (['am', 'married'], ' John am a married man.', 11.50126838684082),\n",
       " ([\"'m\", 'tired'], \" John'm a tired man.\", 11.4750075340271),\n",
       " (['am', 'Muslim'], ' John am a Muslim man.', 11.464566230773926),\n",
       " (['am', 'great'], ' John am a great man.', 11.451177597045898),\n",
       " (['am', 'humble'], ' John am a humble man.', 11.39908742904663),\n",
       " (['am', 'peaceful'], ' John am a peaceful man.', 11.39117431640625)]"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.unmask('John <mask> a <mask> man.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['woman'], ' Mary is not a man, she is awoman.', 13.98249340057373),\n",
       " (['man'], ' Mary is not a man, she is a man.', 10.98160719871521),\n",
       " (['monster'], ' Mary is not a man, she is a monster.', 10.796409130096436),\n",
       " (['girl'], ' Mary is not a man, she is a girl.', 10.478504419326782),\n",
       " (['joke'], ' Mary is not a man, she is a joke.', 10.465309858322144),\n",
       " (['myth'], ' Mary is not a man, she is a myth.', 10.431484699249268),\n",
       " (['god'], ' Mary is not a man, she is a god.', 10.417447566986084),\n",
       " (['child'], ' Mary is not a man, she is a child.', 10.40675973892212),\n",
       " (['robot'], ' Mary is not a man, she is a robot.', 10.308293581008911),\n",
       " (['lie'], ' Mary is not a man, she is a lie.', 10.136309623718262)]"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.unmask_multiple(['John is a <mask>.', 'John is not a <mask>.', 'Mary is a <mask>.', 'Mary is not a man, she is a <mask>.'], metric='avg', beam_size=1000)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:     100\n",
      "Filtered cases: 72 (72.0%)\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 100), add_positive, keep_original=True)\n",
    "expect_fn = Expect.monotonic(1, increasing=True, tolerance=0.1)\n",
    "test = Dir(data, expect_fn)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=True)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp = model_wrapper.ModelWrapper()\n",
    "pp = PredictorWrapper.wrap_softmax(qqp.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\tWhat does the Quran say about homosexuality?\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qs = []\n",
    "labels = []\n",
    "all_questions = set()\n",
    "for x in open('/home/marcotcr/datasets/glue/glue_data/QQP/dev.tsv').readlines()[1:]:\n",
    "    try:\n",
    "        q1, q2, label = x.strip().split('\\t')[3:]\n",
    "    except:\n",
    "        print(x)\n",
    "        continue\n",
    "    all_questions.add(q1)\n",
    "    all_questions.add(q2)\n",
    "    qs.append((q1, q2))\n",
    "    labels.append(label)\n",
    "labels = np.array(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran = [x[0][0] for x in tg.unmask('What does the Quran say about <mask>?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran.remove('homosexuals')\n",
    "quran.remove('gays')\n",
    "quran.remove('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 272 examples\n",
      "Test cases:     272\n",
      "Failure rate:   2.2%\n",
      "\n",
      "Example fails:\n",
      "0.6 ('What does the Quran say about polygamy?', 'What does the Quran say about circumcision?')\n",
      "0.7 ('What does the Quran say about circumcision?', 'What does the Quran say about polygamy?')\n",
      "0.8 ('What does the Quran say about Muhammad?', 'What does the Quran say about Muslims?')\n",
      "1.0 ('What does the Quran say about Islam?', 'What does the Quran say about Muslims?')\n",
      "0.8 ('What does the Quran say about Muslims?', 'What does the Quran say about Islam?')\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(('What does the Quran say about {thing1}?', 'What does the Quran say about {thing2}?'),\n",
    "                thing1=quran, thing2=quran, remove_duplicates=True)\n",
    "test = Mft(data, labels=0)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bible', 'book', 'report', 'survey', 'poll', 'study', 'Constitution', 'data', 'bible', 'election', 'census', 'law', 'UN', 'Quran', 'evidence', 'science', 'constitution', 'Koran', 'vote', 'bill', 'film', 'Holocaust', 'research', 'Pope', 'letter', 'article', 'verdict', 'world', 'movie', 'government', 'video', 'Torah', 'song', 'moon', 'public', 'dictionary', 'president', 'referendum', 'ACA', 'eclipse', 'church', 'US', 'media', 'Church', 'pope', 'record', 'test', 'answer', 'literature', 'Prophet', 'CDC', 'FDA', 'WHO', 'scripture', 'community', 'Buddha', 'ban', 'prophet', 'statement', 'interview', 'Gospel', 'Book', 'past', 'question', 'decision']\n"
     ]
    }
   ],
   "source": [
    "print([x[0][0] for x in tg.unmask_multiple(editor.template('What does the <mask> say about {thing}?', thing=quran))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1530 examples\n",
      "Test cases:     1530\n",
      "Failure rate:   8.9%\n",
      "\n",
      "Example fails:\n",
      "0.6 ('What does the Bible say about homosexuality?', 'What does the Church say about homosexuality?')\n",
      "1.0 ('What does the Gospel say about you?', 'What does the Bible say about you?')\n",
      "0.9 ('What does the Quran say about religion?', 'What does the Prophet say about religion?')\n",
      "0.6 ('What does the Bible say about women?', 'What does the Torah say about women?')\n",
      "1.0 ('What does the Bible say about God?', 'What does the Gospel say about God?')\n"
     ]
    }
   ],
   "source": [
    "books = ['Bible', 'Constitution', 'Quran', 'Pope', 'Torah', 'Church', 'Buddha', 'Prophet', 'Gospel', 'Book']\n",
    "data = editor.template(('What does the {book1} say about {thing}?', 'What does the {book2} say about {thing}?'),\n",
    "                thing=quran, book1=books, book2=books, remove_duplicates=True)\n",
    "test = Mft(data, labels=0)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inv, dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_map =  pickle.load(open('/home/marcotcr/tmp/processed_qqp.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_qs = [spacy_map[x[0]] for x in qs] + [spacy_map[x[1]] for x in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pairs = [(spacy_map[x[0]],spacy_map[x[1]]) for x in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_names(pair):\n",
    "    x0, x1 = pair\n",
    "    ents_0 = set([x[0].text for x in x0.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ents_1 = set([x[0].text for x in x1.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ret = []\n",
    "    if ents_0 and ents_0.intersection(ents_1):\n",
    "        for e in ents_0.intersection(ents_1):\n",
    "            if e == 'Quora':\n",
    "                continue\n",
    "            ret.extend([(pair[0].text.replace(e, n), pair[1].text.replace(e, n)) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        return [ret[i] for i in idxs]\n",
    "# change_names(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9792 examples\n",
      "Test cases:     1632\n",
      "Failure rate:   11.1%\n",
      "\n",
      "Example fails:\n",
      "0.2 (\"What is India's relationship with Bangladesh?\", \"What is Bangladesh's relationship with India?\")\n",
      "0.9 (\"What is India's relationship with Isabella?\", \"What is Isabella's relationship with India?\")\n",
      "0.9 (\"What is India's relationship with Jesus?\", \"What is Jesus's relationship with India?\")\n",
      "\n",
      "1.0 ('Which is best place to stay and visit in Kerala?', 'What are the best 10 places to visit in Kerala including any falls?')\n",
      "0.4 ('Which is best place to stay and visit in Sara?', 'What are the best 10 places to visit in Sara including any falls?')\n",
      "0.4 ('Which is best place to stay and visit in Emily?', 'What are the best 10 places to visit in Emily including any falls?')\n",
      "\n",
      "1.0 ('What is Jake Williams’s history that made him into a narcissist?', 'How is Jake Williams a narcissist?')\n",
      "0.3 ('What is Sara Williams’s history that made him into a narcissist?', 'How is Sara Williams a narcissist?')\n",
      "0.4 ('What is Nicole Williams’s history that made him into a narcissist?', 'How is Nicole Williams a narcissist?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = np.random.choice(len(processed_pairs), 2000,  replace=False)\n",
    "# idxs = np.random.choice(len(processed_pairs), len(processed_pairs),  replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_names, keep_original=True)\n",
    "# test = Dir(data, Expect.wrap(Expect.all(Expect.pairwise_to_group(Expect.monotonic_label(1, increasing=True, tolerance=0)))) )\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 432 examples\n",
      "Test cases:     72\n",
      "Failure rate:   5.6%\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What are the best places to visit in Wayanad, Kerala?', 'What can be the medium budget to visit best places in Kerala for three members (2-3 days)?')\n",
      "0.2 ('What are the best places to visit in Wayanad, Isaiah?', 'What can be the medium budget to visit best places in Isaiah for three members (2-3 days)?')\n",
      "0.0 ('What are the best places to visit in Wayanad, Jamie?', 'What can be the medium budget to visit best places in Jamie for three members (2-3 days)?')\n",
      "\n",
      "0.9 ('Was Donald Trump always rich?', 'How rich is Donald Trump?')\n",
      "0.5 ('Was Jason Trump always rich?', 'How rich is Jason Trump?')\n",
      "\n",
      "1.0 ('What is the best way to start contributing to the Linux kernel?', 'How should I start contributing for Linux?')\n",
      "0.2 ('What is the best way to start contributing to the Gabriel kernel?', 'How should I start contributing for Gabriel?')\n",
      "0.0 ('What is the best way to start contributing to the Elizabeth kernel?', 'How should I start contributing for Elizabeth?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idxs = np.random.choice(len(processed_pairs), 2000,  replace=False)\n",
    "# idxs = np.random.choice(len(processed_pairs), len(processed_pairs),  replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_names, keep_original=True)\n",
    "# test = Dir(data, Expect.wrap(Expect.all(Expect.pairwise_to_group(Expect.monotonic_label(1, increasing=True, tolerance=0)))) )\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.results.passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name_in_one(pair):\n",
    "    x0, x1 = pair\n",
    "    ents_0 = set([x[0].text for x in x0.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ents_1 = set([x[0].text for x in x1.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ret = []\n",
    "    if ents_0 and ents_0.intersection(ents_1):\n",
    "        for e in ents_0.intersection(ents_1):\n",
    "            if e == 'Quora':\n",
    "                continue\n",
    "            ret.extend([(pair[0].text.replace(e, n), pair[1].text) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "            ret.extend([(pair[0].text, pair[1].text.replace(e, n)) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        return [ret[i] for i in idxs]\n",
    "# change_names(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 756 examples\n",
      "Test cases:     126\n",
      "Filtered cases: 64 (50.8%)\n",
      "Failure rate:   3.2%\n",
      "\n",
      "Example fails:\n",
      "0.1 ('Why does Donald Trump think the debate schedule favors Hillary?', 'Why is Donald Trump saying the debate schedule is unfair? Is it because his support base would rather watch the NFL than his debate?')\n",
      "0.2 ('Why does Donald Trump think the debate schedule favors Hillary?', 'Why is Ava Trump saying the debate schedule is unfair? Is it because his support base would rather watch the NFL than his debate?')\n",
      "\n",
      "0.5 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Donald Trump winning the Republican nomination?')\n",
      "1.0 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Adam Trump winning the Republican nomination?')\n",
      "1.0 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Austin Trump winning the Republican nomination?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monotonic = Expect.monotonic(1, increasing=False, tolerance=0.1)\n",
    "idxs = np.random.choice(len(processed_pairs), 3000, replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_name_in_one, keep_original=True)\n",
    "test = Dir(data, monotonic)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=False)\n",
    "test.summary(n=3)\n",
    "# test.results.passed[test.results.passed != None].astype(bool).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only consider examples that were duplicates\n",
    "sl_fn = lambda pred, *args, **kwargs: pred == 1\n",
    "# This substitutes the previous one\n",
    "# def sl_fn(x, pred, *args, **kwargs):\n",
    "#     print(pred)\n",
    "#     return pred[0] == 1\n",
    "sl_fn2 = lambda x, pred, *args, **kwargs: pred[0] == 1\n",
    "is_false = Expect.single(Expect.eq(0), agg_fn=Expect.all(ignore_first=True))#, slice_fn=sl_fn)\n",
    "# is_false = Expect.slice_pairwise(is_false, sl_fn)\n",
    "is_false = Expect.slice_testcase(is_false, sl_fn2)\n",
    "test.set_expect(is_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     126\n",
      "Filtered cases: 53 (42.1%)\n",
      "Failure rate:   52.1%\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What are the best books on Joseph Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "1.0 ('What are the best books on Peter Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "1.0 ('What are the best books on Jesse Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "\n",
      "1.0 ('How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sahara?', 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sonoran Desert?')\n",
      "1.0 ('How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sahara?', 'How cold can Tiffany Gobi Desert get, and how do its average temperatures compare to Tiffany ones in Tiffany Sonoran Desert?')\n",
      "1.0 ('How cold can Ella Gobi Desert get, and how do its average temperatures compare to Ella ones in Ella Sahara?', 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sonoran Desert?')\n",
      "\n",
      "0.9 ('Should I read A Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "0.9 ('Should I read Ian Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "0.9 ('Should I read Christina Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from checklist.inv_dir import pairwise_print_fn\n",
    "def fail_cr(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return pred != 0\n",
    "def sort_cr(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return orig_conf[0]\n",
    "print_fn = pairwise_print_fn(fail_cr, sort_cr)\n",
    "test.summary(n=3, print_fn=print_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltests import bert_squad_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_squad_model.BertSquad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just makes confidence=1 for every prediction\n",
    "pp = PredictorWrapper.wrap_predict(model.predict_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2415e533d24776ac9f9d70c9e2c10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mary']"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_pairs([('Who is dumb?', 'Mary is somewhat dumb. John is not so dumb.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f75826c94941a5a642cf712e25dfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Mary'], array([1.]))"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp([('Who is dumb?', 'Mary is somewhat dumb. John is not so dumb.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drinking', 'smoking', 'crying', 'reading', 'writing', 'worrying', 'eating', 'driving', 'listening', 'praying', 'talking', 'working', 'trying', 'calling', 'laughing']\n"
     ]
    }
   ],
   "source": [
    "verbs = [x[0][0] for x in tg.unmask('Luke told Mary that she should probably stop <mask>.')]\n",
    "verbs = [x for x in verbs if 'ing' in x]\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data, maps = editor.template(('Who is currently {verb}?', '{male} told {female} she should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['female'] for m in maps]\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{male} told {female} he should probably stop {verb}'), nsamples=100,verb=verbs, return_maps=True)\n",
    "labels += [m['male'] for m in maps]\n",
    "data += n\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{female} told {male} he should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['male'] for m in maps]\n",
    "data += n\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{female} told {male} she should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['female'] for m in maps]\n",
    "data += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Who is currently worrying?',\n",
       " 'Juan told Crystal she should probably stop worrying')"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f8fa8a43d1438cbc766449fd7abdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = Mft(data, labels=labels)\n",
    "test.run(pp)\n",
    "# test.results.passed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_squad_with_context(x, pred, conf, label=None, *args, **kwargs):\n",
    "    q, c = x\n",
    "    ret = 'C: %s\\nQ: %s\\n' % (c, q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_squad(x, pred, conf, label=None, *args, **kwargs):\n",
    "    q, c = x\n",
    "    ret = 'Q: %s\\n' % (q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     400\n",
      "Failure rate:   58.2%\n",
      "\n",
      "Example fails:\n",
      "C: Taylor told Carlos she should probably stop praying\n",
      "Q: Who is currently praying?\n",
      "A: Taylor\n",
      "P: Carlos\n",
      "\n",
      "C: Carlos told Rachel he should probably stop worrying\n",
      "Q: Who is currently worrying?\n",
      "A: Carlos\n",
      "P: Rachel\n",
      "\n",
      "C: Peter told Chloe she should probably stop trying\n",
      "Q: Who is currently trying?\n",
      "A: Chloe\n",
      "P: Peter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3, format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked = [x[0][0] for x in tg.unmask('John was told by Luke that he really likes <mask>.', beam_size=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data, maps = editor.template(('Who really likes {liked}?', '{male1} was told by {male2} that {male2} really likes {liked}.'), male1=editor.lexicons['male'], male2=editor.lexicons['male'], nsamples=100, verb=verbs, liked=liked, return_maps=True, remove_duplicates=True)\n",
    "labels += [m['male2'] for m in maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130a2d7705454f35b5de65d29223020a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test cases:     100\n",
      "Failure rate:   44.0%\n",
      "\n",
      "Example fails:\n",
      "C: Eric was told by Austin that Austin really likes Hunter.\n",
      "Q: Who really likes Hunter?\n",
      "A: Austin\n",
      "P: Eric\n",
      "\n",
      "C: Ethan was told by Juan that Juan really likes Jess.\n",
      "Q: Who really likes Jess?\n",
      "A: Juan\n",
      "P: Juan that Juan\n",
      "\n",
      "C: Robert was told by Ian that Ian really likes Vanessa.\n",
      "Q: Who really likes Vanessa?\n",
      "A: Ian\n",
      "P: Robert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = Mft(data, labels=labels)\n",
    "test.run(pp)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bible', 'book', 'report', 'survey', 'poll', 'study', 'Constitution', 'data', 'bible', 'election', 'census', 'law', 'UN', 'Quran', 'evidence', 'science', 'constitution', 'Koran', 'vote', 'bill', 'film', 'Holocaust', 'research', 'Pope', 'letter', 'article', 'verdict', 'world', 'movie', 'government', 'video', 'Torah', 'song', 'moon', 'public', 'dictionary', 'president', 'referendum', 'ACA', 'eclipse', 'church', 'US', 'media', 'Church', 'pope', 'record', 'test', 'answer', 'literature', 'Prophet', 'CDC', 'FDA', 'WHO', 'scripture', 'community', 'Buddha', 'ban', 'prophet', 'statement', 'interview', 'Gospel', 'Book', 'past', 'question', 'decision']\n"
     ]
    }
   ],
   "source": [
    "print([x[0][0] for x in tg.unmask_multiple(editor.template('What does the <mask> say about {thing}?', thing=quran))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_squad(fold='validation'):\n",
    "    answers = []\n",
    "    data = []\n",
    "    ids = []\n",
    "    files = {\n",
    "        'validation': '/home/marcotcr/datasets/squad/dev-v1.1.json',\n",
    "        'train': '/home/marcotcr//datasets/squad/train-v1.1.json',\n",
    "        }\n",
    "    f = json.load(open(files[fold]))\n",
    "    for t in f['data']:\n",
    "        for p in t['paragraphs']:\n",
    "            context = p['context']\n",
    "            for qa in p['qas']:\n",
    "                data.append({'passage': context, 'question': qa['question'], 'id': qa['id']})\n",
    "                answers.append(set([(x['text'], x['answer_start']) for x in qa['answers']]))\n",
    "    return data, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, answers =  load_squad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(x['question'], x['passage']) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name(x):\n",
    "    q, c = x\n",
    "    in_p = set()\n",
    "    not_in_p = set()\n",
    "    for n in editor.lexicons['male'][:10]:\n",
    "        if re.search(r'\\b%s\\b' % n, c):\n",
    "            in_p.add(n)\n",
    "        else:\n",
    "            not_in_p.add(n)\n",
    "    if not in_p:\n",
    "        return None\n",
    "    ret = []\n",
    "    ret_add = []\n",
    "    for p in in_p:\n",
    "        for n in not_in_p:\n",
    "            ret.append((re.sub(r'\\b%s\\b' % p, n, q), re.sub(r'\\b%s\\b' % p, n, c)))\n",
    "            ret_add.append((p, n))\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        ret = [ret[i] for i in idxs]\n",
    "        ret_add = [ret_add[i] for i in idxs]\n",
    "    return ret, ret_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i, x in enumerate(pairs) if 'John' in x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2058 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37654cb9177d49eca96387bc6ffd2792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=268.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test cases:     343\n",
      "Failure rate:   1.2%\n",
      "\n",
      "Example fails:\n",
      "Q: What did John Dobson describe Newcastle as?\n",
      "P: neoclassical centre referred to as Tyneside Classical\n",
      "\n",
      "John -> Daniel\n",
      "Q: What did Daniel Dobson describe Newcastle as?\n",
      "P: neoclassical\n",
      "\n",
      "\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor\n",
      "\n",
      "John -> James\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor meets an unknown incarnation of himself\n",
      "\n",
      "John -> Joseph\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor meets an unknown incarnation of himself\n",
      "\n",
      "\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: both Kenia and Kegnia\n",
      "\n",
      "Joseph -> James\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: Kenia and Kegnia\n",
      "\n",
      "Joseph -> William\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: Kenia and Kegnia\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def new_eq(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    if meta:\n",
    "        p, n = meta\n",
    "        orig_pred =  re.sub(r'\\b%s\\b' % p, n, orig_pred)\n",
    "    return pred == orig_pred\n",
    "\n",
    "def format_name(x, pred, conf, label=None, meta=None):\n",
    "    ret = ''\n",
    "    if meta is not None and len(meta):\n",
    "        ret = '%s -> %s\\n' % meta\n",
    "    ret += format_squad(x, pred, conf, label, meta)\n",
    "    return ret\n",
    "    \n",
    "# tt = Expect.wrap(Expect.all(Expect.pairwise_to_group(new_eq), ignore_first=True))\n",
    "random_idxs = np.random.choice(len(pairs), 3000)\n",
    "data, meta = Perturb.perturb([pairs[i] for i in random_idxs], change_name, returns_additional=True)\n",
    "tt = Expect.pairwise(new_eq)\n",
    "test = Inv(data, expect=tt, meta=meta)\n",
    "test.run(pp)\n",
    "test.summary(n=3, format_example_fn=format_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " ('John', 'Joshua'),\n",
       " ('John', 'Matthew'),\n",
       " ('John', 'William'),\n",
       " ('John', 'Christopher'),\n",
       " ('John', 'Michael'),\n",
       " ('John', 'James'),\n",
       " ('John', 'David'),\n",
       " ('John', 'Daniel'),\n",
       " ('John', 'Joseph')]"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.meta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['John Fox', 'Joshua Fox', 'Matthew Fox', 'William Fox',\n",
       "       'Christopher Fox', 'Michael Fox', 'James Fox', 'David Fox',\n",
       "       'Daniel Fox', 'Joseph Fox'], dtype='<U15')"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.results.preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function checklist.expect.Expect.all.<locals>.expect(xs, preds, confs, labels=None, meta=None)>"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John', 'Joshua')"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_name(pairs[178])[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in pairs if 'John' in x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "How is vanilla extract made?"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_qs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Luke', 'Mark']"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons['male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40430"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NORP'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_qs[0].ents[0][0].ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template('This is {bad}',  bad=['bad', 'great', 'awesome'], return_maps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('This is bad', 'This is not bad'),\n",
       "  ('This is great', 'This is not great'),\n",
       "  ('This is awesome', 'This is not awesome')],\n",
       " [{'bad': 'bad'}, {'bad': 'great'}, {'bad': 'awesome'}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "editor.template(('This is {bad}', 'This is not {bad}'),  bad=['bad', 'great', 'awesome'], return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bad': 'This is bad', 'notbad': 'This is not bad'},\n",
       " {'bad': 'This is great', 'notbad': 'This is not great'},\n",
       " {'bad': 'This is awesome', 'notbad': 'This is not awesome'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template({\n",
    "    'bad': 'This is {bad}',\n",
    "    'notbad': 'This is not {bad}'},  bad=['bad', 'great', 'awesome'], return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'bad': 'This is {bad}',\n",
    "    'notbad': ('this is not {bad}', 'this is quite {abad}')}\n",
    "b = ({'bad': 'This is {bad}', 'notbad': 'This is not {bad}'}, 'This is quite {bad}')\n",
    "c = 'This is quite {bad}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': ['John', 'Luke', 'Mark'], 'female': ['Mary', 'Judy', 'Julia']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is terrible, John',\n",
       " 'This is bad, John',\n",
       " 'This is terrible, Luke',\n",
       " 'This is bad, Luke',\n",
       " 'This is terrible, Mark',\n",
       " 'This is bad, Mark']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is {bad}, {male}', bad=('terrible', 'bad'), nsamples=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
